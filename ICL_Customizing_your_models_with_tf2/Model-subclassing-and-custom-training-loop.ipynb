{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model subclassing and custom training loops\n",
    "\n",
    "> In the section, we're going to look at ways to build fully customizable deep learning models and layers, as well as custom training loops. This is the summary of lecture \"Customizing your model with Tensorflow 2\" from Coursera.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Chanseok Kang\n",
    "- categories: [Python, Coursera, Deep_Learning, Tensorflow]\n",
    "- image: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow: v2.3.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "print('Tensorflow: v' + tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model subclassing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "class MyModel(Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyModel, self).__init__(**kwargs)\n",
    "        self.dense = Dense(16)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.dense(inputs)\n",
    "    \n",
    "my_model = MyModel(name='my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(Model):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super(MyModel, self).__init__(**kwargs)\n",
    "        self.dense1 = Dense(16, activation='sigmoid')\n",
    "        self.dense2 = Dense(num_classes, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        h = self.dense1(inputs)\n",
    "        return self.dense2(h)\n",
    "    \n",
    "my_model = MyModel(10, name='my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "class MyModel(Model):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super(MyModel, self).__init__(**kwargs)\n",
    "        self.dense1 = Dense(16, activation='sigmoid')\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.dense2 = Dense(num_classes, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        h = self.dense1(inputs)\n",
    "        h = self.dropout(h, training=training)\n",
    "        return self.dense2(h)\n",
    "    \n",
    "my_model = MyModel(12, name='my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a simple model using the model subclassing API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Softmax, concatenate\n",
    "\n",
    "# Build the model\n",
    "class MyModel(Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.dense_1 = Dense(64, activation='relu')\n",
    "        self.dense_2 = Dense(10)\n",
    "        self.dense_3 = Dense(5)\n",
    "        self.softmax = Softmax()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense_1(inputs)\n",
    "        y1 = self.dense_2(inputs)\n",
    "        y2 = self.dense_3(y1)\n",
    "        concat = concatenate([x, y2])\n",
    "        return self.softmax(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              multiple                  704       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  110       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              multiple                  55        \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            multiple                  0         \n",
      "=================================================================\n",
      "Total params: 869\n",
      "Trainable params: 869\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print the model summary\n",
    "model = MyModel()\n",
    "model(tf.random.uniform([1, 10]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom layers\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class LinearMap(Layer):\n",
    "    def __init__(self, input_dim, units):\n",
    "        super(LinearMap, self).__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(initial_value=w_init(shape=(input_dim, units)))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.13154064,  0.14464289]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer = LinearMap(3, 2)\n",
    "inputs = tf.ones((1, 3))\n",
    "linear_layer(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(3, 2) dtype=float32, numpy=\n",
       " array([[-0.08358198,  0.01858256],\n",
       "        [ 0.00805996,  0.10795677],\n",
       "        [-0.05601863,  0.01810357]], dtype=float32)>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearMap(Layer):\n",
    "    def __init__(self, input_dim, units):\n",
    "        super(LinearMap, self).__init__()\n",
    "        self.w = self.add_weight(shape=(input_dim, units), initializer='random_normal')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(Model):\n",
    "    def __init__(self, hidden_units, outputs, **kwargs):\n",
    "        super(MyModel, self).__init__(**kwargs)\n",
    "        self.dense = Dense(hidden_units, activation='sigmoid')\n",
    "        self.linear = LinearMap(hidden_units, outputs)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        h = self.dense(inputs)\n",
    "        return self.linear(h)\n",
    "    \n",
    "my_model = MyModel(64, 12, name='my_custom_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create custom layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLayer(Layer):\n",
    "    def __init__(self, units, input_dim):\n",
    "        super(MyLayer, self).__init__()\n",
    "        self.w = self.add_weight(shape=(input_dim, units),\n",
    "                                 initializer='random_normal')\n",
    "        self.b = self.add_weight(shape=(units,),\n",
    "                                 initializer='zeros')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    \n",
    "dense_layer = MyLayer(3, 5)\n",
    "x = tf.ones((1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.01287571, 0.05181859, 0.03777028]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(5, 3) dtype=float32, numpy=\n",
       " array([[ 0.01299221, -0.06098207,  0.05607808],\n",
       "        [-0.03225943, -0.01771073,  0.00998432],\n",
       "        [-0.03099572,  0.04720604,  0.00623957],\n",
       "        [ 0.08610137,  0.05655754, -0.07421251],\n",
       "        [-0.02296271,  0.02674781,  0.03968081]], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_layer.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify trainable weights\n",
    "class MyLayer(Layer):\n",
    "    def __init__(self, units, input_dim, trainable=True):\n",
    "        super(MyLayer, self).__init__()\n",
    "        self.w = self.add_weight(shape=(input_dim, units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=trainable)\n",
    "        self.b = self.add_weight(shape=(units,),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=trainable)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    \n",
    "dense_layer = MyLayer(3, 5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable weights:  0\n",
      "non-trainable weights:  2\n"
     ]
    }
   ],
   "source": [
    "print('trainable weights: ', len(dense_layer.trainable_weights))\n",
    "print('non-trainable weights: ', len(dense_layer.non_trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom layer to accumulate means of output values\n",
    "class MyLayerMean(Layer):\n",
    "    def __init__(self, units, input_dim):\n",
    "        super(MyLayerMean, self).__init__()\n",
    "        self.w = self.add_weight(shape=(input_dim, units),\n",
    "                                 initializer='random_normal')\n",
    "        self.b = self.add_weight(shape=(units, ),\n",
    "                                 initializer='zeros')\n",
    "        self.sum_activation = tf.Variable(initial_value=tf.zeros((units, )),\n",
    "                                          trainable=False)\n",
    "        self.number_call = tf.Variable(initial_value=0, trainable=False)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        activations = tf.matmul(inputs, self.w) + self.b\n",
    "        self.sum_activation.assign_add(tf.reduce_sum(activations, axis=0))\n",
    "        self.number_call.assign_add(inputs.shape[0])\n",
    "        return activations, self.sum_activation / tf.cast(self.number_call, tf.float32)\n",
    "    \n",
    "dense_layer = MyLayerMean(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0.03351649 -0.11652843  0.04783446], shape=(3,), dtype=float32)\n",
      "tf.Tensor([ 0.03351649 -0.11652843  0.04783446], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y, activation_means = dense_layer(tf.ones((1, 5)))\n",
    "print(activation_means)\n",
    "\n",
    "y, activation_means = dense_layer(tf.ones((1, 5)))\n",
    "print(activation_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dropout layer as a custom layer\n",
    "class MyDropout(Layer):\n",
    "    def __init__(self, rate):\n",
    "        super(MyDropout, self).__init__()\n",
    "        self.rate = rate\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.nn.dropout(inputs, rate=self.rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the custom layers into a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(Model):\n",
    "    def __init__(self, units_1, input_dim_1, units_2, units_3):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.layer_1 = MyLayer(units_1, input_dim_1)\n",
    "        self.dropout_1 = MyDropout(0.5)\n",
    "        self.layer_2 = MyLayer(units_2, units_1)\n",
    "        self.dropout_2 = MyDropout(0.5)\n",
    "        self.layer_3 = MyLayer(units_3, units_2)\n",
    "        self.softmax = Softmax()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.layer_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 46), dtype=float32, numpy=\n",
       "array([[0.04749461, 0.0129902 , 0.02068885, 0.02848068, 0.05552515,\n",
       "        0.09300158, 0.00935825, 0.00225375, 0.03781042, 0.00451264,\n",
       "        0.12630779, 0.01227046, 0.01431969, 0.00842943, 0.00126452,\n",
       "        0.00645952, 0.01266537, 0.00493656, 0.02303323, 0.00660236,\n",
       "        0.02455252, 0.0039958 , 0.01692115, 0.06103001, 0.00990681,\n",
       "        0.00736192, 0.00882047, 0.01771174, 0.00564999, 0.01150204,\n",
       "        0.02416532, 0.063745  , 0.00438471, 0.0058762 , 0.05572766,\n",
       "        0.00755914, 0.01855095, 0.00134857, 0.00652165, 0.0333464 ,\n",
       "        0.01779144, 0.01062954, 0.01894175, 0.01143594, 0.01280271,\n",
       "        0.01131547]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyModel(64, 10000, 64, 46)\n",
    "model(tf.ones((1, 10000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "my_layer_2 (MyLayer)         multiple                  640064    \n",
      "_________________________________________________________________\n",
      "my_dropout (MyDropout)       multiple                  0         \n",
      "_________________________________________________________________\n",
      "my_layer_3 (MyLayer)         multiple                  4160      \n",
      "_________________________________________________________________\n",
      "my_dropout_1 (MyDropout)     multiple                  0         \n",
      "_________________________________________________________________\n",
      "my_layer_4 (MyLayer)         multiple                  2990      \n",
      "_________________________________________________________________\n",
      "softmax_1 (Softmax)          multiple                  0         \n",
      "=================================================================\n",
      "Total params: 647,214\n",
      "Trainable params: 647,214\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(2.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = x ** 2\n",
    "    grad = tape.gradient(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=4.0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-0.9364567>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([0, 1, 2, 3], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = tf.reduce_mean(x ** 2)\n",
    "    z = tf.math.sin(y)\n",
    "    dz_dy = tape.gradient(z, y)\n",
    "    \n",
    "dz_dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-0.9364567>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([0, 1, 2, 3], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = tf.reduce_mean(x ** 2)\n",
    "    z = tf.math.sin(y)\n",
    "    dz_dy, dz_dx = tape.gradient(z, [y, x])\n",
    "    \n",
    "dz_dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([-0.        , -0.46822834, -0.9364567 , -1.404685  ], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dz_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data from a noise contaminated linear model\n",
    "def makeNoisyData(m, b, n=20):\n",
    "    x = tf.random.uniform(shape=(n, ))\n",
    "    noise = tf.random.normal(shape=(len(x), ), stddev=0.1)\n",
    "    y = m * x + b + noise\n",
    "    return x, y\n",
    "\n",
    "X_train, y_train = makeNoisyData(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6IAAAI/CAYAAABtd2SuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAeOElEQVR4nO3db4yl51nf8d/FrAeoYoiEV4U6NotEaAkIcLuFHKUVhw6iCRKJKoUWWlIShfpNoKSNKtq0oi15saKogVT8SV2MQlBaoMQCNwKiaMkRTTlxuzYmIXZBFinBiqVsICRpI9h6cvfFM0k3w+zOWe/sdebMfD7S6jkz596Z68Wj3fnOc5/n1BgjAAAA0OWz1j0AAAAAp4sQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoNWZdX3jO+64Y5w7d25d3x4AAIBb6OGHH/7wGOPsQc+tLUTPnTuXS5curevbAwAAcAtV1e9f6zlbcwEAAGglRAEAAGglRAEAAGglRAEAAGglRAEAAGglRAEAAGglRAEAAGglRAEAAGglRAEAAGglRAEAAGglRAEAAGglRAEAAGglRAEAAGglRAEAAGglRAEAAGglRAEAAGglRAEAAGglRAEAAGglRAEAAGglRAEAAGglRAEAAGglRAEAAI7AcplcuDAdub4z6x4AAABg0y2Xyc5OcuVKsr2dXLyYzGbrnur4ckUUAADgJi0WU4Tu7k7HxWLdEx1vQhQAAOAmzefTldCtrek4n697ouPN1lwAAICbNJtN23EXiylCbcu9PiEKAABwBGYzAboqW3MBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABodWiIVtXnVNV/r6rfqqr3VdW/PmDNZ1fVz1XVE1X1UFWduxXDAgAAsPlWuSL6p0n+xhjjq5N8TZIXVtXz9615ZZKPjDG+NMkPJ/nBox0TAACAk+LQEB2T/7334W17f8a+ZS9J8tN7j38hyU5V1ZFNCQAAwImx0mtEq2qrqh5N8qEk7xhjPLRvyZ1J/iBJxhhPJ/loki84ykEBAAA4GVYK0THG7hjja5I8J8nXVtVX7lty0NXP/VdNU1X3VtWlqrp0+fLlG58WAACAjXdDd80dY/xxkkWSF+576skkdyVJVZ1J8vlJ/uiAv3/fGOP8GOP82bNnn9HAAAAAbLZV7pp7tqqevff4c5N8Y5L/uW/Zg0m+c+/xS5P82hjjz1wRBQAAgDMrrPmiJD9dVVuZwvXnxxhvq6ofSHJpjPFgkvuT/ExVPZHpSui33bKJAQAA2GiHhugY4z1J7jng899/1eM/SfKtRzsaAAAAJ9ENvUYUAAAAbpYQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQAANsJymVy4MB3ZbGfWPQAAAMBhlstkZye5ciXZ3k4uXkxms3VPxTPliigAAHDsLRZThO7uTsfFYt0TcTOEKAAAcOzN59OV0K2t6Tifr3uia7OF+HC25gIAAMfebDZtx10spgg9rttybSFejRAFAAA2wmx2/KPuoC3Ex33mdbA1FwAA4Ihs0hbidXJFFAAA4IhsyhbidROiAAAAR2gTthCvm625AAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAG2S5TC5cmI6b6sy6BwAAAGA1y2Wys5NcuZJsbycXLyaz2bqnunGuiAIAAGyIxWKK0N3d6bhYrHuiZ0aIAgAAbIj5fLoSurU1HefzdU/0zNiaCwAAsCFms2k77mIxRegmbstNhCgAAMBGmc02N0A/xdZcAAAAWglRAAAAWglRAAAAWglRAAAAWglRAAAAWglRAADgVFoukwsXpiO9vH0LAABw6iyXyc5OcuVKsr09vTfnpr8lyiZxRRQAADh1FospQnd3p+Nise6JThchCgAAnDrz+XQldGtrOs7n657odLE1FwAAOHVms2k77mIxRahtub2EKAAAcCrNZgJ0XWzNBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQCAU2a5TC5cmI6wDmfWPQAAANBnuUx2dpIrV5Lt7eTixWQ2W/dUnDauiAIAwCmyWEwRurs7HReLdU/EaSREAQDgFJnPpyuhW1vTcT5f90ScRrbmAgDAKTKbTdtxF4spQm3LZR2EKAAAnDKzmQBlvWzNBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoJUQBQAAoNWhIVpVd1XVO6vq8ap6X1V97wFrPr+q/ktV/dbemlfcmnEBAADYdGdWWPN0kteMMR6pqtuTPFxV7xhjPHbVmlcleWyM8S1VdTbJ71TVW8YYV27F0AAAAGyuQ6+IjjGeGmM8svf440keT3Ln/mVJbq+qSvKsJH+UKWABAADgM6xyRfTTqupcknuSPLTvqR9N8mCSDya5PcnfGWN88gjmAwAA4IRZ+WZFVfWsJG9N8uoxxsf2Pf03kzya5C8k+ZokP1pVn3fA17i3qi5V1aXLly/fxNgAAABsqpVCtKpuyxShbxljPHDAklckeWBMnkjy/iR/af+iMcZ9Y4zzY4zzZ8+evZm5AQAA2FCr3DW3ktyf5PExxuuvsewDSXb21v/5JH8xye8d1ZAAAACcHKu8RvQFSV6W5L1V9eje516b5O4kGWO8Mcnrkrypqt6bpJJ83xjjw7dgXgAAADbcoSE6xnhXpri83poPJvmmoxoKAACAk2vlmxUBAADAURCiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAABsrOUyuXBhOgKb48y6BwAAgGdiuUx2dpIrV5Lt7eTixWQ2W/dUwCpcEQUAYCMtFlOE7u5Ox8Vi3RMBqxKiAABspPl8uhK6tTUd5/N1TwSsytZcAAA20mw2bcddLKYItS0XNocQBQBgY81mAhQ2ka25AAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAAAAtBKiAMBNWS6TCxemIwCs4sxhC6rqriRvTvKFST6Z5L4xxhsOWDdP8iNJbkvy4THG1x/tqADAcbNcJjs7yZUryfZ2cvFiMputeyoAjrtVrog+neQ1Y4wvT/L8JK+qquddvaCqnp3kx5O8eIzxFUm+9cgnBQCOncViitDd3em4WKx7IgA2waEhOsZ4aozxyN7jjyd5PMmd+5b93SQPjDE+sLfuQ0c9KABw/Mzn05XQra3pOJ+veyIANsGhW3OvVlXnktyT5KF9T31ZktuqapHk9iRvGGO8+QjmAwCOsdls2o67WEwRalsuAKtYOUSr6llJ3prk1WOMjx3wdf5Kkp0kn5tkWVXvHmP87r6vcW+Se5Pk7rvvvpm5AYBjYjYToADcmJXumltVt2WK0LeMMR44YMmTSX51jPF/xhgfTvLrSb56/6Ixxn1jjPNjjPNnz569mbkBAADYUIeGaFVVkvuTPD7GeP01lv1Skr9eVWeq6s8l+bpMryUFAACAz7DK1twXJHlZkvdW1aN7n3ttkruTZIzxxjHG41X1q0nek+ktXn5yjPHbt2JgAAAANtuhITrGeFeSWmHdDyX5oaMYCgAAjqPl0s254Cjc0F1zAQDgtFouk52d6T1zt7enO0aLUXhmVrpZEQAAnHaLxRShu7vTcbFY90SwuYQoAACsYD6froRubU3H+XzdE8HmsjUXAABWMJtN23G9RhRunhAFAIAVzWYCFI6CrbkAAAC0EqIAAAC0EqIAAAC0EqIAAAC0EqIAAAC0EqIAAAC0EqIAAAC0EqIAAAC0EqIAwJFZLpMLF6YjAFzLmXUPAACcDMtlsrOTXLmSbG8nFy8ms9m6pwLgOHJFFAA4EovFFKG7u9NxsVj3RAAcV0IUADgS8/l0JXRrazrO5+ueCIDjytZcAOBIzGbTdtzFYopQ23IBuBYhCgAcmdlMgAJwOFtzAQAAaCVEAQAAaCVEAeA6vC8mABw9rxEFgGs4ivfFXC7dvAcA9hOiAHANB70v5o3E5FGELACcRLbmAsA13Oz7Yh4UsgCAK6IAcE03+76YnwrZT10RvdGQBYCTSogCwHXczPti3mzIAsBJJUQB4Ba6mZAFgJPKa0QBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBjqHlMrlwYToCAJw0Z9Y9AACfablMdnaSK1eS7e3k4sVkNlv3VHDrLJfJYpHM5851gNNCiAIcM4vFFKG7u9NxsfDDOSeXX7wAnE625gIcM/P59AP51tZ0nM/XPRHcOgf94gWAk88VUYBjZjabrgrZqshp8KlfvHzqiqhfvACcDkIU4BiazQQop4NfvACcTkIUAFgrv3gBOH28RhQAAIBWQhQAAIBWQhQAAIBWQhQAAIBWQhQAAIBWQhQAAIBWQhQAAIBWQhQAAIBWQhQAAIBWQhQAAIBWQhQAAIBWQhQAAIBWQhQAAIBWQhQAAIBWQhQAAIBWQhQAAIBWQhQAAIBWQhQAAIBWh4ZoVd1VVe+sqser6n1V9b3XWftXq2q3ql56tGMCAABwUpxZYc3TSV4zxnikqm5P8nBVvWOM8djVi6pqK8kPJnn7LZgTAACAE+LQK6JjjKfGGI/sPf54kseT3HnA0u9J8tYkHzrSCQEAADhRbug1olV1Lsk9SR7a9/k7k/ytJG88qsEAAAA4mVYO0ap6VqYrnq8eY3xs39M/kuT7xhi7h3yNe6vqUlVdunz58o1PCwAAwMarMcbhi6puS/K2JG8fY7z+gOffn6T2PrwjySeS3DvG+MVrfc3z58+PS5cuPaOhAQAAON6q6uExxvmDnjv0ZkVVVUnuT/L4QRGaJGOML7lq/ZuSvO16EQoAAMDptcpdc1+Q5GVJ3ltVj+597rVJ7k6SMYbXhQIAALCyQ0N0jPGu/P9tt4caY7z8ZgYCAADgZLuhu+YCwLUsl8mFC9MRAOB6VtmaCwDXtVwmOzvJlSvJ9nZy8WIym617KgDguHJFFICbtlhMEbq7Ox0Xi3VPBAAcZ0IUgJs2n09XQre2puN8vu6JAIDjzNZcAG7abDZtx10spgi1LRcAuB4hCsCRmM0EKACwGltzAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAY7QcplcuDAdAQA42Jl1DwBwUiyXyc5OcuVKsr2dXLyYzGbrngoA4PhxRRTgiCwWU4Tu7k7HxWLdEwEAHE9CFOCIzOfTldCtrek4n697IgCA48nWXIAjMptN23EXiylCbcsFADiYEAU4QrOZAAUAOIytuQAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSotxSy2Vy4cJ0BAAASJIz6x6Ak2u5THZ2kitXku3t5OLFZDZb91QAAMC6uSLKLbNYTBG6uzsdF4t1TwQAABwHQpRbZj6froRubU3H+XzdEwEAAMeBrbncMrPZtB13sZgi1LZcAAAgEaLcYrOZAAUAAD6TrbkAAAC0EqIcK97uBQAATj5bczk2vN0LAACcDq6Icmx4uxcAADgdhCjHhrd7AQCA0+HQEK2qu6rqnVX1eFW9r6q+94A1f6+q3rP35zeq6qtvzbicZJ96u5fXvc62XAAAOMlWeY3o00leM8Z4pKpuT/JwVb1jjPHYVWven+TrxxgfqaoXJbkvydfdgnk54bzdCwAAnHyHhugY46kkT+09/nhVPZ7kziSPXbXmN676K+9O8pwjnhMAAIAT4oZeI1pV55Lck+Sh6yx7ZZJfeeYjAQAAcJKt/PYtVfWsJG9N8uoxxseuseYbMoXoX7vG8/cmuTdJ7r777hseFgAAgM230hXRqrotU4S+ZYzxwDXWfFWSn0zykjHGHx60Zoxx3xjj/Bjj/NmzZ5/pzAAAAGywVe6aW0nuT/L4GOP111hzd5IHkrxsjPG7RzsiAAAAJ8kqW3NfkORlSd5bVY/ufe61Se5OkjHGG5N8f5IvSPLjU7fm6THG+aMfFwAAgE23yl1z35WkDlnzXUm+66iGAgAA4OS6obvmAgAAwM0SogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSogAAALQSotewXCYXLkxHAAAAjs6ZdQ9wHC2Xyc5OcuVKsr2dXLyYzGbrngoAAOBkcEX0AIvFFKG7u9NxsVj3RAAAACeHED3AfD5dCd3amo7z+bonAgAAODlszT3AbDZtx10spgi1LRcAAODoHBqiVXVXkjcn+cIkn0xy3xjjDfvWVJI3JPnmJJ9I8vIxxiNHP26f2UyAAgAA3AqrbM19OslrxhhfnuT5SV5VVc/bt+ZFSZ679+feJD9xpFOeYO7OCwAAnDaHXhEdYzyV5Km9xx+vqseT3JnksauWvSTJm8cYI8m7q+rZVfVFe3+Xa3B3XgAA4DS6oZsVVdW5JPckeWjfU3cm+YOrPn5y73Nch7vzAgAAp9HKIVpVz0ry1iSvHmN8bP/TB/yVccDXuLeqLlXVpcuXL9/YpCeQu/MCAACn0Up3za2q2zJF6FvGGA8csOTJJHdd9fFzknxw/6Ixxn1J7kuS8+fP/5lQPW3cnRcAADiNVrlrbiW5P8njY4zXX2PZg0m+u6p+NsnXJfmo14euxt15AQCA02aVK6IvSPKyJO+tqkf3PvfaJHcnyRjjjUl+OdNbtzyR6e1bXnH0owIAAHASrHLX3Hfl4NeAXr1mJHnVUQ0FAADAyXVDd80FAACAmyVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaCVEAQAAaFVjjPV846rLSX6/4VvdkeTDDd8HbpZzlU3hXGVTOFfZFM5VNsEzOU+/eIxx9qAn1haiXarq0hjj/LrngMM4V9kUzlU2hXOVTeFcZRMc9Xlqay4AAACthCgAAACtTkOI3rfuAWBFzlU2hXOVTeFcZVM4V9kER3qenvjXiAIAAHC8nIYrogAAABwjJyZEq+qFVfU7VfVEVf3TA57/7Kr6ub3nH6qqc/1TctqtcJ7+46p6rKreU1UXq+qL1zEnHHauXrXupVU1qsrdHlmLVc7Vqvrbe/+2vq+q/mP3jJCs9DPA3VX1zqr6zb2fA755HXNCVf1UVX2oqn77Gs9XVf27vXP5PVX1l5/J9zkRIVpVW0l+LMmLkjwvybdX1fP2LXtlko+MMb40yQ8n+cHeKTntVjxPfzPJ+THGVyX5hST/pndKWPlcTVXdnuQfJnmod0KYrHKuVtVzk/yzJC8YY3xFkle3D8qpt+K/q/8iyc+PMe5J8m1Jfrx3Svi0NyV54XWef1GS5+79uTfJTzyTb3IiQjTJ1yZ5Yozxe2OMK0l+NslL9q15SZKf3nv8C0l2qqoaZ4RDz9MxxjvHGJ/Y+/DdSZ7TPCMkq/2bmiSvy/TLkj/pHA6ussq5+g+S/NgY4yNJMsb4UPOMkKx2ro4kn7f3+POTfLBxPvi0McavJ/mj6yx5SZI3j8m7kzy7qr7oRr/PSQnRO5P8wVUfP7n3uQPXjDGeTvLRJF/QMh1MVjlPr/bKJL9ySyeCgx16rlbVPUnuGmO8rXMw2GeVf1e/LMmXVdV/q6p3V9X1fssPt8oq5+q/SvIdVfVkkl9O8j09o8ENu9GfaQ905sjGWa+Drmzuvx3wKmvgVlr5HKyq70hyPsnX39KJ4GDXPVer6rMyvcTh5V0DwTWs8u/qmUzbx+aZdpn816r6yjHGH9/i2eBqq5yr357kTWOMf1tVsyQ/s3eufvLWjwc35Ei66qRcEX0yyV1Xffyc/NntDJ9eU1VnMm15uN4lZzhqq5ynqapvTPLPk7x4jPGnTbPB1Q47V29P8pVJFlX1v5I8P8mDbljEGqz6//8vjTH+7xjj/Ul+J1OYQqdVztVXJvn5JBljLJN8TpI7WqaDG7PSz7SHOSkh+j+SPLeqvqSqtjO9wPvBfWseTPKde49fmuTXhjdRpdeh5+nedsd/nylCvY6JdbnuuTrG+OgY444xxrkxxrlMr2d+8Rjj0nrG5RRb5f//X0zyDUlSVXdk2qr7e61Twmrn6geS7CRJVX15phC93DolrObBJH9/7+65z0/y0THGUzf6RU7E1twxxtNV9d1J3p5kK8lPjTHeV1U/kOTSGOPBJPdn2uLwRKYrod+2vok5jVY8T38oybOS/Oe9e2l9YIzx4rUNzam04rkKa7fiufr2JN9UVY8l2U3yT8YYf7i+qTmNVjxXX5PkP1TVP8q0zfHlLpqwDlX1nzK9nOGOvdcs/8sktyXJGOONmV7D/M1JnkjyiSSveEbfx/kNAABAp5OyNRcAAIANIUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABoJUQBAABo9f8ASaNC14O8xngAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1152x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_train, y_train, 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a custom layer for the linear regression model\n",
    "class LinearLayer(Layer):\n",
    "    def __init__(self):\n",
    "        super(LinearLayer, self).__init__()\n",
    "        self.m = self.add_weight(shape=(1, ), initializer='random_normal')\n",
    "        self.b = self.add_weight(shape=(1, ), initializer='zeros')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.m * inputs + self.b\n",
    "    \n",
    "linear_regression = LinearLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
       "array([0.02677927, 0.0499103 , 0.00694781, 0.05119975, 0.02231988,\n",
       "       0.01827421, 0.00265527, 0.00623694, 0.04860107, 0.04246897,\n",
       "       0.0194382 , 0.05286601, 0.0456609 , 0.05133465, 0.04065799,\n",
       "       0.03048198, 0.0257859 , 0.0314845 , 0.03064565, 0.0009532 ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.05466037], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting loss 6.466035\n"
     ]
    }
   ],
   "source": [
    "def SquaredError(y_pred, y_true):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true))\n",
    "\n",
    "starting_loss = SquaredError(linear_regression(X_train), y_train)\n",
    "print('Starting loss', starting_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and plot the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss 6.4660\n",
      "Step 1, Loss 4.8636\n",
      "Step 2, Loss 3.6590\n",
      "Step 3, Loss 2.7534\n",
      "Step 4, Loss 2.0726\n",
      "Step 5, Loss 1.5607\n",
      "Step 6, Loss 1.1760\n",
      "Step 7, Loss 0.8867\n",
      "Step 8, Loss 0.6692\n",
      "Step 9, Loss 0.5057\n",
      "Step 10, Loss 0.3828\n",
      "Step 11, Loss 0.2904\n",
      "Step 12, Loss 0.2209\n",
      "Step 13, Loss 0.1686\n",
      "Step 14, Loss 0.1293\n",
      "Step 15, Loss 0.0997\n",
      "Step 16, Loss 0.0775\n",
      "Step 17, Loss 0.0608\n",
      "Step 18, Loss 0.0482\n",
      "Step 19, Loss 0.0387\n",
      "Step 20, Loss 0.0316\n",
      "Step 21, Loss 0.0262\n",
      "Step 22, Loss 0.0221\n",
      "Step 23, Loss 0.0190\n",
      "Step 24, Loss 0.0167\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.05\n",
    "steps = 25\n",
    "\n",
    "for i in range(steps):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = linear_regression(X_train)\n",
    "        loss = SquaredError(predictions, y_train)\n",
    "        \n",
    "    gradients = tape.gradient(loss, linear_regression.trainable_variables)\n",
    "    \n",
    "    linear_regression.m.assign_sub(learning_rate * gradients[0])\n",
    "    linear_regression.b.assign_sub(learning_rate * gradients[1])\n",
    "    \n",
    "    print('Step {}, Loss {:.4f}'.format(i, loss.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m: 1, trained m: [1.1112217]\n",
      "b: 2, trained b: [1.8627241]\n"
     ]
    }
   ],
   "source": [
    "print('m: {}, trained m: {}'.format(1, linear_regression.m.numpy()))\n",
    "print('b: {}, trained b: {}'.format(2, linear_regression.b.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_train, y_train, 'b.')\n",
    "X_linear_regression = np.linspace(min(X_train), max(X_train), 50)\n",
    "plt.plot(X_linear_regression, linear_regression.m * X_linear_regression + linear_regression.b, 'r.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom training loops\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "my_model = MyModel()\n",
    "\n",
    "def loss(y_pred, y):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    current_loss = loss(my_model(inputs), outputs)\n",
    "    grads = tape.gradient(current_loss, my_model.trainable_variables)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "my_model = MyModel()\n",
    "\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    current_loss = loss(my_model(inputs), outputs)\n",
    "    grads = tape.gradient(current_loss, my_model.trainable_variables)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "my_model = MyModel()\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.05, momentum=0.9)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    current_loss = loss(my_model(inputs), outputs)\n",
    "    grads = tape.gradient(current_loss, my_model.trainable_variables)\n",
    "\n",
    "optimizer.apply_gradients(zip(grads, my_model.trainable_variables))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "my_model = MyModel()\n",
    "\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.05, momentum=0.9)\n",
    "\n",
    "batch_losses = []\n",
    "\n",
    "for inputs, outputs in train_ds:\n",
    "    with tf.GradientTape() as tape:\n",
    "        current_loss = loss(my_model(inputs), outputs)\n",
    "        grads = tape.gradient(current_loss, my_model.trainable_variables)\n",
    "    batch_losses.append(current_loss)\n",
    "    optimizer.apply_gradients(zip(grads, my_model.trainable_variables)) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "my_model = MyModel()\n",
    "\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.05, momentum=0.9)\n",
    "\n",
    "\n",
    "epoch_losses = []\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    batch_losses = []\n",
    "\n",
    "    for inputs, outputs in train_ds:\n",
    "        with tf.GradientTape() as tape:\n",
    "            current_loss = loss(my_model(inputs), outputs)\n",
    "            grads = tape.gradient(current_loss, my_model.trainable_variables)\n",
    "        batch_losses.append(current_loss)\n",
    "        optimizer.apply_gradients(zip(grads, my_model.trainable_variables)) \n",
    "        \n",
    "    epoch_losses.append(np.mean(batch_losses))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
