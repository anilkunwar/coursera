{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequencing Modeling\n",
    "\n",
    "> In this post, it will cover various network architectures and layers that we can use to make predictions from sequence data. This is the summary of lecture \"Customizing your model with Tensorflow 2\" from Coursera.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Chanseok Kang\n",
    "- categories: [Python, Coursera, Deep_Learning, Tensorflow]\n",
    "- image: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (16, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing sequence data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  4, 12, 33, 18],\n",
       "       [63, 23, 54, 30, 19,  3],\n",
       "       [ 0, 43, 37, 11, 33, 15]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "test_data = [\n",
    "    [4, 12, 33, 18],\n",
    "    [63, 23, 54, 30, 19, 3],\n",
    "    [43, 37, 11, 33, 15]\n",
    "]\n",
    "\n",
    "preprocessed_data = pad_sequences(test_data, padding='pre')\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### post padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4, 12, 33, 18,  0,  0],\n",
       "       [63, 23, 54, 30, 19,  3],\n",
       "       [43, 37, 11, 33, 15,  0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data = pad_sequences(test_data, padding='post')\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post padding with maximum length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4, 12, 33, 18,  0],\n",
       "       [23, 54, 30, 19,  3],\n",
       "       [43, 37, 11, 33, 15]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data = pad_sequences(test_data, padding='post', maxlen=5)\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post padding with truncating (Default: 'pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4, 12, 33, 18,  0],\n",
       "       [63, 23, 54, 30, 19],\n",
       "       [43, 37, 11, 33, 15]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data = pad_sequences(test_data, padding='post', maxlen=5, truncating='post')\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post padding with truncating, then filled with value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4, 12, 33, 18, -1],\n",
       "       [63, 23, 54, 30, 19],\n",
       "       [43, 37, 11, 33, 15]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data = pad_sequences(test_data, padding='post', maxlen=5, truncating='post', value=-1)\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example in 2d array sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2, 1],\n",
       "        [3, 3],\n",
       "        [0, 0]],\n",
       "\n",
       "       [[4, 3],\n",
       "        [2, 4],\n",
       "        [1, 1]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = [\n",
    "    [[2, 1], [3, 3]],\n",
    "    [[4, 3], [2, 4], [1, 1]]\n",
    "]\n",
    "\n",
    "preprocessed_data = pad_sequences(test_input, padding='post')\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masking Layer for mask specific sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 6, 1), dtype=int32, numpy=\n",
       "array([[[ 4],\n",
       "        [12],\n",
       "        [33],\n",
       "        [18],\n",
       "        [ 0],\n",
       "        [ 0]],\n",
       "\n",
       "       [[63],\n",
       "        [23],\n",
       "        [54],\n",
       "        [30],\n",
       "        [19],\n",
       "        [ 3]],\n",
       "\n",
       "       [[43],\n",
       "        [37],\n",
       "        [11],\n",
       "        [33],\n",
       "        [15],\n",
       "        [ 0]]])>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Masking\n",
    "\n",
    "preprocessed_data = pad_sequences(test_data, padding='post')\n",
    "\n",
    "masking_layer = Masking(mask_value=0)\n",
    "preprocessed_data = preprocessed_data[..., tf.newaxis] # (batch_size, seq_length, features)\n",
    "masked_input = masking_layer(preprocessed_data)\n",
    "masked_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 6), dtype=bool, numpy=\n",
       "array([[ True,  True,  True,  True, False, False],\n",
       "       [ True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True, False]])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_input._keras_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the IMDB review sentiment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 22665,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 21631,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 31050,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset with different options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([list([1, 14, 22, 16, 43, 2, 2, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 2, 112, 50, 2, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 2, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 2, 38, 76, 15, 13, 2, 4, 22, 17, 2, 17, 12, 16, 2, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 2, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 2, 36, 71, 43, 2, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]),\n",
       "         list([1, 194, 2, 194, 2, 78, 228, 5, 6, 2, 2, 2, 134, 26, 4, 2, 8, 118, 2, 14, 394, 20, 13, 119, 2, 189, 102, 5, 207, 110, 2, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2, 2, 5, 2, 4, 116, 9, 35, 2, 4, 229, 9, 340, 2, 4, 118, 9, 4, 130, 2, 19, 4, 2, 5, 89, 29, 2, 46, 37, 4, 455, 9, 45, 43, 38, 2, 2, 398, 4, 2, 26, 2, 5, 163, 11, 2, 2, 4, 2, 9, 194, 2, 7, 2, 2, 349, 2, 148, 2, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 2, 98, 5, 4, 228, 9, 43, 2, 2, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 2, 228, 2, 5, 2, 2, 245, 2, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 2, 14, 9, 6, 371, 78, 22, 2, 64, 2, 9, 8, 168, 145, 23, 4, 2, 15, 16, 4, 2, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "         list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2, 311, 12, 16, 2, 33, 75, 43, 2, 296, 4, 86, 320, 35, 2, 19, 263, 2, 2, 4, 2, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 2, 43, 2, 2, 8, 257, 85, 2, 42, 2, 2, 83, 68, 2, 15, 36, 165, 2, 278, 36, 69, 2, 2, 8, 106, 14, 2, 2, 18, 6, 22, 12, 215, 28, 2, 40, 6, 87, 326, 23, 2, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2, 51, 9, 170, 23, 2, 116, 2, 2, 13, 191, 79, 2, 89, 2, 14, 9, 8, 106, 2, 2, 35, 2, 6, 227, 7, 129, 113]),\n",
       "         ...,\n",
       "         list([1, 11, 6, 230, 245, 2, 9, 6, 2, 446, 2, 45, 2, 84, 2, 2, 21, 4, 2, 84, 2, 325, 2, 134, 2, 2, 84, 5, 36, 28, 57, 2, 21, 8, 140, 8, 2, 5, 2, 84, 56, 18, 2, 14, 9, 31, 7, 4, 2, 2, 2, 2, 2, 18, 6, 20, 207, 110, 2, 12, 8, 2, 2, 8, 97, 6, 20, 53, 2, 74, 4, 460, 364, 2, 29, 270, 11, 2, 108, 45, 40, 29, 2, 395, 11, 6, 2, 2, 7, 2, 89, 364, 70, 29, 140, 4, 64, 2, 11, 4, 2, 26, 178, 4, 2, 443, 2, 5, 27, 2, 117, 2, 2, 165, 47, 84, 37, 131, 2, 14, 2, 10, 10, 61, 2, 2, 10, 10, 288, 2, 2, 34, 2, 2, 4, 65, 496, 4, 231, 7, 2, 5, 6, 320, 234, 2, 234, 2, 2, 7, 496, 4, 139, 2, 2, 2, 2, 5, 2, 18, 4, 2, 2, 250, 11, 2, 2, 4, 2, 2, 2, 2, 372, 2, 2, 2, 2, 7, 4, 59, 2, 4, 2, 2]),\n",
       "         list([1, 2, 2, 69, 72, 2, 13, 2, 2, 8, 12, 2, 23, 5, 16, 484, 2, 54, 349, 11, 2, 2, 45, 58, 2, 13, 197, 12, 16, 43, 23, 2, 5, 62, 30, 145, 402, 11, 2, 51, 2, 32, 61, 369, 71, 66, 2, 12, 2, 75, 100, 2, 8, 4, 105, 37, 69, 147, 2, 75, 2, 44, 257, 390, 5, 69, 263, 2, 105, 50, 286, 2, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 2, 13, 2, 40, 319, 2, 112, 2, 11, 2, 121, 25, 70, 2, 4, 2, 2, 13, 18, 31, 62, 40, 8, 2, 4, 2, 7, 14, 123, 5, 2, 25, 8, 2, 12, 145, 5, 202, 12, 160, 2, 202, 12, 6, 52, 58, 2, 92, 401, 2, 12, 39, 14, 251, 8, 15, 251, 5, 2, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
       "         list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 2, 270, 2, 5, 2, 2, 2, 2, 101, 405, 39, 14, 2, 4, 2, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 2, 102, 7, 4, 2, 2, 9, 24, 6, 78, 2, 17, 2, 2, 21, 27, 2, 2, 5, 2, 2, 92, 2, 4, 2, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 2, 2, 9, 6, 66, 78, 2, 4, 2, 2, 5, 2, 272, 191, 2, 6, 2, 8, 2, 2, 2, 2, 5, 383, 2, 2, 2, 2, 497, 2, 8, 2, 2, 2, 21, 60, 27, 239, 9, 43, 2, 209, 405, 10, 10, 12, 2, 40, 4, 248, 20, 12, 16, 5, 174, 2, 72, 7, 51, 6, 2, 22, 4, 204, 131, 9])],\n",
       "        dtype=object),\n",
       "  array([1, 0, 0, ..., 0, 1, 0], dtype=int64)),\n",
       " (array([list([1, 2, 202, 14, 31, 6, 2, 10, 10, 2, 2, 5, 4, 360, 7, 4, 177, 2, 394, 354, 4, 123, 9, 2, 2, 2, 10, 10, 13, 92, 124, 89, 488, 2, 100, 28, 2, 14, 31, 23, 27, 2, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 2, 38, 32, 25, 2, 451, 202, 14, 6, 2]),\n",
       "         list([1, 14, 22, 2, 6, 176, 7, 2, 88, 12, 2, 23, 2, 5, 109, 2, 4, 114, 9, 55, 2, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 2, 2, 4, 2, 2, 109, 2, 21, 4, 22, 2, 8, 6, 2, 2, 10, 10, 4, 105, 2, 35, 2, 2, 19, 2, 2, 5, 2, 2, 45, 55, 221, 15, 2, 2, 2, 14, 2, 4, 405, 5, 2, 7, 27, 85, 108, 131, 4, 2, 2, 2, 405, 9, 2, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 2, 239, 34, 2, 2, 45, 407, 31, 7, 41, 2, 105, 21, 59, 299, 12, 38, 2, 5, 2, 15, 45, 2, 488, 2, 127, 6, 52, 292, 17, 4, 2, 185, 132, 2, 2, 2, 488, 2, 47, 6, 392, 173, 4, 2, 2, 270, 2, 4, 2, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 2, 2, 7, 2, 2, 2, 5, 2, 30, 2, 2, 56, 4, 2, 5, 2, 2, 8, 4, 2, 398, 229, 10, 10, 13, 2, 2, 2, 14, 9, 31, 7, 27, 111, 108, 15, 2, 19, 2, 2, 2, 2, 14, 22, 9, 2, 21, 45, 2, 5, 45, 252, 8, 2, 6, 2, 2, 2, 39, 4, 2, 48, 25, 181, 8, 67, 35, 2, 22, 49, 238, 60, 135, 2, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 2, 8, 169, 11, 374, 2, 25, 203, 28, 8, 2, 12, 125, 4, 2]),\n",
       "         list([1, 111, 2, 2, 2, 2, 2, 4, 87, 2, 2, 7, 31, 318, 2, 7, 4, 498, 2, 2, 63, 29, 2, 220, 2, 2, 5, 17, 12, 2, 220, 2, 17, 6, 185, 132, 2, 16, 53, 2, 11, 2, 74, 4, 438, 21, 27, 2, 2, 8, 22, 107, 2, 2, 2, 2, 8, 35, 2, 2, 11, 22, 231, 54, 29, 2, 29, 100, 2, 2, 34, 2, 2, 2, 5, 2, 98, 31, 2, 33, 6, 58, 14, 2, 2, 8, 4, 365, 7, 2, 2, 356, 346, 4, 2, 2, 63, 29, 93, 11, 2, 11, 2, 33, 6, 58, 54, 2, 431, 2, 7, 32, 2, 16, 11, 94, 2, 10, 10, 4, 2, 2, 7, 4, 2, 2, 2, 2, 8, 2, 8, 2, 121, 31, 7, 27, 86, 2, 2, 16, 6, 465, 2, 2, 2, 2, 17, 2, 42, 4, 2, 37, 473, 6, 2, 6, 2, 7, 328, 212, 70, 30, 258, 11, 220, 32, 7, 108, 21, 133, 12, 9, 55, 465, 2, 2, 53, 33, 2, 2, 37, 70, 2, 4, 2, 2, 74, 476, 37, 62, 91, 2, 169, 4, 2, 2, 146, 2, 2, 5, 258, 12, 184, 2, 2, 5, 2, 2, 7, 4, 22, 2, 18, 2, 2, 2, 7, 4, 2, 71, 348, 425, 2, 2, 19, 2, 5, 2, 11, 2, 8, 339, 2, 4, 2, 2, 7, 4, 2, 10, 10, 263, 2, 9, 270, 11, 6, 2, 4, 2, 2, 121, 4, 2, 26, 2, 19, 68, 2, 5, 28, 446, 6, 318, 2, 8, 67, 51, 36, 70, 81, 8, 2, 2, 36, 2, 8, 2, 2, 18, 6, 2, 4, 2, 26, 2, 2, 11, 14, 2, 2, 12, 426, 28, 77, 2, 8, 97, 38, 111, 2, 2, 168, 2, 2, 137, 2, 18, 27, 173, 9, 2, 17, 6, 2, 428, 2, 232, 11, 4, 2, 37, 272, 40, 2, 247, 30, 2, 6, 2, 54, 2, 2, 98, 6, 2, 40, 2, 37, 2, 98, 4, 2, 2, 15, 14, 9, 57, 2, 5, 2, 6, 275, 2, 2, 2, 2, 98, 6, 2, 10, 10, 2, 19, 14, 2, 267, 162, 2, 37, 2, 2, 98, 4, 2, 2, 90, 19, 6, 2, 7, 2, 2, 2, 4, 2, 2, 2, 8, 2, 90, 4, 2, 8, 4, 2, 17, 2, 2, 2, 4, 2, 8, 2, 189, 4, 2, 2, 2, 4, 2, 5, 95, 271, 23, 6, 2, 2, 2, 2, 33, 2, 6, 425, 2, 2, 2, 2, 7, 4, 2, 2, 469, 4, 2, 54, 4, 150, 2, 2, 280, 53, 2, 2, 18, 339, 29, 2, 27, 2, 5, 2, 68, 2, 19, 2, 2, 4, 2, 7, 263, 65, 2, 34, 6, 2, 2, 43, 159, 29, 9, 2, 9, 387, 73, 195, 2, 10, 10, 2, 4, 58, 2, 54, 14, 2, 117, 22, 16, 93, 5, 2, 4, 192, 15, 12, 16, 93, 34, 6, 2, 2, 33, 4, 2, 7, 15, 2, 2, 2, 325, 12, 62, 30, 2, 8, 67, 14, 17, 6, 2, 44, 148, 2, 2, 203, 42, 203, 24, 28, 69, 2, 2, 11, 330, 54, 29, 93, 2, 21, 2, 2, 27, 2, 7, 2, 4, 22, 2, 17, 6, 2, 2, 7, 2, 2, 2, 100, 30, 4, 2, 2, 2, 2, 42, 2, 11, 4, 2, 42, 101, 2, 7, 101, 2, 15, 2, 94, 2, 180, 5, 9, 2, 34, 2, 45, 6, 2, 22, 60, 6, 2, 31, 11, 94, 2, 96, 21, 94, 2, 9, 57, 2]),\n",
       "         ...,\n",
       "         list([1, 13, 2, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 2, 21, 45, 184, 78, 4, 2, 2, 2, 2, 2, 395, 2, 5, 2, 11, 119, 2, 89, 2, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2, 284, 2, 2, 37, 315, 4, 226, 20, 272, 2, 40, 29, 152, 60, 181, 8, 30, 50, 2, 362, 80, 119, 12, 21, 2, 2]),\n",
       "         list([1, 11, 119, 241, 9, 4, 2, 20, 12, 468, 15, 94, 2, 2, 2, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2, 7, 2, 46, 2, 9, 2, 5, 4, 2, 47, 8, 79, 90, 145, 164, 162, 50, 6, 2, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 2, 200, 5, 2, 5, 9, 2, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 2, 92, 124, 51, 45, 2, 71, 2, 13, 2, 14, 20, 6, 2, 7, 470]),\n",
       "         list([1, 6, 52, 2, 430, 22, 9, 220, 2, 8, 28, 2, 2, 2, 6, 2, 15, 47, 6, 2, 2, 8, 114, 5, 33, 222, 31, 55, 184, 2, 2, 2, 19, 346, 2, 5, 6, 364, 350, 4, 184, 2, 9, 133, 2, 11, 2, 2, 21, 4, 2, 2, 2, 50, 2, 2, 9, 6, 2, 17, 6, 2, 2, 21, 17, 6, 2, 232, 2, 2, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 2, 19, 4, 78, 173, 7, 27, 2, 2, 2, 2, 2, 9, 6, 2, 17, 210, 5, 2, 2, 47, 77, 395, 14, 172, 173, 18, 2, 2, 2, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 2, 53, 40, 35, 390, 7, 11, 4, 2, 7, 4, 314, 74, 6, 2, 22, 2, 19, 2, 2, 2, 382, 4, 91, 2, 439, 19, 14, 20, 9, 2, 2, 2, 4, 2, 25, 124, 4, 31, 12, 16, 93, 2, 34, 2, 2])],\n",
       "        dtype=object),\n",
       "  array([0, 1, 1, ..., 0, 0, 0], dtype=int64)))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limit the vocabulary to the top 500 words using num_words\n",
    "imdb.load_data(num_words=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([list([2, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 2, 173, 36, 256, 2, 25, 100, 43, 838, 112, 50, 670, 2, 2, 35, 480, 284, 2, 150, 2, 172, 112, 167, 2, 336, 385, 39, 2, 172, 2, 2, 17, 546, 38, 13, 447, 2, 192, 50, 16, 2, 147, 2, 19, 14, 22, 2, 2, 2, 469, 2, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 2, 22, 17, 515, 17, 12, 16, 626, 18, 2, 2, 62, 386, 12, 2, 316, 2, 106, 2, 2, 2, 2, 16, 480, 66, 2, 33, 2, 130, 12, 16, 38, 619, 2, 25, 124, 51, 36, 135, 48, 25, 2, 33, 2, 22, 12, 215, 28, 77, 52, 2, 14, 407, 16, 82, 2, 2, 2, 107, 117, 2, 15, 256, 2, 2, 2, 2, 2, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 2, 2, 2, 2, 13, 104, 88, 2, 381, 15, 297, 98, 32, 2, 56, 26, 141, 2, 194, 2, 18, 2, 226, 22, 21, 134, 476, 26, 480, 2, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 2, 226, 65, 16, 38, 2, 88, 12, 16, 283, 2, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]),\n",
       "         list([2, 194, 2, 194, 2, 78, 228, 2, 2, 2, 2, 2, 134, 26, 2, 715, 2, 118, 2, 14, 394, 20, 13, 119, 954, 189, 102, 2, 207, 110, 2, 21, 14, 69, 188, 2, 30, 23, 2, 2, 249, 126, 93, 2, 114, 2, 2, 2, 2, 647, 2, 116, 2, 35, 2, 2, 229, 2, 340, 2, 2, 118, 2, 2, 130, 2, 19, 2, 2, 2, 89, 29, 952, 46, 37, 2, 455, 2, 45, 43, 38, 2, 2, 398, 2, 2, 26, 2, 2, 163, 11, 2, 2, 2, 2, 2, 194, 775, 2, 2, 2, 349, 2, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 2, 98, 2, 2, 228, 2, 43, 2, 2, 15, 299, 120, 2, 120, 174, 11, 220, 175, 136, 50, 2, 2, 228, 2, 2, 2, 656, 245, 2, 2, 2, 2, 131, 152, 491, 18, 2, 32, 2, 2, 14, 2, 2, 371, 78, 22, 625, 64, 2, 2, 2, 168, 145, 23, 2, 2, 15, 16, 2, 2, 2, 28, 2, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "         list([2, 14, 47, 2, 30, 31, 2, 2, 249, 108, 2, 2, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 2, 2, 311, 12, 16, 2, 33, 75, 43, 2, 296, 2, 86, 320, 35, 534, 19, 263, 2, 2, 2, 2, 33, 89, 78, 12, 66, 16, 2, 360, 2, 2, 58, 316, 334, 11, 2, 2, 43, 645, 662, 2, 257, 85, 2, 42, 2, 2, 83, 68, 2, 15, 36, 165, 2, 278, 36, 69, 2, 780, 2, 106, 14, 2, 2, 18, 2, 22, 12, 215, 28, 610, 40, 2, 87, 326, 23, 2, 21, 23, 22, 12, 272, 40, 57, 31, 11, 2, 22, 47, 2, 2, 51, 2, 170, 23, 595, 116, 595, 2, 13, 191, 79, 638, 89, 2, 14, 2, 2, 106, 607, 624, 35, 534, 2, 227, 2, 129, 113]),\n",
       "         ...,\n",
       "         list([2, 11, 2, 230, 245, 2, 2, 2, 2, 446, 2, 45, 2, 84, 2, 2, 21, 2, 912, 84, 2, 325, 725, 134, 2, 2, 84, 2, 36, 28, 57, 2, 21, 2, 140, 2, 703, 2, 2, 84, 56, 18, 2, 14, 2, 31, 2, 2, 2, 2, 2, 2, 2, 18, 2, 20, 207, 110, 563, 12, 2, 2, 2, 2, 97, 2, 20, 53, 2, 74, 2, 460, 364, 2, 29, 270, 11, 960, 108, 45, 40, 29, 2, 395, 11, 2, 2, 500, 2, 2, 89, 364, 70, 29, 140, 2, 64, 2, 11, 2, 2, 26, 178, 2, 529, 443, 2, 2, 27, 710, 117, 2, 2, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 2, 2, 10, 10, 288, 2, 2, 34, 2, 2, 2, 65, 496, 2, 231, 2, 790, 2, 2, 320, 234, 2, 234, 2, 2, 2, 496, 2, 139, 929, 2, 2, 2, 2, 2, 18, 2, 2, 2, 250, 11, 2, 2, 2, 2, 2, 747, 2, 372, 2, 2, 541, 2, 2, 2, 59, 2, 2, 2, 2]),\n",
       "         list([2, 2, 2, 69, 72, 2, 13, 610, 930, 2, 12, 582, 23, 2, 16, 484, 685, 54, 349, 11, 2, 2, 45, 58, 2, 13, 197, 12, 16, 43, 23, 2, 2, 62, 30, 145, 402, 11, 2, 51, 575, 32, 61, 369, 71, 66, 770, 12, 2, 75, 100, 2, 2, 2, 105, 37, 69, 147, 712, 75, 2, 44, 257, 390, 2, 69, 263, 514, 105, 50, 286, 2, 23, 2, 123, 13, 161, 40, 2, 421, 2, 116, 16, 897, 13, 2, 40, 319, 2, 112, 2, 11, 2, 121, 25, 70, 2, 2, 719, 2, 13, 18, 31, 62, 40, 2, 2, 2, 2, 2, 14, 123, 2, 942, 25, 2, 721, 12, 145, 2, 202, 12, 160, 580, 202, 12, 2, 52, 58, 2, 92, 401, 728, 12, 39, 14, 251, 2, 15, 251, 2, 2, 12, 38, 84, 80, 124, 12, 2, 23]),\n",
       "         list([2, 17, 2, 194, 337, 2, 2, 204, 22, 45, 254, 2, 106, 14, 123, 2, 2, 270, 2, 2, 2, 2, 732, 2, 101, 405, 39, 14, 2, 2, 2, 2, 115, 50, 305, 12, 47, 2, 168, 2, 235, 2, 38, 111, 699, 102, 2, 2, 2, 2, 2, 24, 2, 78, 2, 17, 2, 2, 21, 27, 2, 2, 2, 2, 2, 92, 2, 2, 2, 2, 2, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 2, 97, 12, 157, 21, 2, 2, 2, 2, 66, 78, 2, 2, 631, 2, 2, 2, 272, 191, 2, 2, 2, 2, 2, 2, 2, 544, 2, 383, 2, 848, 2, 2, 497, 2, 2, 2, 2, 2, 21, 60, 27, 239, 2, 43, 2, 209, 405, 10, 10, 12, 764, 40, 2, 248, 20, 12, 16, 2, 174, 2, 72, 2, 51, 2, 2, 22, 2, 204, 131, 2])],\n",
       "        dtype=object),\n",
       "  array([1, 0, 0, ..., 0, 1, 0], dtype=int64)),\n",
       " (array([list([2, 591, 202, 14, 31, 2, 717, 10, 10, 2, 2, 2, 2, 360, 2, 2, 177, 2, 394, 354, 2, 123, 2, 2, 2, 2, 10, 10, 13, 92, 124, 89, 488, 2, 100, 28, 2, 14, 31, 23, 27, 2, 29, 220, 468, 2, 124, 14, 286, 170, 2, 157, 46, 2, 27, 239, 16, 179, 2, 38, 32, 25, 2, 451, 202, 14, 2, 717]),\n",
       "         list([2, 14, 22, 2, 2, 176, 2, 2, 88, 12, 2, 23, 2, 2, 109, 943, 2, 114, 2, 55, 606, 2, 111, 2, 2, 139, 193, 273, 23, 2, 172, 270, 11, 2, 2, 2, 2, 2, 109, 2, 21, 2, 22, 2, 2, 2, 2, 2, 10, 10, 2, 105, 987, 35, 841, 2, 19, 861, 2, 2, 2, 2, 45, 55, 221, 15, 670, 2, 526, 14, 2, 2, 405, 2, 2, 2, 27, 85, 108, 131, 2, 2, 2, 2, 405, 2, 2, 133, 2, 50, 13, 104, 51, 66, 166, 14, 22, 157, 2, 2, 530, 239, 34, 2, 2, 45, 407, 31, 2, 41, 2, 105, 21, 59, 299, 12, 38, 950, 2, 2, 15, 45, 629, 488, 2, 127, 2, 52, 292, 17, 2, 2, 185, 132, 2, 2, 2, 488, 2, 47, 2, 392, 173, 2, 2, 2, 270, 2, 2, 2, 2, 2, 65, 55, 73, 11, 346, 14, 20, 2, 2, 976, 2, 2, 2, 861, 2, 2, 2, 30, 2, 2, 56, 2, 841, 2, 990, 692, 2, 2, 2, 398, 229, 10, 10, 13, 2, 670, 2, 14, 2, 31, 2, 27, 111, 108, 15, 2, 19, 2, 2, 875, 551, 14, 22, 2, 2, 21, 45, 2, 2, 45, 252, 2, 2, 2, 565, 921, 2, 39, 2, 529, 48, 25, 181, 2, 67, 35, 2, 22, 49, 238, 60, 135, 2, 14, 2, 290, 2, 58, 10, 10, 472, 45, 55, 878, 2, 169, 11, 374, 2, 25, 203, 28, 2, 818, 12, 125, 2, 2]),\n",
       "         list([2, 111, 748, 2, 2, 2, 2, 2, 87, 2, 2, 2, 31, 318, 2, 2, 2, 498, 2, 748, 63, 29, 2, 220, 686, 2, 2, 17, 12, 575, 220, 2, 17, 2, 185, 132, 2, 16, 53, 928, 11, 2, 74, 2, 438, 21, 27, 2, 589, 2, 22, 107, 2, 2, 997, 2, 2, 35, 2, 2, 11, 22, 231, 54, 29, 2, 29, 100, 2, 2, 34, 2, 2, 2, 2, 2, 98, 31, 2, 33, 2, 58, 14, 2, 2, 2, 2, 365, 2, 2, 2, 356, 346, 2, 2, 2, 63, 29, 93, 11, 2, 11, 2, 33, 2, 58, 54, 2, 431, 748, 2, 32, 2, 16, 11, 94, 2, 10, 10, 2, 993, 2, 2, 2, 2, 2, 2, 2, 2, 847, 2, 2, 121, 31, 2, 27, 86, 2, 2, 16, 2, 465, 993, 2, 2, 573, 17, 2, 42, 2, 2, 37, 473, 2, 711, 2, 2, 2, 328, 212, 70, 30, 258, 11, 220, 32, 2, 108, 21, 133, 12, 2, 55, 465, 849, 2, 53, 33, 2, 2, 37, 70, 2, 2, 2, 2, 74, 476, 37, 62, 91, 2, 169, 2, 2, 2, 146, 655, 2, 2, 258, 12, 184, 2, 546, 2, 849, 2, 2, 2, 22, 2, 18, 631, 2, 797, 2, 2, 2, 71, 348, 425, 2, 2, 19, 2, 2, 2, 11, 661, 2, 339, 2, 2, 2, 2, 2, 2, 2, 10, 10, 263, 787, 2, 270, 11, 2, 2, 2, 2, 2, 121, 2, 2, 26, 2, 19, 68, 2, 2, 28, 446, 2, 318, 2, 2, 67, 51, 36, 70, 81, 2, 2, 2, 36, 2, 2, 2, 2, 18, 2, 711, 2, 2, 26, 2, 2, 11, 14, 636, 720, 12, 426, 28, 77, 776, 2, 97, 38, 111, 2, 2, 168, 2, 2, 137, 2, 18, 27, 173, 2, 2, 17, 2, 2, 428, 2, 232, 11, 2, 2, 37, 272, 40, 2, 247, 30, 656, 2, 2, 54, 2, 2, 98, 2, 2, 40, 558, 37, 2, 98, 2, 2, 2, 15, 14, 2, 57, 2, 2, 2, 2, 275, 711, 2, 2, 2, 98, 2, 2, 10, 10, 2, 19, 14, 2, 267, 162, 711, 37, 2, 752, 98, 2, 2, 2, 90, 19, 2, 2, 2, 2, 2, 2, 2, 2, 2, 930, 2, 508, 90, 2, 2, 2, 2, 2, 17, 2, 2, 2, 2, 2, 2, 2, 189, 2, 2, 2, 2, 2, 2, 2, 95, 271, 23, 2, 2, 2, 2, 2, 33, 2, 2, 425, 2, 2, 2, 2, 2, 2, 2, 2, 469, 2, 2, 54, 2, 150, 2, 2, 280, 53, 2, 2, 18, 339, 29, 2, 27, 2, 2, 2, 68, 2, 19, 2, 2, 2, 2, 2, 263, 65, 2, 34, 2, 2, 2, 43, 159, 29, 2, 2, 2, 387, 73, 195, 584, 10, 10, 2, 2, 58, 810, 54, 14, 2, 117, 22, 16, 93, 2, 2, 2, 192, 15, 12, 16, 93, 34, 2, 2, 2, 33, 2, 2, 2, 15, 2, 2, 2, 325, 12, 62, 30, 776, 2, 67, 14, 17, 2, 2, 44, 148, 687, 2, 203, 42, 203, 24, 28, 69, 2, 2, 11, 330, 54, 29, 93, 2, 21, 845, 2, 27, 2, 2, 819, 2, 22, 2, 17, 2, 2, 787, 2, 2, 2, 2, 100, 30, 2, 2, 2, 2, 2, 42, 2, 11, 2, 2, 42, 101, 704, 2, 101, 999, 15, 2, 94, 2, 180, 2, 2, 2, 34, 2, 45, 2, 2, 22, 60, 2, 2, 31, 11, 94, 2, 96, 21, 94, 749, 2, 57, 975]),\n",
       "         ...,\n",
       "         list([2, 13, 2, 15, 2, 135, 14, 2, 35, 32, 46, 394, 20, 62, 30, 2, 21, 45, 184, 78, 2, 2, 910, 769, 2, 2, 395, 2, 2, 2, 11, 119, 2, 89, 2, 2, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 2, 185, 2, 284, 2, 2, 37, 315, 2, 226, 20, 272, 2, 40, 29, 152, 60, 181, 2, 30, 50, 553, 362, 80, 119, 12, 21, 846, 2]),\n",
       "         list([2, 11, 119, 241, 2, 2, 840, 20, 12, 468, 15, 94, 2, 562, 791, 39, 2, 86, 107, 2, 97, 14, 31, 33, 2, 2, 2, 743, 46, 2, 2, 2, 2, 2, 768, 47, 2, 79, 90, 145, 164, 162, 50, 2, 501, 119, 2, 2, 2, 78, 232, 15, 16, 224, 11, 2, 333, 20, 2, 985, 200, 2, 2, 2, 2, 2, 2, 79, 357, 2, 20, 47, 220, 57, 206, 139, 11, 12, 2, 55, 117, 212, 13, 2, 92, 124, 51, 45, 2, 71, 536, 13, 520, 14, 20, 2, 2, 2, 470]),\n",
       "         list([2, 2, 52, 2, 430, 22, 2, 220, 2, 2, 28, 2, 519, 2, 2, 769, 15, 47, 2, 2, 2, 2, 114, 2, 33, 222, 31, 55, 184, 704, 2, 2, 19, 346, 2, 2, 2, 364, 350, 2, 184, 2, 2, 133, 2, 11, 2, 2, 21, 2, 2, 2, 570, 50, 2, 2, 2, 2, 2, 17, 2, 2, 2, 21, 17, 2, 2, 232, 2, 2, 29, 266, 56, 96, 346, 194, 308, 2, 194, 21, 29, 218, 2, 19, 2, 78, 173, 2, 27, 2, 2, 2, 718, 2, 2, 2, 2, 17, 210, 2, 2, 2, 47, 77, 395, 14, 172, 173, 18, 2, 2, 2, 82, 127, 27, 173, 11, 2, 392, 217, 21, 50, 2, 57, 65, 12, 2, 53, 40, 35, 390, 2, 11, 2, 2, 2, 2, 314, 74, 2, 792, 22, 2, 19, 714, 727, 2, 382, 2, 91, 2, 439, 19, 14, 20, 2, 2, 2, 2, 2, 756, 25, 124, 2, 31, 12, 16, 93, 804, 34, 2, 2])],\n",
       "        dtype=object),\n",
       "  array([0, 1, 1, ..., 0, 0, 0], dtype=int64)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ignore the top 10 most frequent words\n",
    "imdb.load_data(skip_top=10, num_words=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
       "         list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "         list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
       "         ...,\n",
       "         list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 86527, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 14532, 325, 725, 134, 15271, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 11656, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 26094, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 17793, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 14492, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 17793, 5, 27, 710, 117, 74936, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 17793, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 17793, 7750, 5, 4241, 18, 4, 8497, 13164, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 11027, 4, 3586, 22459]),\n",
       "         list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 21469, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 40691, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 29455, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 11418, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 21213, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
       "         list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 12815, 270, 14437, 5, 16923, 12255, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 16553, 21, 27, 9685, 6139, 5, 29043, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 85010, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 70907, 10755, 544, 5, 383, 1271, 848, 1468, 12183, 497, 16876, 8, 1597, 8778, 19280, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
       "        dtype=object),\n",
       "  array([1, 0, 0, ..., 0, 1, 0], dtype=int64)),\n",
       " (array([list([1, 591, 202, 14, 31, 6, 717, 10, 10, 18142, 10698, 5, 4, 360, 7, 4, 177, 5760, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 7944, 100, 28, 1668, 14, 31, 23, 27, 7479, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 15387, 38, 32, 25, 7944, 451, 202, 14, 6, 717]),\n",
       "         list([1, 14, 22, 3443, 6, 176, 7, 5063, 88, 12, 2679, 23, 1310, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 7216, 10626, 4, 8463, 2801, 109, 1603, 21, 4, 22, 3861, 8, 6, 1193, 1330, 10, 10, 4, 105, 987, 35, 841, 16873, 19, 861, 1074, 5, 1987, 17975, 45, 55, 221, 15, 670, 5304, 526, 14, 1069, 4, 405, 5, 2438, 7, 27, 85, 108, 131, 4, 5045, 5304, 3884, 405, 9, 3523, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 8463, 2801, 45, 407, 31, 7, 41, 3778, 105, 21, 59, 299, 12, 38, 950, 5, 4521, 15, 45, 629, 488, 2733, 127, 6, 52, 292, 17, 4, 6936, 185, 132, 1988, 5304, 1799, 488, 2693, 47, 6, 392, 173, 4, 21686, 4378, 270, 2352, 4, 1500, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2078, 7, 5293, 861, 12746, 5, 4182, 30, 3127, 23651, 56, 4, 841, 5, 990, 692, 8, 4, 1669, 398, 229, 10, 10, 13, 2822, 670, 5304, 14, 9, 31, 7, 27, 111, 108, 15, 2033, 19, 7836, 1429, 875, 551, 14, 22, 9, 1193, 21, 45, 4829, 5, 45, 252, 8, 12508, 6, 565, 921, 3639, 39, 4, 529, 48, 25, 181, 8, 67, 35, 1732, 22, 49, 238, 60, 135, 1162, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 5687, 25, 203, 28, 8, 818, 12, 125, 4, 3077]),\n",
       "         list([1, 13, 1228, 119, 14, 552, 7, 20, 190, 14, 58, 13, 258, 546, 1786, 8, 1968, 4, 268, 237, 13, 191, 81, 15, 13, 80, 43, 3824, 44, 12, 14, 16, 427, 3192, 4, 183, 15, 593, 19, 4, 351, 362, 26, 55, 646, 21, 4, 1239, 84, 26, 1557, 3755, 13, 244, 6, 2071, 132, 184, 194, 5, 13, 70, 4478, 546, 73, 190, 13, 62, 24, 81, 320, 4, 538, 4, 117, 250, 127, 11, 14, 20, 82, 4, 452, 11, 14, 20, 9, 8654, 19, 41, 476, 8, 4, 213, 7, 9185, 13, 657, 13, 286, 38, 1612, 44, 41, 5, 41, 1729, 88, 13, 62, 28, 900, 510, 4, 509, 51, 6, 612, 59, 16, 193, 61, 4666, 5, 702, 930, 143, 285, 25, 67, 41, 81, 366, 4, 130, 82, 9, 259, 334, 397, 1195, 7, 149, 102, 15, 26, 814, 38, 465, 1627, 31, 70, 983, 67, 51, 9, 112, 814, 17, 35, 311, 75, 26, 11649, 574, 19, 4, 1729, 23, 4, 268, 38, 95, 138, 4, 609, 191, 75, 28, 314, 1772]),\n",
       "         ...,\n",
       "         list([1, 13, 1408, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 5093, 21, 45, 184, 78, 4, 1492, 910, 769, 2290, 2515, 395, 4257, 5, 1454, 11, 119, 16946, 89, 1036, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2280, 284, 1842, 60664, 37, 315, 4, 226, 20, 272, 2942, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 5518]),\n",
       "         list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 3684, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2960, 7, 743, 46, 1028, 9, 3531, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 28739, 5, 9, 1861, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 1276, 92, 124, 51, 45, 1188, 71, 536, 13, 520, 14, 20, 6, 2302, 7, 470]),\n",
       "         list([1, 6, 52, 7465, 430, 22, 9, 220, 2594, 8, 28, 24357, 519, 3227, 6, 769, 15, 47, 6, 3482, 4067, 8, 114, 5, 33, 222, 31, 55, 184, 704, 5586, 18020, 19, 346, 3153, 5, 6, 364, 350, 4, 184, 5586, 9, 133, 1810, 11, 5417, 13226, 21, 4, 7298, 42657, 570, 50, 2005, 2643, 9, 6, 1249, 17, 6, 25194, 27803, 21, 17, 6, 1211, 232, 1138, 2249, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 1078, 19, 4, 78, 173, 7, 27, 20067, 5698, 3406, 718, 21264, 9, 6, 6907, 17, 210, 5, 3281, 5677, 47, 77, 395, 14, 172, 173, 18, 2740, 2931, 4517, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 14274, 53, 40, 35, 390, 7, 11, 4, 3567, 7, 4, 314, 74, 6, 792, 22, 16261, 19, 714, 727, 5205, 382, 4, 91, 6533, 439, 19, 14, 20, 9, 1441, 5805, 1118, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2005, 2643])],\n",
       "        dtype=object),\n",
       "  array([0, 1, 0, ..., 0, 0, 0], dtype=int64)))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.load_data(maxlen=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
       "         list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "         list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
       "         ...,\n",
       "         list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 86527, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 14532, 325, 725, 134, 15271, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 11656, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 26094, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 17793, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 14492, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 17793, 5, 27, 710, 117, 74936, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 17793, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 17793, 7750, 5, 4241, 18, 4, 8497, 13164, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 11027, 4, 3586, 22459]),\n",
       "         list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 21469, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 40691, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 29455, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 11418, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 21213, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
       "         list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 12815, 270, 14437, 5, 16923, 12255, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 16553, 21, 27, 9685, 6139, 5, 29043, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 85010, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 70907, 10755, 544, 5, 383, 1271, 848, 1468, 12183, 497, 16876, 8, 1597, 8778, 19280, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
       "        dtype=object),\n",
       "  array([1, 0, 0, ..., 0, 1, 0], dtype=int64)),\n",
       " (array([list([1, 591, 202, 14, 31, 6, 717, 10, 10, 18142, 10698, 5, 4, 360, 7, 4, 177, 5760, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 7944, 100, 28, 1668, 14, 31, 23, 27, 7479, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 15387, 38, 32, 25, 7944, 451, 202, 14, 6, 717]),\n",
       "         list([1, 14, 22, 3443, 6, 176, 7, 5063, 88, 12, 2679, 23, 1310, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 7216, 10626, 4, 8463, 2801, 109, 1603, 21, 4, 22, 3861, 8, 6, 1193, 1330, 10, 10, 4, 105, 987, 35, 841, 16873, 19, 861, 1074, 5, 1987, 17975, 45, 55, 221, 15, 670, 5304, 526, 14, 1069, 4, 405, 5, 2438, 7, 27, 85, 108, 131, 4, 5045, 5304, 3884, 405, 9, 3523, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 8463, 2801, 45, 407, 31, 7, 41, 3778, 105, 21, 59, 299, 12, 38, 950, 5, 4521, 15, 45, 629, 488, 2733, 127, 6, 52, 292, 17, 4, 6936, 185, 132, 1988, 5304, 1799, 488, 2693, 47, 6, 392, 173, 4, 21686, 4378, 270, 2352, 4, 1500, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2078, 7, 5293, 861, 12746, 5, 4182, 30, 3127, 23651, 56, 4, 841, 5, 990, 692, 8, 4, 1669, 398, 229, 10, 10, 13, 2822, 670, 5304, 14, 9, 31, 7, 27, 111, 108, 15, 2033, 19, 7836, 1429, 875, 551, 14, 22, 9, 1193, 21, 45, 4829, 5, 45, 252, 8, 12508, 6, 565, 921, 3639, 39, 4, 529, 48, 25, 181, 8, 67, 35, 1732, 22, 49, 238, 60, 135, 1162, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 5687, 25, 203, 28, 8, 818, 12, 125, 4, 3077]),\n",
       "         list([1, 111, 748, 4368, 1133, 33782, 24563, 4, 87, 1551, 1262, 7, 31, 318, 9459, 7, 4, 498, 5076, 748, 63, 29, 5161, 220, 686, 10941, 5, 17, 12, 575, 220, 2507, 17, 6, 185, 132, 24563, 16, 53, 928, 11, 51278, 74, 4, 438, 21, 27, 10044, 589, 8, 22, 107, 20123, 19550, 997, 1638, 8, 35, 2076, 9019, 11, 22, 231, 54, 29, 1706, 29, 100, 18995, 2425, 34, 12998, 8738, 48078, 5, 19353, 98, 31, 2122, 33, 6, 58, 14, 3808, 1638, 8, 4, 365, 7, 2789, 3761, 356, 346, 4, 27608, 1060, 63, 29, 93, 11, 5421, 11, 15236, 33, 6, 58, 54, 1270, 431, 748, 7, 32, 2580, 16, 11, 94, 19469, 10, 10, 4, 993, 45222, 7, 4, 1766, 2634, 2164, 24563, 8, 847, 8, 1450, 121, 31, 7, 27, 86, 2663, 10760, 16, 6, 465, 993, 2006, 30995, 573, 17, 61862, 42, 4, 17345, 37, 473, 6, 711, 6, 8869, 7, 328, 212, 70, 30, 258, 11, 220, 32, 7, 108, 21, 133, 12, 9, 55, 465, 849, 3711, 53, 33, 2071, 1969, 37, 70, 1144, 4, 5940, 1409, 74, 476, 37, 62, 91, 1329, 169, 4, 1330, 10104, 146, 655, 2212, 5, 258, 12, 184, 10104, 546, 5, 849, 10333, 7, 4, 22, 1436, 18, 631, 1386, 797, 7, 4, 8712, 71, 348, 425, 4320, 1061, 19, 10288, 5, 12141, 11, 661, 8, 339, 17863, 4, 2455, 11434, 7, 4, 1962, 10, 10, 263, 787, 9, 270, 11, 6, 9466, 4, 61862, 48414, 121, 4, 5437, 26, 4434, 19, 68, 1372, 5, 28, 446, 6, 318, 7149, 8, 67, 51, 36, 70, 81, 8, 4392, 2294, 36, 1197, 8, 68411, 25399, 18, 6, 711, 4, 9909, 26, 10296, 1125, 11, 14, 636, 720, 12, 426, 28, 77, 776, 8, 97, 38, 111, 7489, 6175, 168, 1239, 5189, 137, 25399, 18, 27, 173, 9, 2399, 17, 6, 12397, 428, 14657, 232, 11, 4, 8014, 37, 272, 40, 2708, 247, 30, 656, 6, 13182, 54, 25399, 3292, 98, 6, 2840, 40, 558, 37, 6093, 98, 4, 17345, 1197, 15, 14, 9, 57, 4893, 5, 4659, 6, 275, 711, 7937, 25399, 3292, 98, 6, 31036, 10, 10, 6639, 19, 14, 10241, 267, 162, 711, 37, 5900, 752, 98, 4, 17345, 2378, 90, 19, 6, 73284, 7, 36744, 1810, 77553, 4, 4770, 3183, 930, 8, 508, 90, 4, 1317, 8, 4, 48414, 17, 15454, 3965, 1853, 4, 1494, 8, 4468, 189, 4, 31036, 6287, 5774, 4, 4770, 5, 95, 271, 23, 6, 7742, 6063, 21627, 5437, 33, 1526, 6, 425, 3155, 33697, 4535, 1636, 7, 4, 4669, 11966, 469, 4, 4552, 54, 4, 150, 5664, 17345, 280, 53, 68411, 25399, 18, 339, 29, 1978, 27, 7885, 5, 17303, 68, 1830, 19, 6571, 14605, 4, 1515, 7, 263, 65, 2132, 34, 6, 5680, 7489, 43, 159, 29, 9, 4706, 9, 387, 73, 195, 584, 10, 10, 1069, 4, 58, 810, 54, 14, 6078, 117, 22, 16, 93, 5, 1069, 4, 192, 15, 12, 16, 93, 34, 6, 1766, 28228, 33, 4, 5673, 7, 15, 18760, 9252, 3286, 325, 12, 62, 30, 776, 8, 67, 14, 17, 6, 12214, 44, 148, 687, 24563, 203, 42, 203, 24, 28, 69, 32157, 6676, 11, 330, 54, 29, 93, 61862, 21, 845, 14148, 27, 1099, 7, 819, 4, 22, 1407, 17, 6, 14967, 787, 7, 2460, 19569, 61862, 100, 30, 4, 3737, 3617, 3169, 2321, 42, 1898, 11, 4, 3814, 42, 101, 704, 7, 101, 999, 15, 1625, 94, 2926, 180, 5, 9, 9101, 34, 15205, 45, 6, 1429, 22, 60, 6, 1220, 31, 11, 94, 6408, 96, 21, 94, 749, 9, 57, 975]),\n",
       "         ...,\n",
       "         list([1, 13, 1408, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 5093, 21, 45, 184, 78, 4, 1492, 910, 769, 2290, 2515, 395, 4257, 5, 1454, 11, 119, 16946, 89, 1036, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2280, 284, 1842, 60664, 37, 315, 4, 226, 20, 272, 2942, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 5518]),\n",
       "         list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 3684, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2960, 7, 743, 46, 1028, 9, 3531, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 28739, 5, 9, 1861, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 1276, 92, 124, 51, 45, 1188, 71, 536, 13, 520, 14, 20, 6, 2302, 7, 470]),\n",
       "         list([1, 6, 52, 7465, 430, 22, 9, 220, 2594, 8, 28, 24357, 519, 3227, 6, 769, 15, 47, 6, 3482, 4067, 8, 114, 5, 33, 222, 31, 55, 184, 704, 5586, 18020, 19, 346, 3153, 5, 6, 364, 350, 4, 184, 5586, 9, 133, 1810, 11, 5417, 13226, 21, 4, 7298, 42657, 570, 50, 2005, 2643, 9, 6, 1249, 17, 6, 25194, 27803, 21, 17, 6, 1211, 232, 1138, 2249, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 1078, 19, 4, 78, 173, 7, 27, 20067, 5698, 3406, 718, 21264, 9, 6, 6907, 17, 210, 5, 3281, 5677, 47, 77, 395, 14, 172, 173, 18, 2740, 2931, 4517, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 14274, 53, 40, 35, 390, 7, 11, 4, 3567, 7, 4, 314, 74, 6, 792, 22, 16261, 19, 714, 727, 5205, 382, 4, 91, 6533, 439, 19, 14, 20, 9, 1441, 5805, 1118, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2005, 2643])],\n",
       "        dtype=object),\n",
       "  array([0, 1, 1, ..., 0, 0, 0], dtype=int64)))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use '1' as the character that indicates the start of a sequence\n",
    "imdb.load_data(start_char=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the dataset word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the word index as a dictionary\n",
    "# Accounting for index_from\n",
    "index_from = 3\n",
    "test = {key: value + index_from for key, value in imdb_word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fawn': 34704,\n",
       " 'tsukino': 52009,\n",
       " 'nunnery': 52010,\n",
       " 'sonja': 16819,\n",
       " 'vani': 63954,\n",
       " 'woods': 1411,\n",
       " 'spiders': 16118,\n",
       " 'hanging': 2348,\n",
       " 'woody': 2292,\n",
       " 'trawling': 52011,\n",
       " \"hold's\": 52012,\n",
       " 'comically': 11310,\n",
       " 'localized': 40833,\n",
       " 'disobeying': 30571,\n",
       " \"'royale\": 52013,\n",
       " \"harpo's\": 40834,\n",
       " 'canet': 52014,\n",
       " 'aileen': 19316,\n",
       " 'acurately': 52015,\n",
       " \"diplomat's\": 52016,\n",
       " 'rickman': 25245,\n",
       " 'arranged': 6749,\n",
       " 'rumbustious': 52017,\n",
       " 'familiarness': 52018,\n",
       " \"spider'\": 52019,\n",
       " 'hahahah': 68807,\n",
       " \"wood'\": 52020,\n",
       " 'transvestism': 40836,\n",
       " \"hangin'\": 34705,\n",
       " 'bringing': 2341,\n",
       " 'seamier': 40837,\n",
       " 'wooded': 34706,\n",
       " 'bravora': 52021,\n",
       " 'grueling': 16820,\n",
       " 'wooden': 1639,\n",
       " 'wednesday': 16821,\n",
       " \"'prix\": 52022,\n",
       " 'altagracia': 34707,\n",
       " 'circuitry': 52023,\n",
       " 'crotch': 11588,\n",
       " 'busybody': 57769,\n",
       " \"tart'n'tangy\": 52024,\n",
       " 'burgade': 14132,\n",
       " 'thrace': 52026,\n",
       " \"tom's\": 11041,\n",
       " 'snuggles': 52028,\n",
       " 'francesco': 29117,\n",
       " 'complainers': 52030,\n",
       " 'templarios': 52128,\n",
       " '272': 40838,\n",
       " '273': 52031,\n",
       " 'zaniacs': 52133,\n",
       " '275': 34709,\n",
       " 'consenting': 27634,\n",
       " 'snuggled': 40839,\n",
       " 'inanimate': 15495,\n",
       " 'uality': 52033,\n",
       " 'bronte': 11929,\n",
       " 'errors': 4013,\n",
       " 'dialogs': 3233,\n",
       " \"yomada's\": 52034,\n",
       " \"madman's\": 34710,\n",
       " 'dialoge': 30588,\n",
       " 'usenet': 52036,\n",
       " 'videodrome': 40840,\n",
       " \"kid'\": 26341,\n",
       " 'pawed': 52037,\n",
       " \"'girlfriend'\": 30572,\n",
       " \"'pleasure\": 52038,\n",
       " \"'reloaded'\": 52039,\n",
       " \"kazakos'\": 40842,\n",
       " 'rocque': 52040,\n",
       " 'mailings': 52041,\n",
       " 'brainwashed': 11930,\n",
       " 'mcanally': 16822,\n",
       " \"tom''\": 52042,\n",
       " 'kurupt': 25246,\n",
       " 'affiliated': 21908,\n",
       " 'babaganoosh': 52043,\n",
       " \"noe's\": 40843,\n",
       " 'quart': 40844,\n",
       " 'kids': 362,\n",
       " 'uplifting': 5037,\n",
       " 'controversy': 7096,\n",
       " 'kida': 21909,\n",
       " 'kidd': 23382,\n",
       " \"error'\": 52044,\n",
       " 'neurologist': 52045,\n",
       " 'spotty': 18513,\n",
       " 'cobblers': 30573,\n",
       " 'projection': 9881,\n",
       " 'fastforwarding': 40845,\n",
       " 'sters': 52046,\n",
       " \"eggar's\": 52047,\n",
       " 'etherything': 52048,\n",
       " 'gateshead': 40846,\n",
       " 'airball': 34711,\n",
       " 'unsinkable': 25247,\n",
       " 'stern': 7183,\n",
       " \"cervi's\": 52049,\n",
       " 'dnd': 40847,\n",
       " 'dna': 11589,\n",
       " 'insecurity': 20601,\n",
       " \"'reboot'\": 52050,\n",
       " 'trelkovsky': 11040,\n",
       " 'jaekel': 52051,\n",
       " 'sidebars': 52052,\n",
       " \"sforza's\": 52053,\n",
       " 'distortions': 17636,\n",
       " 'mutinies': 52054,\n",
       " 'sermons': 30605,\n",
       " '7ft': 40849,\n",
       " 'boobage': 52055,\n",
       " \"o'bannon's\": 52056,\n",
       " 'populations': 23383,\n",
       " 'chulak': 52057,\n",
       " 'mesmerize': 27636,\n",
       " 'quinnell': 52058,\n",
       " 'yahoo': 10310,\n",
       " 'meteorologist': 52060,\n",
       " 'beswick': 42580,\n",
       " 'boorman': 15496,\n",
       " 'voicework': 40850,\n",
       " \"ster'\": 52061,\n",
       " 'blustering': 22925,\n",
       " 'hj': 52062,\n",
       " 'intake': 27637,\n",
       " 'morally': 5624,\n",
       " 'jumbling': 40852,\n",
       " 'bowersock': 52063,\n",
       " \"'porky's'\": 52064,\n",
       " 'gershon': 16824,\n",
       " 'ludicrosity': 40853,\n",
       " 'coprophilia': 52065,\n",
       " 'expressively': 40854,\n",
       " \"india's\": 19503,\n",
       " \"post's\": 34713,\n",
       " 'wana': 52066,\n",
       " 'wang': 5286,\n",
       " 'wand': 30574,\n",
       " 'wane': 25248,\n",
       " 'edgeways': 52324,\n",
       " 'titanium': 34714,\n",
       " 'pinta': 40855,\n",
       " 'want': 181,\n",
       " 'pinto': 30575,\n",
       " 'whoopdedoodles': 52068,\n",
       " 'tchaikovsky': 21911,\n",
       " 'travel': 2106,\n",
       " \"'victory'\": 52069,\n",
       " 'copious': 11931,\n",
       " 'gouge': 22436,\n",
       " \"chapters'\": 52070,\n",
       " 'barbra': 6705,\n",
       " 'uselessness': 30576,\n",
       " \"wan'\": 52071,\n",
       " 'assimilated': 27638,\n",
       " 'petiot': 16119,\n",
       " 'most\\x85and': 52072,\n",
       " 'dinosaurs': 3933,\n",
       " 'wrong': 355,\n",
       " 'seda': 52073,\n",
       " 'stollen': 52074,\n",
       " 'sentencing': 34715,\n",
       " 'ouroboros': 40856,\n",
       " 'assimilates': 40857,\n",
       " 'colorfully': 40858,\n",
       " 'glenne': 27639,\n",
       " 'dongen': 52075,\n",
       " 'subplots': 4763,\n",
       " 'kiloton': 52076,\n",
       " 'chandon': 23384,\n",
       " \"effect'\": 34716,\n",
       " 'snugly': 27640,\n",
       " 'kuei': 40859,\n",
       " 'welcomed': 9095,\n",
       " 'dishonor': 30074,\n",
       " 'concurrence': 52078,\n",
       " 'stoicism': 23385,\n",
       " \"guys'\": 14899,\n",
       " \"beroemd'\": 52080,\n",
       " 'butcher': 6706,\n",
       " \"melfi's\": 40860,\n",
       " 'aargh': 30626,\n",
       " 'playhouse': 20602,\n",
       " 'wickedly': 11311,\n",
       " 'fit': 1183,\n",
       " 'labratory': 52081,\n",
       " 'lifeline': 40862,\n",
       " 'screaming': 1930,\n",
       " 'fix': 4290,\n",
       " 'cineliterate': 52082,\n",
       " 'fic': 52083,\n",
       " 'fia': 52084,\n",
       " 'fig': 34717,\n",
       " 'fmvs': 52085,\n",
       " 'fie': 52086,\n",
       " 'reentered': 52087,\n",
       " 'fin': 30577,\n",
       " 'doctresses': 52088,\n",
       " 'fil': 52089,\n",
       " 'zucker': 12609,\n",
       " 'ached': 31934,\n",
       " 'counsil': 52091,\n",
       " 'paterfamilias': 52092,\n",
       " 'songwriter': 13888,\n",
       " 'shivam': 34718,\n",
       " 'hurting': 9657,\n",
       " 'effects': 302,\n",
       " 'slauther': 52093,\n",
       " \"'flame'\": 52094,\n",
       " 'sommerset': 52095,\n",
       " 'interwhined': 52096,\n",
       " 'whacking': 27641,\n",
       " 'bartok': 52097,\n",
       " 'barton': 8778,\n",
       " 'frewer': 21912,\n",
       " \"fi'\": 52098,\n",
       " 'ingrid': 6195,\n",
       " 'stribor': 30578,\n",
       " 'approporiately': 52099,\n",
       " 'wobblyhand': 52100,\n",
       " 'tantalisingly': 52101,\n",
       " 'ankylosaurus': 52102,\n",
       " 'parasites': 17637,\n",
       " 'childen': 52103,\n",
       " \"jenkins'\": 52104,\n",
       " 'metafiction': 52105,\n",
       " 'golem': 17638,\n",
       " 'indiscretion': 40863,\n",
       " \"reeves'\": 23386,\n",
       " \"inamorata's\": 57784,\n",
       " 'brittannica': 52107,\n",
       " 'adapt': 7919,\n",
       " \"russo's\": 30579,\n",
       " 'guitarists': 48249,\n",
       " 'abbott': 10556,\n",
       " 'abbots': 40864,\n",
       " 'lanisha': 17652,\n",
       " 'magickal': 40866,\n",
       " 'mattter': 52108,\n",
       " \"'willy\": 52109,\n",
       " 'pumpkins': 34719,\n",
       " 'stuntpeople': 52110,\n",
       " 'estimate': 30580,\n",
       " 'ugghhh': 40867,\n",
       " 'gameplay': 11312,\n",
       " \"wern't\": 52111,\n",
       " \"n'sync\": 40868,\n",
       " 'sickeningly': 16120,\n",
       " 'chiara': 40869,\n",
       " 'disturbed': 4014,\n",
       " 'portmanteau': 40870,\n",
       " 'ineffectively': 52112,\n",
       " \"duchonvey's\": 82146,\n",
       " \"nasty'\": 37522,\n",
       " 'purpose': 1288,\n",
       " 'lazers': 52115,\n",
       " 'lightened': 28108,\n",
       " 'kaliganj': 52116,\n",
       " 'popularism': 52117,\n",
       " \"damme's\": 18514,\n",
       " 'stylistics': 30581,\n",
       " 'mindgaming': 52118,\n",
       " 'spoilerish': 46452,\n",
       " \"'corny'\": 52120,\n",
       " 'boerner': 34721,\n",
       " 'olds': 6795,\n",
       " 'bakelite': 52121,\n",
       " 'renovated': 27642,\n",
       " 'forrester': 27643,\n",
       " \"lumiere's\": 52122,\n",
       " 'gaskets': 52027,\n",
       " 'needed': 887,\n",
       " 'smight': 34722,\n",
       " 'master': 1300,\n",
       " \"edie's\": 25908,\n",
       " 'seeber': 40871,\n",
       " 'hiya': 52123,\n",
       " 'fuzziness': 52124,\n",
       " 'genesis': 14900,\n",
       " 'rewards': 12610,\n",
       " 'enthrall': 30582,\n",
       " \"'about\": 40872,\n",
       " \"recollection's\": 52125,\n",
       " 'mutilated': 11042,\n",
       " 'fatherlands': 52126,\n",
       " \"fischer's\": 52127,\n",
       " 'positively': 5402,\n",
       " '270': 34708,\n",
       " 'ahmed': 34723,\n",
       " 'zatoichi': 9839,\n",
       " 'bannister': 13889,\n",
       " 'anniversaries': 52130,\n",
       " \"helm's\": 30583,\n",
       " \"'work'\": 52131,\n",
       " 'exclaimed': 34724,\n",
       " \"'unfunny'\": 52132,\n",
       " '274': 52032,\n",
       " 'feeling': 547,\n",
       " \"wanda's\": 52134,\n",
       " 'dolan': 33269,\n",
       " '278': 52136,\n",
       " 'peacoat': 52137,\n",
       " 'brawny': 40873,\n",
       " 'mishra': 40874,\n",
       " 'worlders': 40875,\n",
       " 'protags': 52138,\n",
       " 'skullcap': 52139,\n",
       " 'dastagir': 57599,\n",
       " 'affairs': 5625,\n",
       " 'wholesome': 7802,\n",
       " 'hymen': 52140,\n",
       " 'paramedics': 25249,\n",
       " 'unpersons': 52141,\n",
       " 'heavyarms': 52142,\n",
       " 'affaire': 52143,\n",
       " 'coulisses': 52144,\n",
       " 'hymer': 40876,\n",
       " 'kremlin': 52145,\n",
       " 'shipments': 30584,\n",
       " 'pixilated': 52146,\n",
       " \"'00s\": 30585,\n",
       " 'diminishing': 18515,\n",
       " 'cinematic': 1360,\n",
       " 'resonates': 14901,\n",
       " 'simplify': 40877,\n",
       " \"nature'\": 40878,\n",
       " 'temptresses': 40879,\n",
       " 'reverence': 16825,\n",
       " 'resonated': 19505,\n",
       " 'dailey': 34725,\n",
       " '2\\x85': 52147,\n",
       " 'treize': 27644,\n",
       " 'majo': 52148,\n",
       " 'kiya': 21913,\n",
       " 'woolnough': 52149,\n",
       " 'thanatos': 39800,\n",
       " 'sandoval': 35734,\n",
       " 'dorama': 40882,\n",
       " \"o'shaughnessy\": 52150,\n",
       " 'tech': 4991,\n",
       " 'fugitives': 32021,\n",
       " 'teck': 30586,\n",
       " \"'e'\": 76128,\n",
       " 'doesnt': 40884,\n",
       " 'purged': 52152,\n",
       " 'saying': 660,\n",
       " \"martians'\": 41098,\n",
       " 'norliss': 23421,\n",
       " 'dickey': 27645,\n",
       " 'dicker': 52155,\n",
       " \"'sependipity\": 52156,\n",
       " 'padded': 8425,\n",
       " 'ordell': 57795,\n",
       " \"sturges'\": 40885,\n",
       " 'independentcritics': 52157,\n",
       " 'tempted': 5748,\n",
       " \"atkinson's\": 34727,\n",
       " 'hounded': 25250,\n",
       " 'apace': 52158,\n",
       " 'clicked': 15497,\n",
       " \"'humor'\": 30587,\n",
       " \"martino's\": 17180,\n",
       " \"'supporting\": 52159,\n",
       " 'warmongering': 52035,\n",
       " \"zemeckis's\": 34728,\n",
       " 'lube': 21914,\n",
       " 'shocky': 52160,\n",
       " 'plate': 7479,\n",
       " 'plata': 40886,\n",
       " 'sturgess': 40887,\n",
       " \"nerds'\": 40888,\n",
       " 'plato': 20603,\n",
       " 'plath': 34729,\n",
       " 'platt': 40889,\n",
       " 'mcnab': 52162,\n",
       " 'clumsiness': 27646,\n",
       " 'altogether': 3902,\n",
       " 'massacring': 42587,\n",
       " 'bicenntinial': 52163,\n",
       " 'skaal': 40890,\n",
       " 'droning': 14363,\n",
       " 'lds': 8779,\n",
       " 'jaguar': 21915,\n",
       " \"cale's\": 34730,\n",
       " 'nicely': 1780,\n",
       " 'mummy': 4591,\n",
       " \"lot's\": 18516,\n",
       " 'patch': 10089,\n",
       " 'kerkhof': 50205,\n",
       " \"leader's\": 52164,\n",
       " \"'movie\": 27647,\n",
       " 'uncomfirmed': 52165,\n",
       " 'heirloom': 40891,\n",
       " 'wrangle': 47363,\n",
       " 'emotion\\x85': 52166,\n",
       " \"'stargate'\": 52167,\n",
       " 'pinoy': 40892,\n",
       " 'conchatta': 40893,\n",
       " 'broeke': 41131,\n",
       " 'advisedly': 40894,\n",
       " \"barker's\": 17639,\n",
       " 'descours': 52169,\n",
       " 'lots': 775,\n",
       " 'lotr': 9262,\n",
       " 'irs': 9882,\n",
       " 'lott': 52170,\n",
       " 'xvi': 40895,\n",
       " 'irk': 34731,\n",
       " 'irl': 52171,\n",
       " 'ira': 6890,\n",
       " 'belzer': 21916,\n",
       " 'irc': 52172,\n",
       " 'ire': 27648,\n",
       " 'requisites': 40896,\n",
       " 'discipline': 7696,\n",
       " 'lyoko': 52964,\n",
       " 'extend': 11313,\n",
       " 'nature': 876,\n",
       " \"'dickie'\": 52173,\n",
       " 'optimist': 40897,\n",
       " 'lapping': 30589,\n",
       " 'superficial': 3903,\n",
       " 'vestment': 52174,\n",
       " 'extent': 2826,\n",
       " 'tendons': 52175,\n",
       " \"heller's\": 52176,\n",
       " 'quagmires': 52177,\n",
       " 'miyako': 52178,\n",
       " 'moocow': 20604,\n",
       " \"coles'\": 52179,\n",
       " 'lookit': 40898,\n",
       " 'ravenously': 52180,\n",
       " 'levitating': 40899,\n",
       " 'perfunctorily': 52181,\n",
       " 'lookin': 30590,\n",
       " \"lot'\": 40901,\n",
       " 'lookie': 52182,\n",
       " 'fearlessly': 34873,\n",
       " 'libyan': 52184,\n",
       " 'fondles': 40902,\n",
       " 'gopher': 35717,\n",
       " 'wearying': 40904,\n",
       " \"nz's\": 52185,\n",
       " 'minuses': 27649,\n",
       " 'puposelessly': 52186,\n",
       " 'shandling': 52187,\n",
       " 'decapitates': 31271,\n",
       " 'humming': 11932,\n",
       " \"'nother\": 40905,\n",
       " 'smackdown': 21917,\n",
       " 'underdone': 30591,\n",
       " 'frf': 40906,\n",
       " 'triviality': 52188,\n",
       " 'fro': 25251,\n",
       " 'bothers': 8780,\n",
       " \"'kensington\": 52189,\n",
       " 'much': 76,\n",
       " 'muco': 34733,\n",
       " 'wiseguy': 22618,\n",
       " \"richie's\": 27651,\n",
       " 'tonino': 40907,\n",
       " 'unleavened': 52190,\n",
       " 'fry': 11590,\n",
       " \"'tv'\": 40908,\n",
       " 'toning': 40909,\n",
       " 'obese': 14364,\n",
       " 'sensationalized': 30592,\n",
       " 'spiv': 40910,\n",
       " 'spit': 6262,\n",
       " 'arkin': 7367,\n",
       " 'charleton': 21918,\n",
       " 'jeon': 16826,\n",
       " 'boardroom': 21919,\n",
       " 'doubts': 4992,\n",
       " 'spin': 3087,\n",
       " 'hepo': 53086,\n",
       " 'wildcat': 27652,\n",
       " 'venoms': 10587,\n",
       " 'misconstrues': 52194,\n",
       " 'mesmerising': 18517,\n",
       " 'misconstrued': 40911,\n",
       " 'rescinds': 52195,\n",
       " 'prostrate': 52196,\n",
       " 'majid': 40912,\n",
       " 'climbed': 16482,\n",
       " 'canoeing': 34734,\n",
       " 'majin': 52198,\n",
       " 'animie': 57807,\n",
       " 'sylke': 40913,\n",
       " 'conditioned': 14902,\n",
       " 'waddell': 40914,\n",
       " '3\\x85': 52199,\n",
       " 'hyperdrive': 41191,\n",
       " 'conditioner': 34735,\n",
       " 'bricklayer': 53156,\n",
       " 'hong': 2579,\n",
       " 'memoriam': 52201,\n",
       " 'inventively': 30595,\n",
       " \"levant's\": 25252,\n",
       " 'portobello': 20641,\n",
       " 'remand': 52203,\n",
       " 'mummified': 19507,\n",
       " 'honk': 27653,\n",
       " 'spews': 19508,\n",
       " 'visitations': 40915,\n",
       " 'mummifies': 52204,\n",
       " 'cavanaugh': 25253,\n",
       " 'zeon': 23388,\n",
       " \"jungle's\": 40916,\n",
       " 'viertel': 34736,\n",
       " 'frenchmen': 27654,\n",
       " 'torpedoes': 52205,\n",
       " 'schlessinger': 52206,\n",
       " 'torpedoed': 34737,\n",
       " 'blister': 69879,\n",
       " 'cinefest': 52207,\n",
       " 'furlough': 34738,\n",
       " 'mainsequence': 52208,\n",
       " 'mentors': 40917,\n",
       " 'academic': 9097,\n",
       " 'stillness': 20605,\n",
       " 'academia': 40918,\n",
       " 'lonelier': 52209,\n",
       " 'nibby': 52210,\n",
       " \"losers'\": 52211,\n",
       " 'cineastes': 40919,\n",
       " 'corporate': 4452,\n",
       " 'massaging': 40920,\n",
       " 'bellow': 30596,\n",
       " 'absurdities': 19509,\n",
       " 'expetations': 53244,\n",
       " 'nyfiken': 40921,\n",
       " 'mehras': 75641,\n",
       " 'lasse': 52212,\n",
       " 'visability': 52213,\n",
       " 'militarily': 33949,\n",
       " \"elder'\": 52214,\n",
       " 'gainsbourg': 19026,\n",
       " 'hah': 20606,\n",
       " 'hai': 13423,\n",
       " 'haj': 34739,\n",
       " 'hak': 25254,\n",
       " 'hal': 4314,\n",
       " 'ham': 4895,\n",
       " 'duffer': 53262,\n",
       " 'haa': 52216,\n",
       " 'had': 69,\n",
       " 'advancement': 11933,\n",
       " 'hag': 16828,\n",
       " \"hand'\": 25255,\n",
       " 'hay': 13424,\n",
       " 'mcnamara': 20607,\n",
       " \"mozart's\": 52217,\n",
       " 'duffel': 30734,\n",
       " 'haq': 30597,\n",
       " 'har': 13890,\n",
       " 'has': 47,\n",
       " 'hat': 2404,\n",
       " 'hav': 40922,\n",
       " 'haw': 30598,\n",
       " 'figtings': 52218,\n",
       " 'elders': 15498,\n",
       " 'underpanted': 52219,\n",
       " 'pninson': 52220,\n",
       " 'unequivocally': 27655,\n",
       " \"barbara's\": 23676,\n",
       " \"bello'\": 52222,\n",
       " 'indicative': 13000,\n",
       " 'yawnfest': 40923,\n",
       " 'hexploitation': 52223,\n",
       " \"loder's\": 52224,\n",
       " 'sleuthing': 27656,\n",
       " \"justin's\": 32625,\n",
       " \"'ball\": 52225,\n",
       " \"'summer\": 52226,\n",
       " \"'demons'\": 34938,\n",
       " \"mormon's\": 52228,\n",
       " \"laughton's\": 34740,\n",
       " 'debell': 52229,\n",
       " 'shipyard': 39727,\n",
       " 'unabashedly': 30600,\n",
       " 'disks': 40404,\n",
       " 'crowd': 2293,\n",
       " 'crowe': 10090,\n",
       " \"vancouver's\": 56437,\n",
       " 'mosques': 34741,\n",
       " 'crown': 6630,\n",
       " 'culpas': 52230,\n",
       " 'crows': 27657,\n",
       " 'surrell': 53347,\n",
       " 'flowless': 52232,\n",
       " 'sheirk': 52233,\n",
       " \"'three\": 40926,\n",
       " \"peterson'\": 52234,\n",
       " 'ooverall': 52235,\n",
       " 'perchance': 40927,\n",
       " 'bottom': 1324,\n",
       " 'chabert': 53366,\n",
       " 'sneha': 52236,\n",
       " 'inhuman': 13891,\n",
       " 'ichii': 52237,\n",
       " 'ursla': 52238,\n",
       " 'completly': 30601,\n",
       " 'moviedom': 40928,\n",
       " 'raddick': 52239,\n",
       " 'brundage': 51998,\n",
       " 'brigades': 40929,\n",
       " 'starring': 1184,\n",
       " \"'goal'\": 52240,\n",
       " 'caskets': 52241,\n",
       " 'willcock': 52242,\n",
       " \"threesome's\": 52243,\n",
       " \"mosque'\": 52244,\n",
       " \"cover's\": 52245,\n",
       " 'spaceships': 17640,\n",
       " 'anomalous': 40930,\n",
       " 'ptsd': 27658,\n",
       " 'shirdan': 52246,\n",
       " 'obscenity': 21965,\n",
       " 'lemmings': 30602,\n",
       " 'duccio': 30603,\n",
       " \"levene's\": 52247,\n",
       " \"'gorby'\": 52248,\n",
       " \"teenager's\": 25258,\n",
       " 'marshall': 5343,\n",
       " 'honeymoon': 9098,\n",
       " 'shoots': 3234,\n",
       " 'despised': 12261,\n",
       " 'okabasho': 52249,\n",
       " 'fabric': 8292,\n",
       " 'cannavale': 18518,\n",
       " 'raped': 3540,\n",
       " \"tutt's\": 52250,\n",
       " 'grasping': 17641,\n",
       " 'despises': 18519,\n",
       " \"thief's\": 40931,\n",
       " 'rapes': 8929,\n",
       " 'raper': 52251,\n",
       " \"eyre'\": 27659,\n",
       " 'walchek': 52252,\n",
       " \"elmo's\": 23389,\n",
       " 'perfumes': 40932,\n",
       " 'spurting': 21921,\n",
       " \"exposition'\\x85\": 52253,\n",
       " 'denoting': 52254,\n",
       " 'thesaurus': 34743,\n",
       " \"shoot'\": 40933,\n",
       " 'bonejack': 49762,\n",
       " 'simpsonian': 52256,\n",
       " 'hebetude': 30604,\n",
       " \"hallow's\": 34744,\n",
       " 'desperation\\x85': 52257,\n",
       " 'incinerator': 34745,\n",
       " 'congratulations': 10311,\n",
       " 'humbled': 52258,\n",
       " \"else's\": 5927,\n",
       " 'trelkovski': 40848,\n",
       " \"rape'\": 52259,\n",
       " \"'chapters'\": 59389,\n",
       " '1600s': 52260,\n",
       " 'martian': 7256,\n",
       " 'nicest': 25259,\n",
       " 'eyred': 52262,\n",
       " 'passenger': 9460,\n",
       " 'disgrace': 6044,\n",
       " 'moderne': 52263,\n",
       " 'barrymore': 5123,\n",
       " 'yankovich': 52264,\n",
       " 'moderns': 40934,\n",
       " 'studliest': 52265,\n",
       " 'bedsheet': 52266,\n",
       " 'decapitation': 14903,\n",
       " 'slurring': 52267,\n",
       " \"'nunsploitation'\": 52268,\n",
       " \"'character'\": 34746,\n",
       " 'cambodia': 9883,\n",
       " 'rebelious': 52269,\n",
       " 'pasadena': 27660,\n",
       " 'crowne': 40935,\n",
       " \"'bedchamber\": 52270,\n",
       " 'conjectural': 52271,\n",
       " 'appologize': 52272,\n",
       " 'halfassing': 52273,\n",
       " 'paycheque': 57819,\n",
       " 'palms': 20609,\n",
       " \"'islands\": 52274,\n",
       " 'hawked': 40936,\n",
       " 'palme': 21922,\n",
       " 'conservatively': 40937,\n",
       " 'larp': 64010,\n",
       " 'palma': 5561,\n",
       " 'smelling': 21923,\n",
       " 'aragorn': 13001,\n",
       " 'hawker': 52275,\n",
       " 'hawkes': 52276,\n",
       " 'explosions': 3978,\n",
       " 'loren': 8062,\n",
       " \"pyle's\": 52277,\n",
       " 'shootout': 6707,\n",
       " \"mike's\": 18520,\n",
       " \"driscoll's\": 52278,\n",
       " 'cogsworth': 40938,\n",
       " \"britian's\": 52279,\n",
       " 'childs': 34747,\n",
       " \"portrait's\": 52280,\n",
       " 'chain': 3629,\n",
       " 'whoever': 2500,\n",
       " 'puttered': 52281,\n",
       " 'childe': 52282,\n",
       " 'maywether': 52283,\n",
       " 'chair': 3039,\n",
       " \"rance's\": 52284,\n",
       " 'machu': 34748,\n",
       " 'ballet': 4520,\n",
       " 'grapples': 34749,\n",
       " 'summerize': 76155,\n",
       " 'freelance': 30606,\n",
       " \"andrea's\": 52286,\n",
       " '\\x91very': 52287,\n",
       " 'coolidge': 45882,\n",
       " 'mache': 18521,\n",
       " 'balled': 52288,\n",
       " 'grappled': 40940,\n",
       " 'macha': 18522,\n",
       " 'underlining': 21924,\n",
       " 'macho': 5626,\n",
       " 'oversight': 19510,\n",
       " 'machi': 25260,\n",
       " 'verbally': 11314,\n",
       " 'tenacious': 21925,\n",
       " 'windshields': 40941,\n",
       " 'paychecks': 18560,\n",
       " 'jerk': 3399,\n",
       " \"good'\": 11934,\n",
       " 'prancer': 34751,\n",
       " 'prances': 21926,\n",
       " 'olympus': 52289,\n",
       " 'lark': 21927,\n",
       " 'embark': 10788,\n",
       " 'gloomy': 7368,\n",
       " 'jehaan': 52290,\n",
       " 'turaqui': 52291,\n",
       " \"child'\": 20610,\n",
       " 'locked': 2897,\n",
       " 'pranced': 52292,\n",
       " 'exact': 2591,\n",
       " 'unattuned': 52293,\n",
       " 'minute': 786,\n",
       " 'skewed': 16121,\n",
       " 'hodgins': 40943,\n",
       " 'skewer': 34752,\n",
       " 'think\\x85': 52294,\n",
       " 'rosenstein': 38768,\n",
       " 'helmit': 52295,\n",
       " 'wrestlemanias': 34753,\n",
       " 'hindered': 16829,\n",
       " \"martha's\": 30607,\n",
       " 'cheree': 52296,\n",
       " \"pluckin'\": 52297,\n",
       " 'ogles': 40944,\n",
       " 'heavyweight': 11935,\n",
       " 'aada': 82193,\n",
       " 'chopping': 11315,\n",
       " 'strongboy': 61537,\n",
       " 'hegemonic': 41345,\n",
       " 'adorns': 40945,\n",
       " 'xxth': 41349,\n",
       " 'nobuhiro': 34754,\n",
       " 'capites': 52301,\n",
       " 'kavogianni': 52302,\n",
       " 'antwerp': 13425,\n",
       " 'celebrated': 6541,\n",
       " 'roarke': 52303,\n",
       " 'baggins': 40946,\n",
       " 'cheeseburgers': 31273,\n",
       " 'matras': 52304,\n",
       " \"nineties'\": 52305,\n",
       " \"'craig'\": 52306,\n",
       " 'celebrates': 13002,\n",
       " 'unintentionally': 3386,\n",
       " 'drafted': 14365,\n",
       " 'climby': 52307,\n",
       " '303': 52308,\n",
       " 'oldies': 18523,\n",
       " 'climbs': 9099,\n",
       " 'honour': 9658,\n",
       " 'plucking': 34755,\n",
       " '305': 30077,\n",
       " 'address': 5517,\n",
       " 'menjou': 40947,\n",
       " \"'freak'\": 42595,\n",
       " 'dwindling': 19511,\n",
       " 'benson': 9461,\n",
       " 'whites': 52310,\n",
       " 'shamelessness': 40948,\n",
       " 'impacted': 21928,\n",
       " 'upatz': 52311,\n",
       " 'cusack': 3843,\n",
       " \"flavia's\": 37570,\n",
       " 'effette': 52312,\n",
       " 'influx': 34756,\n",
       " 'boooooooo': 52313,\n",
       " 'dimitrova': 52314,\n",
       " 'houseman': 13426,\n",
       " 'bigas': 25262,\n",
       " 'boylen': 52315,\n",
       " 'phillipenes': 52316,\n",
       " 'fakery': 40949,\n",
       " \"grandpa's\": 27661,\n",
       " 'darnell': 27662,\n",
       " 'undergone': 19512,\n",
       " 'handbags': 52318,\n",
       " 'perished': 21929,\n",
       " 'pooped': 37781,\n",
       " 'vigour': 27663,\n",
       " 'opposed': 3630,\n",
       " 'etude': 52319,\n",
       " \"caine's\": 11802,\n",
       " 'doozers': 52320,\n",
       " 'photojournals': 34757,\n",
       " 'perishes': 52321,\n",
       " 'constrains': 34758,\n",
       " 'migenes': 40951,\n",
       " 'consoled': 30608,\n",
       " 'alastair': 16830,\n",
       " 'wvs': 52322,\n",
       " 'ooooooh': 52323,\n",
       " 'approving': 34759,\n",
       " 'consoles': 40952,\n",
       " 'disparagement': 52067,\n",
       " 'futureistic': 52325,\n",
       " 'rebounding': 52326,\n",
       " \"'date\": 52327,\n",
       " 'gregoire': 52328,\n",
       " 'rutherford': 21930,\n",
       " 'americanised': 34760,\n",
       " 'novikov': 82199,\n",
       " 'following': 1045,\n",
       " 'munroe': 34761,\n",
       " \"morita'\": 52329,\n",
       " 'christenssen': 52330,\n",
       " 'oatmeal': 23109,\n",
       " 'fossey': 25263,\n",
       " 'livered': 40953,\n",
       " 'listens': 13003,\n",
       " \"'marci\": 76167,\n",
       " \"otis's\": 52333,\n",
       " 'thanking': 23390,\n",
       " 'maude': 16022,\n",
       " 'extensions': 34762,\n",
       " 'ameteurish': 52335,\n",
       " \"commender's\": 52336,\n",
       " 'agricultural': 27664,\n",
       " 'convincingly': 4521,\n",
       " 'fueled': 17642,\n",
       " 'mahattan': 54017,\n",
       " \"paris's\": 40955,\n",
       " 'vulkan': 52339,\n",
       " 'stapes': 52340,\n",
       " 'odysessy': 52341,\n",
       " 'harmon': 12262,\n",
       " 'surfing': 4255,\n",
       " 'halloran': 23497,\n",
       " 'unbelieveably': 49583,\n",
       " \"'offed'\": 52342,\n",
       " 'quadrant': 30610,\n",
       " 'inhabiting': 19513,\n",
       " 'nebbish': 34763,\n",
       " 'forebears': 40956,\n",
       " 'skirmish': 34764,\n",
       " 'ocassionally': 52343,\n",
       " \"'resist\": 52344,\n",
       " 'impactful': 21931,\n",
       " 'spicier': 52345,\n",
       " 'touristy': 40957,\n",
       " \"'football'\": 52346,\n",
       " 'webpage': 40958,\n",
       " 'exurbia': 52348,\n",
       " 'jucier': 52349,\n",
       " 'professors': 14904,\n",
       " 'structuring': 34765,\n",
       " 'jig': 30611,\n",
       " 'overlord': 40959,\n",
       " 'disconnect': 25264,\n",
       " 'sniffle': 82204,\n",
       " 'slimeball': 40960,\n",
       " 'jia': 40961,\n",
       " 'milked': 16831,\n",
       " 'banjoes': 40962,\n",
       " 'jim': 1240,\n",
       " 'workforces': 52351,\n",
       " 'jip': 52352,\n",
       " 'rotweiller': 52353,\n",
       " 'mundaneness': 34766,\n",
       " \"'ninja'\": 52354,\n",
       " \"dead'\": 11043,\n",
       " \"cipriani's\": 40963,\n",
       " 'modestly': 20611,\n",
       " \"professor'\": 52355,\n",
       " 'shacked': 40964,\n",
       " 'bashful': 34767,\n",
       " 'sorter': 23391,\n",
       " 'overpowering': 16123,\n",
       " 'workmanlike': 18524,\n",
       " 'henpecked': 27665,\n",
       " 'sorted': 18525,\n",
       " \"jb's\": 52357,\n",
       " \"'always\": 52358,\n",
       " \"'baptists\": 34768,\n",
       " 'dreamcatchers': 52359,\n",
       " \"'silence'\": 52360,\n",
       " 'hickory': 21932,\n",
       " 'fun\\x97yet': 52361,\n",
       " 'breakumentary': 52362,\n",
       " 'didn': 15499,\n",
       " 'didi': 52363,\n",
       " 'pealing': 52364,\n",
       " 'dispite': 40965,\n",
       " \"italy's\": 25265,\n",
       " 'instability': 21933,\n",
       " 'quarter': 6542,\n",
       " 'quartet': 12611,\n",
       " 'padm': 52365,\n",
       " \"'bleedmedry\": 52366,\n",
       " 'pahalniuk': 52367,\n",
       " 'honduras': 52368,\n",
       " 'bursting': 10789,\n",
       " \"pablo's\": 41468,\n",
       " 'irremediably': 52370,\n",
       " 'presages': 40966,\n",
       " 'bowlegged': 57835,\n",
       " 'dalip': 65186,\n",
       " 'entering': 6263,\n",
       " 'newsradio': 76175,\n",
       " 'presaged': 54153,\n",
       " \"giallo's\": 27666,\n",
       " 'bouyant': 40967,\n",
       " 'amerterish': 52371,\n",
       " 'rajni': 18526,\n",
       " 'leeves': 30613,\n",
       " 'macauley': 34770,\n",
       " 'seriously': 615,\n",
       " 'sugercoma': 52372,\n",
       " 'grimstead': 52373,\n",
       " \"'fairy'\": 52374,\n",
       " 'zenda': 30614,\n",
       " \"'twins'\": 52375,\n",
       " 'realisation': 17643,\n",
       " 'highsmith': 27667,\n",
       " 'raunchy': 7820,\n",
       " 'incentives': 40968,\n",
       " 'flatson': 52377,\n",
       " 'snooker': 35100,\n",
       " 'crazies': 16832,\n",
       " 'crazier': 14905,\n",
       " 'grandma': 7097,\n",
       " 'napunsaktha': 52378,\n",
       " 'workmanship': 30615,\n",
       " 'reisner': 52379,\n",
       " \"sanford's\": 61309,\n",
       " '\\x91doa': 52380,\n",
       " 'modest': 6111,\n",
       " \"everything's\": 19156,\n",
       " 'hamer': 40969,\n",
       " \"couldn't'\": 52382,\n",
       " 'quibble': 13004,\n",
       " 'socking': 52383,\n",
       " 'tingler': 21934,\n",
       " 'gutman': 52384,\n",
       " 'lachlan': 40970,\n",
       " 'tableaus': 52385,\n",
       " 'headbanger': 52386,\n",
       " 'spoken': 2850,\n",
       " 'cerebrally': 34771,\n",
       " \"'road\": 23493,\n",
       " 'tableaux': 21935,\n",
       " \"proust's\": 40971,\n",
       " 'periodical': 40972,\n",
       " \"shoveller's\": 52388,\n",
       " 'tamara': 25266,\n",
       " 'affords': 17644,\n",
       " 'concert': 3252,\n",
       " \"yara's\": 87958,\n",
       " 'someome': 52389,\n",
       " 'lingering': 8427,\n",
       " \"abraham's\": 41514,\n",
       " 'beesley': 34772,\n",
       " 'cherbourg': 34773,\n",
       " 'kagan': 28627,\n",
       " 'snatch': 9100,\n",
       " \"miyazaki's\": 9263,\n",
       " 'absorbs': 25267,\n",
       " \"koltai's\": 40973,\n",
       " 'tingled': 64030,\n",
       " 'crossroads': 19514,\n",
       " 'rehab': 16124,\n",
       " 'falworth': 52392,\n",
       " 'sequals': 52393,\n",
       " ...}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52256"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['simpsonian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'film',\n",
       " 'was',\n",
       " 'just',\n",
       " 'brilliant',\n",
       " 'casting',\n",
       " 'location',\n",
       " 'scenery',\n",
       " 'story',\n",
       " 'direction',\n",
       " \"everyone's\",\n",
       " 'really',\n",
       " 'suited',\n",
       " 'the',\n",
       " 'part',\n",
       " 'they',\n",
       " 'played',\n",
       " 'and',\n",
       " 'you',\n",
       " 'could',\n",
       " 'just',\n",
       " 'imagine',\n",
       " 'being',\n",
       " 'there',\n",
       " 'robert',\n",
       " \"redford's\",\n",
       " 'is',\n",
       " 'an',\n",
       " 'amazing',\n",
       " 'actor',\n",
       " 'and',\n",
       " 'now',\n",
       " 'the',\n",
       " 'same',\n",
       " 'being',\n",
       " 'director',\n",
       " \"norman's\",\n",
       " 'father',\n",
       " 'came',\n",
       " 'from',\n",
       " 'the',\n",
       " 'same',\n",
       " 'scottish',\n",
       " 'island',\n",
       " 'as',\n",
       " 'myself',\n",
       " 'so',\n",
       " 'i',\n",
       " 'loved',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'there',\n",
       " 'was',\n",
       " 'a',\n",
       " 'real',\n",
       " 'connection',\n",
       " 'with',\n",
       " 'this',\n",
       " 'film',\n",
       " 'the',\n",
       " 'witty',\n",
       " 'remarks',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'film',\n",
       " 'were',\n",
       " 'great',\n",
       " 'it',\n",
       " 'was',\n",
       " 'just',\n",
       " 'brilliant',\n",
       " 'so',\n",
       " 'much',\n",
       " 'that',\n",
       " 'i',\n",
       " 'bought',\n",
       " 'the',\n",
       " 'film',\n",
       " 'as',\n",
       " 'soon',\n",
       " 'as',\n",
       " 'it',\n",
       " 'was',\n",
       " 'released',\n",
       " 'for',\n",
       " 'retail',\n",
       " 'and',\n",
       " 'would',\n",
       " 'recommend',\n",
       " 'it',\n",
       " 'to',\n",
       " 'everyone',\n",
       " 'to',\n",
       " 'watch',\n",
       " 'and',\n",
       " 'the',\n",
       " 'fly',\n",
       " 'fishing',\n",
       " 'was',\n",
       " 'amazing',\n",
       " 'really',\n",
       " 'cried',\n",
       " 'at',\n",
       " 'the',\n",
       " 'end',\n",
       " 'it',\n",
       " 'was',\n",
       " 'so',\n",
       " 'sad',\n",
       " 'and',\n",
       " 'you',\n",
       " 'know',\n",
       " 'what',\n",
       " 'they',\n",
       " 'say',\n",
       " 'if',\n",
       " 'you',\n",
       " 'cry',\n",
       " 'at',\n",
       " 'a',\n",
       " 'film',\n",
       " 'it',\n",
       " 'must',\n",
       " 'have',\n",
       " 'been',\n",
       " 'good',\n",
       " 'and',\n",
       " 'this',\n",
       " 'definitely',\n",
       " 'was',\n",
       " 'also',\n",
       " 'congratulations',\n",
       " 'to',\n",
       " 'the',\n",
       " 'two',\n",
       " 'little',\n",
       " \"boy's\",\n",
       " 'that',\n",
       " 'played',\n",
       " 'the',\n",
       " \"part's\",\n",
       " 'of',\n",
       " 'norman',\n",
       " 'and',\n",
       " 'paul',\n",
       " 'they',\n",
       " 'were',\n",
       " 'just',\n",
       " 'brilliant',\n",
       " 'children',\n",
       " 'are',\n",
       " 'often',\n",
       " 'left',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'praising',\n",
       " 'list',\n",
       " 'i',\n",
       " 'think',\n",
       " 'because',\n",
       " 'the',\n",
       " 'stars',\n",
       " 'that',\n",
       " 'play',\n",
       " 'them',\n",
       " 'all',\n",
       " 'grown',\n",
       " 'up',\n",
       " 'are',\n",
       " 'such',\n",
       " 'a',\n",
       " 'big',\n",
       " 'profile',\n",
       " 'for',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'film',\n",
       " 'but',\n",
       " 'these',\n",
       " 'children',\n",
       " 'are',\n",
       " 'amazing',\n",
       " 'and',\n",
       " 'should',\n",
       " 'be',\n",
       " 'praised',\n",
       " 'for',\n",
       " 'what',\n",
       " 'they',\n",
       " 'have',\n",
       " 'done',\n",
       " \"don't\",\n",
       " 'you',\n",
       " 'think',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'story',\n",
       " 'was',\n",
       " 'so',\n",
       " 'lovely',\n",
       " 'because',\n",
       " 'it',\n",
       " 'was',\n",
       " 'true',\n",
       " 'and',\n",
       " 'was',\n",
       " \"someone's\",\n",
       " 'life',\n",
       " 'after',\n",
       " 'all',\n",
       " 'that',\n",
       " 'was',\n",
       " 'shared',\n",
       " 'with',\n",
       " 'us',\n",
       " 'all']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View an input sentence\n",
    "inv_imdb_word_index = {value: key for key, value in test.items()}\n",
    "[inv_imdb_word_index[index] for index in X_train[0] if index > index_from]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding and masking sequence data - Coding tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data with padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad the inputs to the maximum length using maxlen\n",
    "padded_X_train = pad_sequences(X_train, maxlen=300, padding='post', truncating='pre')\n",
    "padded_X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Masking layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([25000, 300, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Masking expects to see (batch, sequence, features)\n",
    "# Create a dummy feature dimension using expand_dims\n",
    "\n",
    "padded_X_train = tf.expand_dims(padded_X_train, -1)\n",
    "padded_X_train = tf.cast(padded_X_train, 'float32')\n",
    "padded_X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can expand it with `np.expand_dims` then, convert it to tensor with `tf.convert_to_tensor()`\n",
    "\n",
    "```python\n",
    "padded_X_train = np.expand_dims(padded_X_train, -1)\n",
    "tf_X_train = tf.convert_to_tensor(padded_X_train, dtype='float32')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a masking layer\n",
    "masking_layer = Masking(mask_value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass padded_X_train to it\n",
    "masked_X_train = masking_layer(padded_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(25000, 300, 1), dtype=float32, numpy=\n",
       "array([[[1.000e+00],\n",
       "        [1.400e+01],\n",
       "        [2.200e+01],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.940e+02],\n",
       "        [1.153e+03],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.400e+01],\n",
       "        [4.700e+01],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.100e+01],\n",
       "        [6.000e+00],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.446e+03],\n",
       "        [7.079e+03],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.700e+01],\n",
       "        [6.000e+00],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]]], dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(25000, 300, 1), dtype=float32, numpy=\n",
       "array([[[1.000e+00],\n",
       "        [1.400e+01],\n",
       "        [2.200e+01],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.940e+02],\n",
       "        [1.153e+03],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.400e+01],\n",
       "        [4.700e+01],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.100e+01],\n",
       "        [6.000e+00],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.446e+03],\n",
       "        [7.079e+03],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.700e+01],\n",
       "        [6.000e+00],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]]], dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object has no attribute '_keras_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-b0f845e22e46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpadded_X_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keras_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute '_keras_mask'"
     ]
    }
   ],
   "source": [
    "padded_X_train._keras_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(25000, 300), dtype=bool, numpy=\n",
       "array([[ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       ...,\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False]])>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_X_train._keras_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Text Data\n",
    "\n",
    "In this section, you will learn how to tokenise text data using `tf.keras.preprocessing.text.Tokenizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The text dataset\n",
    "\n",
    "The text we will work with in this notebook is Three Men in a Boat by Jerome K. Jerome, a comical short story about the perils of going outside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "with open('./dataset/ThreeMenInABoat.txt', 'r', encoding='utf-8') as f:\n",
    "    text_string = f.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform some simple preprocessing, replacing dashes with empty spaces\n",
    "text_string = text_string.replace('', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHAPTER I.   Three invalids.Sufferings of George and Harris.A victim to one hundred and seven fatal maladies.Useful prescriptions.Cure for liver complaint in children.We agree that we are overworked, and need rest.A week on the rolling deep?George suggests the River.Montmorency lodges an objection.Original motion carried by majority of three to one.  There were four of usGeorge, and William Samuel Harris, and myself, and Montmorency.  We were sitting in my room, smoking, and talking about how bad we werebad from a medical point of view I mean, of course.  We were all feeling seedy, and we were getting quite nervous about it. Harris said he felt such extraordinary fits of giddiness come over him at times, that he hardly knew what he was doing; and then George said that _he_ had fits of giddiness too, and hardly knew what _he_ was doing. With me, it was my liver that was out of order.  I knew it was my liver that was out of order, because I had just been reading a patent liver-pill circular, in which were detailed the various symptoms by which a man could tell when his liver was out of order.  I had them all.  It is a most extraordinary thing, but I never read a patent medicine advertisement without being impelled to the conclusion that I am suffering from the particular disease therein dealt with in its most virulent form.  The diagnosis seems in every case to correspond exactly with all the sensations that I have ever felt.  [Picture: Man reading book] I remember going to the British Museum one day to read up the treatment for some slight ailment of which I had a touchhay fever, I fancy it was.  I got down the book, and read all I came to read; and then, in an unthinking moment, I idly turned the leaves, and began to indolently study diseases, generally.  I forget which was the first distemper I plunged intosome fearful, devastating scourge, I knowand, before I had glanced half down the list of premonitory symptoms, it was borne in upon me that I had fairly got it.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_string[:2001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  I got down the book, and read all I came to read; and then, in an unthinking moment, I idly turned the leaves, and began to indolently study diseases, generally',\n",
       " '  I forget which was the first distemper I plunged intosome fearful, devastating scourge, I knowand, before I had glanced half down the list of premonitory symptoms, it was borne in upon me that I had fairly got it',\n",
       " '  I sat for awhile, frozen with horror; and then, in the listlessness of despair, I again turned over the pages',\n",
       " '  I came to typhoid feverread the symptomsdiscovered that I had typhoid fever, must have had it for months without knowing itwondered what else I had got; turned up St',\n",
       " ' Vituss Dancefound, as I expected, that I had that too,began to get interested in my case, and determined to sift it to the bottom, and so started alphabeticallyread up ague, and learnt that I was sickening for it, and that the acute stage would commence in about another fortnight',\n",
       " '  Brights disease, I was relieved to find, I had only in a modified form, and, so far as that was concerned, I might live for years',\n",
       " '  Cholera I had, with severe complications; and diphtheria I seemed to have been born with',\n",
       " '  I plodded conscientiously through the twenty-six letters, and the only malady I could conclude I had not got was housemaids knee',\n",
       " '  I felt rather hurt about this at first; it seemed somehow to be a sort of slight',\n",
       " '  Why hadnt I got housemaids knee?  Why this invidious reservation?  After a while, however, less grasping feelings prevailed']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the text into sentences\n",
    "sentence_strings = text_string.split('.')\n",
    "sentence_strings[20:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Tokenizer object\n",
    "\n",
    "The `Tokenizer` object allows you to easily tokenise words or characters from a text document. It has several options to allow you to adjust the tokenisation process. Documentation is available for the `Tokenizer` [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define any additional characters that we want to filter out (ignore) from the text\n",
    "additional_filters = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tokenizer has a `filters` keyword argument, that determines which characters will be filtered out from the text. The cell below shows the default characters that are filtered, to which we are adding our additional filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=None,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n' + additional_filters,\n",
    "    lower=True,\n",
    "    split=' ',\n",
    "    char_level=False,\n",
    "    oov_token='<UNK>',\n",
    "    document_count=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all, the `Tokenizer` has the following keyword arguments:\n",
    "\n",
    "`num_words`: int. the maximum number of words to keep, based on word frequency. Only the most common `num_words-1` words will be kept. If set to `None`, all words are kept.\n",
    "    \n",
    "`filters`: str. Each element is a character that will be filtered from the texts. Defaults to all punctuation (inc. tabs and line breaks), except `'`.\n",
    "\n",
    "`lower`: bool. Whether to convert the texts to lowercase. Defaults to `True`.\n",
    "\n",
    "`split`: str. Separator for word splitting. Defaults to `' '`.\n",
    "    \n",
    "`char_level`: bool. if True, every character will be treated as a token. Defaults to `False`.\n",
    "\n",
    "`oov_token`: if given, it will be added to word_index and used to replace out-of-vocabulary words during sequence_to_text calls. Defaults to `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Tokenizer to the text\n",
    "\n",
    "We can now tokenize our text using the `fit_on_texts` method. This method takes a list of strings to tokenize, as we have prepared with `sentence_strings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Tokenizer vocabulary\n",
    "tokenizer.fit_on_texts(sentence_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit_on_texts` method could also take a list of lists of strings, and in this case it would recognise each element of each sublist as an individual token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Tokenizer configuration\n",
    "\n",
    "Now that the Tokenizer has ingested the data, we can see what it has extracted from the text by viewing its configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['num_words', 'filters', 'lower', 'split', 'char_level', 'oov_token', 'document_count', 'word_counts', 'word_docs', 'index_docs', 'index_word', 'word_index'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the tokenizer config as a python dict\n",
    "tokenizer_config = tokenizer.get_config()\n",
    "tokenizer_config.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"chapter\": 19, \"i\": 1195, \"three\": 79, \"invalids\": 1, \"sufferings\": 2, \"of\": 1487, \"george\": 306, \"and\": 3375, \"harris\": 314, \"a\": 1696, \"victim\": 3, \"to\": 1785, \"one\": 241, \"hundred\": 19, \"seven\": 15, \"fatal\": 1, \"maladies\": 2, \"useful\": 2, \"prescriptions\": 1, \"cure\": 1, \"for\": 525, \"liver\": 8, \"complaint\": 2, \"in\": 976, \"children\": 13, \"we\": 866, \"agree\": 2, \"that\": 944, \"are\": 181, \"overworked\": 1, \"need\": 7, \"rest\": 14, \"week\": 19, \"on\": 501, \"the\": 3603, \"rolling\": 1, \"deep\": 18, \"suggests'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_config['word_counts'][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is the number of times each word appears in the corpus. As you can see, the word counts dictionaries in the config are serialized into plain JSON. The `loads()` method in the Python library `json` can be used to convert this JSON string into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "word_counts = json.loads(tokenizer_config['word_counts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word index is derived from the `word_counts`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"<UNK>\": 1, \"the\": 2, \"and\": 3, \"to\": 4, \"a\": 5, \"of\": 6, \"it\": 7, \"i\": 8, \"in\": 9, \"that\": 10, \"he\": 11, \"we\": 12, \"was\": 13, \"you\": 14, \"had\": 15, \"for\": 16, \"at\": 17, \"on\": 18, \"with\": 19, \"up\": 20, \"they\": 21, \"is\": 22, \"as\": 23, \"not\": 24, \"his\": 25, \"said\": 26, \"but\": 27, \"would\": 28, \"all\": 29, \"s\": 30, \"have\": 31, \"him\": 32, \"there\": 33, \"be\": 34, \"harris\": 35, \"george\": 36, \"out\": 37, \"t\": 38, \"so\": 39, \"then\": 40, \"when\": 41, \"them\": 42, \"one\": 43, \"were\": 44, \"about\": 45, \"us\": 46, \"'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_config['word_index'][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word = json.loads(tokenizer_config['index_word'])\n",
    "word_index = json.loads(tokenizer_config['word_index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the sentences to tokens\n",
    "\n",
    "You can map each sentence to a sequence of integer tokens using the Tokenizer's `texts_to_sequences()` method. As was the case for the IMDb data set, the number corresponding to a word is that word's frequency rank in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHAPTER I',\n",
       " '   Three invalids',\n",
       " 'Sufferings of George and Harris',\n",
       " 'A victim to one hundred and seven fatal maladies',\n",
       " 'Useful prescriptions']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_strings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the data\n",
    "sentence_seq = tokenizer.texts_to_sequences(sentence_strings)\n",
    "type(sentence_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[362, 8],\n",
       " [126, 3362],\n",
       " [2319, 6, 36, 3, 35],\n",
       " [5, 1779, 4, 43, 363, 3, 468, 3363, 2320],\n",
       " [2321, 3364]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_seq[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362 8\n",
      "126 3362\n",
      "2319 6 36 3 35\n",
      "5 1779 4 43 363 3 468 3363 2320\n",
      "2321 3364\n"
     ]
    }
   ],
   "source": [
    "print(word_index['chapter'], word_index['i'])\n",
    "print(word_index['three'], word_index['invalids'])\n",
    "print(word_index['sufferings'], word_index['of'], word_index['george'], word_index['and'], word_index['harris'])\n",
    "print(word_index['a'], word_index['victim'], word_index['to'], word_index['one'], word_index['hundred'], word_index['and'], word_index['seven'], word_index['fatal'], word_index['maladies'])\n",
    "print(word_index['useful'], word_index['prescriptions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the tokens to sentences\n",
    "\n",
    "You can map the tokens back to sentences using the Tokenizer's `sequences_to_texts` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[362, 8],\n",
       " [126, 3362],\n",
       " [2319, 6, 36, 3, 35],\n",
       " [5, 1779, 4, 43, 363, 3, 468, 3363, 2320],\n",
       " [2321, 3364]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_seq[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chapter i',\n",
       " 'three invalids',\n",
       " 'sufferings of george and harris',\n",
       " 'a victim to one hundred and seven fatal maladies',\n",
       " 'useful prescriptions']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map the token sequences back to sentences\n",
    "tokenizer.sequences_to_texts(sentence_seq)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chapter i\n",
      "three invalids\n",
      "sufferings of george and harris\n",
      "a victim to one hundred and seven fatal maladies\n",
      "useful prescriptions\n"
     ]
    }
   ],
   "source": [
    "# Verify the mappings in the config\n",
    "print(index_word['362'], index_word['8'])\n",
    "print(index_word['126'], index_word['3362'])\n",
    "print(index_word['2319'], index_word['6'], index_word['36'], index_word['3'], index_word['35'])\n",
    "print(index_word['5'], index_word['1779'], index_word['4'], index_word['43'], index_word['363'], index_word['3'], index_word['468'], index_word['3363'], index_word['2320'])\n",
    "print(index_word['2321'], index_word['3364'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good day world', 'montmorency bit my finger']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Any valida sequence of tokens can be converted to text\n",
    "tokenizer.sequences_to_texts([[92, 104, 241], [152, 169, 53, 2491]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a word is not featured in the Tokenizer's word index, then it will be mapped to the value of the Tokenizer's `oov_token` property. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8, 28, 78, 1, 1]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize unrecognized words\n",
    "tokenizer.texts_to_sequences(['i would like goobleydoobly hobbledyho'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<UNK>'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the OOV token\n",
    "index_word['1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further reading and resources\n",
    "\n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "* https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(input_dim=1000, output_dim=32, input_length=64, mask_zero=True)\n",
    "test_input = np.random.randint(1000, size=(16, 64))\n",
    "\n",
    "embedded_inputs = embedding_layer(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([16, 64, 32])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (batch_size, sequence, embedding_dim)\n",
    "embedded_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16, 64, 32), dtype=float32, numpy=\n",
       "array([[[ 4.54709195e-02, -1.77626386e-02,  1.23503581e-02, ...,\n",
       "         -8.92143324e-03,  1.71394236e-02, -1.32649764e-02],\n",
       "        [-1.63089409e-02, -1.82863958e-02, -7.96171278e-03, ...,\n",
       "          2.84917466e-02,  4.54032458e-02,  2.71193422e-02],\n",
       "        [ 2.91286223e-02,  2.35450529e-02,  4.55331095e-02, ...,\n",
       "         -3.85778658e-02,  2.31413879e-02, -1.10698715e-02],\n",
       "        ...,\n",
       "        [ 4.81118225e-02,  3.27598341e-02,  4.36899327e-02, ...,\n",
       "          1.17005035e-03,  3.74303795e-02, -3.46264988e-03],\n",
       "        [-7.19379261e-03, -1.71089396e-02, -4.29847948e-02, ...,\n",
       "         -1.78243965e-03, -5.57011366e-03,  2.07354873e-03],\n",
       "        [ 2.16959603e-02, -4.52200286e-02,  4.11232375e-02, ...,\n",
       "          3.00465859e-02,  1.75691508e-02,  4.90453094e-03]],\n",
       "\n",
       "       [[-3.57106328e-02,  4.27293815e-02, -2.88544055e-02, ...,\n",
       "          3.50793861e-02, -4.66176160e-02,  4.89638560e-02],\n",
       "        [-4.71092723e-02,  3.32412161e-02,  3.97421457e-02, ...,\n",
       "          3.02657969e-02,  3.04098167e-02,  3.85107435e-02],\n",
       "        [-2.79840361e-02,  4.42895554e-02,  2.03010552e-02, ...,\n",
       "         -3.81867178e-02,  1.39027722e-02,  3.58216800e-02],\n",
       "        ...,\n",
       "        [-4.20042276e-02,  2.56568827e-02,  4.11235727e-02, ...,\n",
       "         -2.05665361e-02,  1.24825835e-02,  3.15173380e-02],\n",
       "        [ 1.74818300e-02,  7.03442097e-03,  2.19393112e-02, ...,\n",
       "          3.52237113e-02, -9.85435396e-03, -1.07921660e-04],\n",
       "        [ 3.26446183e-02,  3.54833491e-02, -4.75739129e-02, ...,\n",
       "          3.95614766e-02,  1.57867111e-02, -2.22642180e-02]],\n",
       "\n",
       "       [[-3.10653336e-02, -4.62761298e-02, -3.51119041e-03, ...,\n",
       "          2.64005177e-02, -4.64947335e-02,  3.78461815e-02],\n",
       "        [-3.14531475e-02,  3.98205593e-03, -1.01020187e-03, ...,\n",
       "          1.20545514e-02, -9.02581960e-04, -2.26480495e-02],\n",
       "        [ 1.18234865e-02,  4.16276492e-02,  2.98680998e-02, ...,\n",
       "          2.18725093e-02,  3.02855410e-02,  1.76040642e-02],\n",
       "        ...,\n",
       "        [-4.12417166e-02,  2.50990316e-03,  3.25945728e-02, ...,\n",
       "          2.35217549e-02, -1.06699355e-02, -3.94793749e-02],\n",
       "        [ 3.83201130e-02, -3.94366682e-04,  3.88566405e-03, ...,\n",
       "         -1.32735968e-02, -2.95857545e-02,  4.47099693e-02],\n",
       "        [ 2.37386301e-03,  1.95182301e-02,  3.98190953e-02, ...,\n",
       "          3.47610824e-02, -1.05669126e-02,  3.46662663e-02]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 4.66317572e-02, -8.60816240e-03, -1.28156170e-02, ...,\n",
       "         -1.06521621e-02, -4.19033170e-02, -3.00365686e-02],\n",
       "        [ 3.38991173e-02, -2.60227919e-02, -3.38451155e-02, ...,\n",
       "         -5.85718080e-03,  3.38442624e-05, -1.62121542e-02],\n",
       "        [ 6.45149499e-04,  9.44596529e-03, -6.93113729e-03, ...,\n",
       "          2.65256651e-02,  2.14218982e-02,  3.13434862e-02],\n",
       "        ...,\n",
       "        [ 1.89061277e-02, -2.63143778e-02, -2.60673463e-04, ...,\n",
       "         -1.24373063e-02, -1.57365426e-02, -1.11509338e-02],\n",
       "        [ 4.18805964e-02,  3.71170081e-02, -5.42331859e-03, ...,\n",
       "          2.93044709e-02,  1.58885382e-02, -1.93105470e-02],\n",
       "        [ 4.75899316e-02,  4.41560261e-02, -1.69016346e-02, ...,\n",
       "          4.30271775e-03,  3.83104123e-02,  4.52180393e-02]],\n",
       "\n",
       "       [[-4.75773923e-02, -3.75273116e-02, -2.26864945e-02, ...,\n",
       "         -3.06804907e-02,  4.99299802e-02,  4.08269875e-02],\n",
       "        [ 1.66247375e-02,  2.51954310e-02,  1.82617418e-02, ...,\n",
       "          1.91941299e-02, -4.34215069e-02,  9.85565037e-03],\n",
       "        [ 1.78148411e-02,  3.37292813e-02, -7.17192888e-03, ...,\n",
       "          3.21301259e-02,  2.87581421e-02,  4.62207310e-02],\n",
       "        ...,\n",
       "        [ 2.33653523e-02, -1.30403042e-02, -3.11663877e-02, ...,\n",
       "         -2.28770021e-02, -2.15767696e-03,  1.15109310e-02],\n",
       "        [-1.01348273e-02, -2.04695351e-02,  1.58627294e-02, ...,\n",
       "          2.65843607e-02,  1.87380947e-02, -3.05179488e-02],\n",
       "        [-2.86276340e-02, -4.69233058e-02,  2.89666988e-02, ...,\n",
       "          1.06341727e-02,  2.39708461e-02,  2.49942206e-02]],\n",
       "\n",
       "       [[-3.04350853e-02,  4.57785837e-02,  1.63220651e-02, ...,\n",
       "          2.68115066e-02,  1.45981200e-02,  1.81261785e-02],\n",
       "        [-4.38972004e-02,  3.89542468e-02, -3.33079584e-02, ...,\n",
       "         -1.01762526e-02,  2.94268131e-04, -1.77008137e-02],\n",
       "        [ 4.95645888e-02, -2.61839759e-02, -6.84548542e-03, ...,\n",
       "         -1.05083100e-02, -2.25598700e-02, -5.49086183e-03],\n",
       "        ...,\n",
       "        [-3.15336473e-02,  4.41922061e-02,  1.91932060e-02, ...,\n",
       "         -4.51357961e-02,  2.75616311e-02, -7.12074339e-04],\n",
       "        [ 2.60187648e-02, -4.01594266e-02, -4.21540849e-02, ...,\n",
       "         -1.72236189e-02, -6.72876835e-04,  4.41423804e-03],\n",
       "        [ 7.26381689e-03,  2.82172300e-02,  2.17457898e-02, ...,\n",
       "         -2.84889471e-02, -2.89775264e-02,  2.24024169e-02]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16, 64), dtype=bool, numpy=\n",
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]])>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_inputs._keras_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and apply an Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding layer using layers.Embedding\n",
    "# Specify input_dim, output_dim, input_length\n",
    "embedding_layer = Embedding(input_dim=501, output_dim=16, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4, 1, 16), dtype=float32, numpy=\n",
       "array([[[[ 0.03615503,  0.02699599,  0.02197993, -0.02257776,\n",
       "           0.03625304, -0.04491499, -0.03947458,  0.00571863,\n",
       "          -0.00073884,  0.03149379,  0.02724958,  0.015219  ,\n",
       "          -0.00847061, -0.03867587,  0.01018803, -0.02879494]],\n",
       "\n",
       "        [[ 0.02973011, -0.03907546,  0.0435545 , -0.01402504,\n",
       "          -0.02090627,  0.02692297,  0.03412557,  0.04161522,\n",
       "           0.00378848,  0.01749673, -0.04387602, -0.03638283,\n",
       "          -0.03651943, -0.02022092, -0.04511742,  0.02920753]],\n",
       "\n",
       "        [[ 0.04976347,  0.01325666, -0.01178676, -0.04453927,\n",
       "          -0.02589066,  0.02850509,  0.03851863,  0.00504046,\n",
       "          -0.03890175,  0.04834689, -0.03485524,  0.04128521,\n",
       "           0.04406049,  0.02423641, -0.03900483, -0.00430876]],\n",
       "\n",
       "        [[-0.01852437, -0.00169905, -0.0378039 , -0.03088001,\n",
       "           0.0288916 , -0.03454541, -0.01922549, -0.03326606,\n",
       "          -0.04663494,  0.04445828, -0.03179701, -0.04655389,\n",
       "          -0.01324985, -0.03952395, -0.02752073, -0.03977352]]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect an Embedding layer output for a fixed input\n",
    "sequence_of_indices = tf.constant([[[0], [1], [5], [500]]])\n",
    "sequence_of_embeddings = embedding_layer(sequence_of_indices)\n",
    "sequence_of_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.03615503,  0.02699599,  0.02197993, ..., -0.03867587,\n",
       "          0.01018803, -0.02879494],\n",
       "        [ 0.02973011, -0.03907546,  0.0435545 , ..., -0.02022092,\n",
       "         -0.04511742,  0.02920753],\n",
       "        [-0.04840726,  0.03174824,  0.01721177, ...,  0.01318869,\n",
       "         -0.01293137, -0.0122309 ],\n",
       "        ...,\n",
       "        [-0.018606  ,  0.00338522, -0.02874858, ...,  0.04443378,\n",
       "         -0.02466921, -0.0154874 ],\n",
       "        [-0.02641989, -0.02600249,  0.04907103, ..., -0.03444717,\n",
       "          0.02769487,  0.04433098],\n",
       "        [-0.01852437, -0.00169905, -0.0378039 , ..., -0.03952395,\n",
       "         -0.02752073, -0.03977352]], dtype=float32)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04901156, -0.02908782,  0.01971569,  0.03537515, -0.04085471,\n",
       "        0.02410069, -0.02417364,  0.01499442,  0.01682932, -0.00193727,\n",
       "        0.048031  ,  0.00896348,  0.04007876, -0.01329513, -0.01659901,\n",
       "       -0.04318047], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the embedding for the 14th index\n",
    "embedding_layer.get_weights()[0][14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a layer that uses the mask_zero kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "masking_embdding_layer = Embedding(input_dim=501, output_dim=16, mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4, 1), dtype=bool, numpy=\n",
       "array([[[False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True]]])>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply this layer to the sequence and see the _keras_mask property\n",
    "masked_sequence_of_embeddings = masking_embdding_layer(sequence_of_indices)\n",
    "masked_sequence_of_embeddings._keras_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Embedding Projector - Coding Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess the IMDb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to load and preprocess the IMDB dataset\n",
    "\n",
    "def get_and_pad_imdb_dataset(num_words=10000, maxlen=None, index_from=2):\n",
    "    from tensorflow.keras.datasets import imdb\n",
    "\n",
    "    # Load the reviews\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(path='imdb.npz',\n",
    "                                                          num_words=num_words,\n",
    "                                                          skip_top=0,\n",
    "                                                          maxlen=maxlen,\n",
    "                                                          start_char=1,\n",
    "                                                          oov_char=2,\n",
    "                                                          index_from=index_from)\n",
    "\n",
    "    x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        maxlen=None,\n",
    "                                                        padding='pre',\n",
    "                                                        truncating='pre',\n",
    "                                                        value=0)\n",
    "    \n",
    "    x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                           maxlen=None,\n",
    "                                                           padding='pre',\n",
    "                                                           truncating='pre',\n",
    "                                                           value=0)\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = get_and_pad_imdb_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to get the dataset word index\n",
    "def get_imdb_word_index(num_words=10000, index_from=2):\n",
    "    imdb_word_index = tf.keras.datasets.imdb.get_word_index(\n",
    "                                        path='imdb_word_index.json')\n",
    "    imdb_word_index = {key: value + index_from for\n",
    "                       key, value in imdb_word_index.items() if value <= num_words-index_from}\n",
    "    return imdb_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word index\n",
    "imdb_word_index = get_imdb_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swap the keys and values of the word index\n",
    "inv_imdb_word_index = {value: key for key, value in imdb_word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'am',\n",
       " 'a',\n",
       " 'great',\n",
       " 'fan',\n",
       " 'of',\n",
       " 'david',\n",
       " 'lynch',\n",
       " 'and',\n",
       " 'have',\n",
       " 'everything',\n",
       " 'that',\n",
       " \"he's\",\n",
       " 'made',\n",
       " 'on',\n",
       " 'dvd',\n",
       " 'except',\n",
       " 'for',\n",
       " 'hotel',\n",
       " 'room',\n",
       " 'the',\n",
       " '2',\n",
       " 'hour',\n",
       " 'twin',\n",
       " 'peaks',\n",
       " 'movie',\n",
       " 'so',\n",
       " 'when',\n",
       " 'i',\n",
       " 'found',\n",
       " 'out',\n",
       " 'about',\n",
       " 'this',\n",
       " 'i',\n",
       " 'immediately',\n",
       " 'grabbed',\n",
       " 'it',\n",
       " 'and',\n",
       " 'and',\n",
       " 'what',\n",
       " 'is',\n",
       " 'this',\n",
       " \"it's\",\n",
       " 'a',\n",
       " 'bunch',\n",
       " 'of',\n",
       " 'drawn',\n",
       " 'black',\n",
       " 'and',\n",
       " 'white',\n",
       " 'cartoons',\n",
       " 'that',\n",
       " 'are',\n",
       " 'loud',\n",
       " 'and',\n",
       " 'foul',\n",
       " 'mouthed',\n",
       " 'and',\n",
       " 'unfunny',\n",
       " 'maybe',\n",
       " 'i',\n",
       " \"don't\",\n",
       " 'know',\n",
       " \"what's\",\n",
       " 'good',\n",
       " 'but',\n",
       " 'maybe',\n",
       " 'this',\n",
       " 'is',\n",
       " 'just',\n",
       " 'a',\n",
       " 'bunch',\n",
       " 'of',\n",
       " 'crap',\n",
       " 'that',\n",
       " 'was',\n",
       " 'on',\n",
       " 'the',\n",
       " 'public',\n",
       " 'under',\n",
       " 'the',\n",
       " 'name',\n",
       " 'of',\n",
       " 'david',\n",
       " 'lynch',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'few',\n",
       " 'bucks',\n",
       " 'too',\n",
       " 'let',\n",
       " 'me',\n",
       " 'make',\n",
       " 'it',\n",
       " 'clear',\n",
       " 'that',\n",
       " 'i',\n",
       " \"didn't\",\n",
       " 'care',\n",
       " 'about',\n",
       " 'the',\n",
       " 'foul',\n",
       " 'language',\n",
       " 'part',\n",
       " 'but',\n",
       " 'had',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'the',\n",
       " 'sound',\n",
       " 'because',\n",
       " 'my',\n",
       " 'neighbors',\n",
       " 'might',\n",
       " 'have',\n",
       " 'all',\n",
       " 'in',\n",
       " 'all',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'highly',\n",
       " 'disappointing',\n",
       " 'release',\n",
       " 'and',\n",
       " 'may',\n",
       " 'well',\n",
       " 'have',\n",
       " 'just',\n",
       " 'been',\n",
       " 'left',\n",
       " 'in',\n",
       " 'the',\n",
       " 'box',\n",
       " 'set',\n",
       " 'as',\n",
       " 'a',\n",
       " 'curiosity',\n",
       " 'i',\n",
       " 'highly',\n",
       " 'recommend',\n",
       " 'you',\n",
       " \"don't\",\n",
       " 'spend',\n",
       " 'your',\n",
       " 'money',\n",
       " 'on',\n",
       " 'this',\n",
       " '2',\n",
       " 'out',\n",
       " 'of',\n",
       " '10']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inv_imdb_word_index[index] for index in X_train[100] if index > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an Embedding layer into a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum index value\n",
    "max_index_value = max(imdb_word_index.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify an embedding dimension\n",
    "embedding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, Dense\n",
    "\n",
    "# Build a model using sequential\n",
    "# 1. Embedding layer\n",
    "# 2. GlobalAveragePooling1D\n",
    "# 3. Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_index_value+1, output_dim=embedding_dim, mask_zero=False),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 16)          160016    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Functional API refresher.\n",
    "\n",
    "inputs = Input((None, ))\n",
    "embedding_sequence = Embedding(input_dim=max_index_value+1, output_dim=embedding_dim, mask_zero=False)(inputs)\n",
    "average_embedding = GlobalAveragePooling1D()(embedding_sequence)\n",
    "positive_probability = Dense(1, activation='sigmoid')(average_embedding)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=positive_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, None, 16)          160016    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile, train, and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.6892 - accuracy: 0.5762 - val_loss: 0.6803 - val_accuracy: 0.7312\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.6636 - accuracy: 0.7062 - val_loss: 0.6383 - val_accuracy: 0.7016\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.6141 - accuracy: 0.7602 - val_loss: 0.5904 - val_accuracy: 0.7484\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.5607 - accuracy: 0.8045 - val_loss: 0.5388 - val_accuracy: 0.7891\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.5120 - accuracy: 0.8274 - val_loss: 0.4952 - val_accuracy: 0.8016\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test), validation_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, marker='.', label='Training acc')\n",
    "plt.plot(epochs, val_acc, marker='.', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification acc')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow embedding projector\n",
    "\n",
    "The Tensorflow embedding projector can be found [here](https://projector.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the embedding layer's weights from the trained model\n",
    "weights = model.layers[1].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the word Embeddings to tsv files\n",
    "# Two files: \n",
    "#     one contains the embedding labels (meta.tsv),\n",
    "#     one contains the embeddings (vecs.tsv)\n",
    "\n",
    "k = 0\n",
    "\n",
    "for word, token in word_index.items():\n",
    "    if k != 0:\n",
    "        with open('meta.tsv', 'w', encoding='utf-8') as out_m:\n",
    "            out_m.write('\\n')\n",
    "        with open('vecs.tsv', 'w', encoding='utf-8') as out_v:\n",
    "            out_v.write('\\n')\n",
    "    \n",
    "    with open('vecs.tsv', 'w', encoding='utf-8') as out_v:\n",
    "        out_v.write('\\t'.join([str(x) for x in weights[token]]))\n",
    "    with open('meta.tsv', 'w', encoding='utf-8') as out_m:\n",
    "        out_m.write(word)\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins import projector\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(embedding=tf.Variable(model.layers[1].get_weights()[0][1:]))\n",
    "checkpoint.save('./embedding.ckpt')\n",
    "\n",
    "config = projector.ProjectorConfig()\n",
    "embedding_config = config.embeddings.add()\n",
    "\n",
    "embedding_config.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "embedding_config.metadata_path = 'meta.tsv'\n",
    "projector.visualize_embeddings('.', config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 27064."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir . --host 0.0.0.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.01042059,  0.0241474 , -0.00299377, ..., -0.01668531,\n",
       "          0.00656084, -0.0044386 ],\n",
       "        [-0.04191216, -0.00232149, -0.04823232, ..., -0.03771457,\n",
       "          0.02168761, -0.00698147],\n",
       "        [ 0.04825957, -0.03170212, -0.01626107, ..., -0.06890705,\n",
       "         -0.0202513 , -0.01327332],\n",
       "        ...,\n",
       "        [-0.31562623,  0.32271776, -0.37283412, ...,  0.30035743,\n",
       "          0.3507641 ,  0.332658  ],\n",
       "        [-0.09547014,  0.09850451, -0.14634694, ...,  0.155563  ,\n",
       "          0.18552595,  0.08848833],\n",
       "        [-0.03951053, -0.00823544,  0.01197039, ..., -0.02196658,\n",
       "          0.00353637,  0.04313335]], dtype=float32)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural network layers\n",
    "\n",
    "### Example - Fixed input length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 64, 32)            32000     \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 64)                6208      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 38,533\n",
      "Trainable params: 38,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import SimpleRNN\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(1000, 32, input_length=64),\n",
    "    SimpleRNN(64, activation='tanh'),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - Variable input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, None, 32)          32000     \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 64)                6208      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 38,533\n",
      "Trainable params: 38,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(1000, 32),\n",
    "    SimpleRNN(64, activation='tanh'),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, None, 32)          32000     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 57,157\n",
      "Trainable params: 57,157\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(1000, 32),\n",
    "    LSTM(64, activation='tanh'),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, None, 32)          32000     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 64)                18816     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 51,141\n",
      "Trainable params: 51,141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(1000, 32),\n",
    "    GRU(64, activation='tanh'),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize and pass an input to a SimpleRNN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SimpleRNN layer and test it\n",
    "simplernn_layer = SimpleRNN(units=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 16), dtype=float32, numpy=\n",
       "array([[ 1.        ,  1.        , -1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  0.84455746,  1.        ,  1.        , -1.        ,\n",
       "        -1.        , -1.        , -1.        ,  1.        ,  1.        ,\n",
       "        -1.        ]], dtype=float32)>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = tf.constant([[[1., 1.], [2., 2.], [56., -100.]]])\n",
    "layer_output = simplernn_layer(sequence)\n",
    "layer_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and transform the IMDB review sentiment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = get_and_pad_imdb_dataset(maxlen=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_word_index = get_imdb_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a recurrent neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum index value\n",
    "max_index_value = max(imdb_word_index.values())\n",
    "embedding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sequential, build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, None, 16)          160016    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 16)                2112      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 162,145\n",
      "Trainable params: 162,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=max_index_value+1, output_dim=embedding_dim, mask_zero=True),\n",
    "    LSTM(units=16),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "536/536 [==============================] - 284s 530ms/step - loss: 0.4355 - accuracy: 0.7949 - val_loss: 0.3172 - val_accuracy: 0.8531\n",
      "Epoch 2/3\n",
      "536/536 [==============================] - 277s 517ms/step - loss: 0.2255 - accuracy: 0.9151 - val_loss: 0.2912 - val_accuracy: 0.8594\n",
      "Epoch 3/3\n",
      "536/536 [==============================] - 281s 525ms/step - loss: 0.1606 - accuracy: 0.9450 - val_loss: 0.3011 - val_accuracy: 0.8656\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=3, batch_size=32, \n",
    "                    validation_data=(X_test, y_test), validation_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, marker='.', label='Training acc')\n",
    "plt.plot(epochs, val_acc, marker='.', label='Validation acc')\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification acc')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['please',\n",
       " 'give',\n",
       " 'this',\n",
       " 'one',\n",
       " 'a',\n",
       " 'miss',\n",
       " 'br',\n",
       " 'br',\n",
       " 'and',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'cast',\n",
       " 'rendered',\n",
       " 'terrible',\n",
       " 'performances',\n",
       " 'the',\n",
       " 'show',\n",
       " 'is',\n",
       " 'flat',\n",
       " 'flat',\n",
       " 'flat',\n",
       " 'br',\n",
       " 'br',\n",
       " 'i',\n",
       " \"don't\",\n",
       " 'know',\n",
       " 'how',\n",
       " 'michael',\n",
       " 'madison',\n",
       " 'could',\n",
       " 'have',\n",
       " 'allowed',\n",
       " 'this',\n",
       " 'one',\n",
       " 'on',\n",
       " 'his',\n",
       " 'plate',\n",
       " 'he',\n",
       " 'almost',\n",
       " 'seemed',\n",
       " 'to',\n",
       " 'know',\n",
       " 'this',\n",
       " \"wasn't\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'work',\n",
       " 'out',\n",
       " 'and',\n",
       " 'his',\n",
       " 'performance',\n",
       " 'was',\n",
       " 'quite',\n",
       " 'so',\n",
       " 'all',\n",
       " 'you',\n",
       " 'madison',\n",
       " 'fans',\n",
       " 'give',\n",
       " 'this',\n",
       " 'a',\n",
       " 'miss']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_imdb_word_index = {value: key for key, value in imdb_word_index.items()}\n",
    "[inv_imdb_word_index[index] for index in X_test[0] if index > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18316856]], dtype=float32)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the model prediction \n",
    "model.predict(X_test[None, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the corresponding label\n",
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
