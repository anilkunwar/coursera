{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequencing Modeling\n",
    "\n",
    "> In this post, it will cover various network architectures and layers that we can use to make predictions from sequence data. This is the summary of lecture \"Customizing your model with Tensorflow 2\" from Coursera.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Chanseok Kang\n",
    "- categories: [Python, Coursera, Deep_Learning, Tensorflow]\n",
    "- image: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (16, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing sequence data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  4, 12, 33, 18],\n",
       "       [63, 23, 54, 30, 19,  3],\n",
       "       [ 0, 43, 37, 11, 33, 15]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "test_data = [\n",
    "    [4, 12, 33, 18],\n",
    "    [63, 23, 54, 30, 19, 3],\n",
    "    [43, 37, 11, 33, 15]\n",
    "]\n",
    "\n",
    "preprocessed_data = pad_sequences(test_data, padding='pre')\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### post padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4, 12, 33, 18,  0,  0],\n",
       "       [63, 23, 54, 30, 19,  3],\n",
       "       [43, 37, 11, 33, 15,  0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data = pad_sequences(test_data, padding='post')\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post padding with maximum length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4, 12, 33, 18,  0],\n",
       "       [23, 54, 30, 19,  3],\n",
       "       [43, 37, 11, 33, 15]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data = pad_sequences(test_data, padding='post', maxlen=5)\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post padding with truncating (Default: 'pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4, 12, 33, 18,  0],\n",
       "       [63, 23, 54, 30, 19],\n",
       "       [43, 37, 11, 33, 15]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data = pad_sequences(test_data, padding='post', maxlen=5, truncating='post')\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post padding with truncating, then filled with value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4, 12, 33, 18, -1],\n",
       "       [63, 23, 54, 30, 19],\n",
       "       [43, 37, 11, 33, 15]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data = pad_sequences(test_data, padding='post', maxlen=5, truncating='post', value=-1)\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example in 2d array sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2, 1],\n",
       "        [3, 3],\n",
       "        [0, 0]],\n",
       "\n",
       "       [[4, 3],\n",
       "        [2, 4],\n",
       "        [1, 1]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = [\n",
    "    [[2, 1], [3, 3]],\n",
    "    [[4, 3], [2, 4], [1, 1]]\n",
    "]\n",
    "\n",
    "preprocessed_data = pad_sequences(test_input, padding='post')\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masking Layer for mask specific sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 6, 1), dtype=int32, numpy=\n",
       "array([[[ 4],\n",
       "        [12],\n",
       "        [33],\n",
       "        [18],\n",
       "        [ 0],\n",
       "        [ 0]],\n",
       "\n",
       "       [[63],\n",
       "        [23],\n",
       "        [54],\n",
       "        [30],\n",
       "        [19],\n",
       "        [ 3]],\n",
       "\n",
       "       [[43],\n",
       "        [37],\n",
       "        [11],\n",
       "        [33],\n",
       "        [15],\n",
       "        [ 0]]])>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Masking\n",
    "\n",
    "preprocessed_data = pad_sequences(test_data, padding='post')\n",
    "\n",
    "masking_layer = Masking(mask_value=0)\n",
    "preprocessed_data = preprocessed_data[..., tf.newaxis] # (batch_size, seq_length, features)\n",
    "masked_input = masking_layer(preprocessed_data)\n",
    "masked_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 6), dtype=bool, numpy=\n",
       "array([[ True,  True,  True,  True, False, False],\n",
       "       [ True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True, False]])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_input._keras_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the IMDB review sentiment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 22665,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 21631,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 31050,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset with different options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([list([1, 14, 22, 16, 43, 2, 2, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 2, 112, 50, 2, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 2, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 2, 38, 76, 15, 13, 2, 4, 22, 17, 2, 17, 12, 16, 2, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 2, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 2, 36, 71, 43, 2, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]),\n",
       "         list([1, 194, 2, 194, 2, 78, 228, 5, 6, 2, 2, 2, 134, 26, 4, 2, 8, 118, 2, 14, 394, 20, 13, 119, 2, 189, 102, 5, 207, 110, 2, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2, 2, 5, 2, 4, 116, 9, 35, 2, 4, 229, 9, 340, 2, 4, 118, 9, 4, 130, 2, 19, 4, 2, 5, 89, 29, 2, 46, 37, 4, 455, 9, 45, 43, 38, 2, 2, 398, 4, 2, 26, 2, 5, 163, 11, 2, 2, 4, 2, 9, 194, 2, 7, 2, 2, 349, 2, 148, 2, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 2, 98, 5, 4, 228, 9, 43, 2, 2, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 2, 228, 2, 5, 2, 2, 245, 2, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 2, 14, 9, 6, 371, 78, 22, 2, 64, 2, 9, 8, 168, 145, 23, 4, 2, 15, 16, 4, 2, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "         list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2, 311, 12, 16, 2, 33, 75, 43, 2, 296, 4, 86, 320, 35, 2, 19, 263, 2, 2, 4, 2, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 2, 43, 2, 2, 8, 257, 85, 2, 42, 2, 2, 83, 68, 2, 15, 36, 165, 2, 278, 36, 69, 2, 2, 8, 106, 14, 2, 2, 18, 6, 22, 12, 215, 28, 2, 40, 6, 87, 326, 23, 2, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2, 51, 9, 170, 23, 2, 116, 2, 2, 13, 191, 79, 2, 89, 2, 14, 9, 8, 106, 2, 2, 35, 2, 6, 227, 7, 129, 113]),\n",
       "         ...,\n",
       "         list([1, 11, 6, 230, 245, 2, 9, 6, 2, 446, 2, 45, 2, 84, 2, 2, 21, 4, 2, 84, 2, 325, 2, 134, 2, 2, 84, 5, 36, 28, 57, 2, 21, 8, 140, 8, 2, 5, 2, 84, 56, 18, 2, 14, 9, 31, 7, 4, 2, 2, 2, 2, 2, 18, 6, 20, 207, 110, 2, 12, 8, 2, 2, 8, 97, 6, 20, 53, 2, 74, 4, 460, 364, 2, 29, 270, 11, 2, 108, 45, 40, 29, 2, 395, 11, 6, 2, 2, 7, 2, 89, 364, 70, 29, 140, 4, 64, 2, 11, 4, 2, 26, 178, 4, 2, 443, 2, 5, 27, 2, 117, 2, 2, 165, 47, 84, 37, 131, 2, 14, 2, 10, 10, 61, 2, 2, 10, 10, 288, 2, 2, 34, 2, 2, 4, 65, 496, 4, 231, 7, 2, 5, 6, 320, 234, 2, 234, 2, 2, 7, 496, 4, 139, 2, 2, 2, 2, 5, 2, 18, 4, 2, 2, 250, 11, 2, 2, 4, 2, 2, 2, 2, 372, 2, 2, 2, 2, 7, 4, 59, 2, 4, 2, 2]),\n",
       "         list([1, 2, 2, 69, 72, 2, 13, 2, 2, 8, 12, 2, 23, 5, 16, 484, 2, 54, 349, 11, 2, 2, 45, 58, 2, 13, 197, 12, 16, 43, 23, 2, 5, 62, 30, 145, 402, 11, 2, 51, 2, 32, 61, 369, 71, 66, 2, 12, 2, 75, 100, 2, 8, 4, 105, 37, 69, 147, 2, 75, 2, 44, 257, 390, 5, 69, 263, 2, 105, 50, 286, 2, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 2, 13, 2, 40, 319, 2, 112, 2, 11, 2, 121, 25, 70, 2, 4, 2, 2, 13, 18, 31, 62, 40, 8, 2, 4, 2, 7, 14, 123, 5, 2, 25, 8, 2, 12, 145, 5, 202, 12, 160, 2, 202, 12, 6, 52, 58, 2, 92, 401, 2, 12, 39, 14, 251, 8, 15, 251, 5, 2, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
       "         list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 2, 270, 2, 5, 2, 2, 2, 2, 101, 405, 39, 14, 2, 4, 2, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 2, 102, 7, 4, 2, 2, 9, 24, 6, 78, 2, 17, 2, 2, 21, 27, 2, 2, 5, 2, 2, 92, 2, 4, 2, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 2, 2, 9, 6, 66, 78, 2, 4, 2, 2, 5, 2, 272, 191, 2, 6, 2, 8, 2, 2, 2, 2, 5, 383, 2, 2, 2, 2, 497, 2, 8, 2, 2, 2, 21, 60, 27, 239, 9, 43, 2, 209, 405, 10, 10, 12, 2, 40, 4, 248, 20, 12, 16, 5, 174, 2, 72, 7, 51, 6, 2, 22, 4, 204, 131, 9])],\n",
       "        dtype=object),\n",
       "  array([1, 0, 0, ..., 0, 1, 0], dtype=int64)),\n",
       " (array([list([1, 2, 202, 14, 31, 6, 2, 10, 10, 2, 2, 5, 4, 360, 7, 4, 177, 2, 394, 354, 4, 123, 9, 2, 2, 2, 10, 10, 13, 92, 124, 89, 488, 2, 100, 28, 2, 14, 31, 23, 27, 2, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 2, 38, 32, 25, 2, 451, 202, 14, 6, 2]),\n",
       "         list([1, 14, 22, 2, 6, 176, 7, 2, 88, 12, 2, 23, 2, 5, 109, 2, 4, 114, 9, 55, 2, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 2, 2, 4, 2, 2, 109, 2, 21, 4, 22, 2, 8, 6, 2, 2, 10, 10, 4, 105, 2, 35, 2, 2, 19, 2, 2, 5, 2, 2, 45, 55, 221, 15, 2, 2, 2, 14, 2, 4, 405, 5, 2, 7, 27, 85, 108, 131, 4, 2, 2, 2, 405, 9, 2, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 2, 239, 34, 2, 2, 45, 407, 31, 7, 41, 2, 105, 21, 59, 299, 12, 38, 2, 5, 2, 15, 45, 2, 488, 2, 127, 6, 52, 292, 17, 4, 2, 185, 132, 2, 2, 2, 488, 2, 47, 6, 392, 173, 4, 2, 2, 270, 2, 4, 2, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 2, 2, 7, 2, 2, 2, 5, 2, 30, 2, 2, 56, 4, 2, 5, 2, 2, 8, 4, 2, 398, 229, 10, 10, 13, 2, 2, 2, 14, 9, 31, 7, 27, 111, 108, 15, 2, 19, 2, 2, 2, 2, 14, 22, 9, 2, 21, 45, 2, 5, 45, 252, 8, 2, 6, 2, 2, 2, 39, 4, 2, 48, 25, 181, 8, 67, 35, 2, 22, 49, 238, 60, 135, 2, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 2, 8, 169, 11, 374, 2, 25, 203, 28, 8, 2, 12, 125, 4, 2]),\n",
       "         list([1, 111, 2, 2, 2, 2, 2, 4, 87, 2, 2, 7, 31, 318, 2, 7, 4, 498, 2, 2, 63, 29, 2, 220, 2, 2, 5, 17, 12, 2, 220, 2, 17, 6, 185, 132, 2, 16, 53, 2, 11, 2, 74, 4, 438, 21, 27, 2, 2, 8, 22, 107, 2, 2, 2, 2, 8, 35, 2, 2, 11, 22, 231, 54, 29, 2, 29, 100, 2, 2, 34, 2, 2, 2, 5, 2, 98, 31, 2, 33, 6, 58, 14, 2, 2, 8, 4, 365, 7, 2, 2, 356, 346, 4, 2, 2, 63, 29, 93, 11, 2, 11, 2, 33, 6, 58, 54, 2, 431, 2, 7, 32, 2, 16, 11, 94, 2, 10, 10, 4, 2, 2, 7, 4, 2, 2, 2, 2, 8, 2, 8, 2, 121, 31, 7, 27, 86, 2, 2, 16, 6, 465, 2, 2, 2, 2, 17, 2, 42, 4, 2, 37, 473, 6, 2, 6, 2, 7, 328, 212, 70, 30, 258, 11, 220, 32, 7, 108, 21, 133, 12, 9, 55, 465, 2, 2, 53, 33, 2, 2, 37, 70, 2, 4, 2, 2, 74, 476, 37, 62, 91, 2, 169, 4, 2, 2, 146, 2, 2, 5, 258, 12, 184, 2, 2, 5, 2, 2, 7, 4, 22, 2, 18, 2, 2, 2, 7, 4, 2, 71, 348, 425, 2, 2, 19, 2, 5, 2, 11, 2, 8, 339, 2, 4, 2, 2, 7, 4, 2, 10, 10, 263, 2, 9, 270, 11, 6, 2, 4, 2, 2, 121, 4, 2, 26, 2, 19, 68, 2, 5, 28, 446, 6, 318, 2, 8, 67, 51, 36, 70, 81, 8, 2, 2, 36, 2, 8, 2, 2, 18, 6, 2, 4, 2, 26, 2, 2, 11, 14, 2, 2, 12, 426, 28, 77, 2, 8, 97, 38, 111, 2, 2, 168, 2, 2, 137, 2, 18, 27, 173, 9, 2, 17, 6, 2, 428, 2, 232, 11, 4, 2, 37, 272, 40, 2, 247, 30, 2, 6, 2, 54, 2, 2, 98, 6, 2, 40, 2, 37, 2, 98, 4, 2, 2, 15, 14, 9, 57, 2, 5, 2, 6, 275, 2, 2, 2, 2, 98, 6, 2, 10, 10, 2, 19, 14, 2, 267, 162, 2, 37, 2, 2, 98, 4, 2, 2, 90, 19, 6, 2, 7, 2, 2, 2, 4, 2, 2, 2, 8, 2, 90, 4, 2, 8, 4, 2, 17, 2, 2, 2, 4, 2, 8, 2, 189, 4, 2, 2, 2, 4, 2, 5, 95, 271, 23, 6, 2, 2, 2, 2, 33, 2, 6, 425, 2, 2, 2, 2, 7, 4, 2, 2, 469, 4, 2, 54, 4, 150, 2, 2, 280, 53, 2, 2, 18, 339, 29, 2, 27, 2, 5, 2, 68, 2, 19, 2, 2, 4, 2, 7, 263, 65, 2, 34, 6, 2, 2, 43, 159, 29, 9, 2, 9, 387, 73, 195, 2, 10, 10, 2, 4, 58, 2, 54, 14, 2, 117, 22, 16, 93, 5, 2, 4, 192, 15, 12, 16, 93, 34, 6, 2, 2, 33, 4, 2, 7, 15, 2, 2, 2, 325, 12, 62, 30, 2, 8, 67, 14, 17, 6, 2, 44, 148, 2, 2, 203, 42, 203, 24, 28, 69, 2, 2, 11, 330, 54, 29, 93, 2, 21, 2, 2, 27, 2, 7, 2, 4, 22, 2, 17, 6, 2, 2, 7, 2, 2, 2, 100, 30, 4, 2, 2, 2, 2, 42, 2, 11, 4, 2, 42, 101, 2, 7, 101, 2, 15, 2, 94, 2, 180, 5, 9, 2, 34, 2, 45, 6, 2, 22, 60, 6, 2, 31, 11, 94, 2, 96, 21, 94, 2, 9, 57, 2]),\n",
       "         ...,\n",
       "         list([1, 13, 2, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 2, 21, 45, 184, 78, 4, 2, 2, 2, 2, 2, 395, 2, 5, 2, 11, 119, 2, 89, 2, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2, 284, 2, 2, 37, 315, 4, 226, 20, 272, 2, 40, 29, 152, 60, 181, 8, 30, 50, 2, 362, 80, 119, 12, 21, 2, 2]),\n",
       "         list([1, 11, 119, 241, 9, 4, 2, 20, 12, 468, 15, 94, 2, 2, 2, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2, 7, 2, 46, 2, 9, 2, 5, 4, 2, 47, 8, 79, 90, 145, 164, 162, 50, 6, 2, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 2, 200, 5, 2, 5, 9, 2, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 2, 92, 124, 51, 45, 2, 71, 2, 13, 2, 14, 20, 6, 2, 7, 470]),\n",
       "         list([1, 6, 52, 2, 430, 22, 9, 220, 2, 8, 28, 2, 2, 2, 6, 2, 15, 47, 6, 2, 2, 8, 114, 5, 33, 222, 31, 55, 184, 2, 2, 2, 19, 346, 2, 5, 6, 364, 350, 4, 184, 2, 9, 133, 2, 11, 2, 2, 21, 4, 2, 2, 2, 50, 2, 2, 9, 6, 2, 17, 6, 2, 2, 21, 17, 6, 2, 232, 2, 2, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 2, 19, 4, 78, 173, 7, 27, 2, 2, 2, 2, 2, 9, 6, 2, 17, 210, 5, 2, 2, 47, 77, 395, 14, 172, 173, 18, 2, 2, 2, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 2, 53, 40, 35, 390, 7, 11, 4, 2, 7, 4, 314, 74, 6, 2, 22, 2, 19, 2, 2, 2, 382, 4, 91, 2, 439, 19, 14, 20, 9, 2, 2, 2, 4, 2, 25, 124, 4, 31, 12, 16, 93, 2, 34, 2, 2])],\n",
       "        dtype=object),\n",
       "  array([0, 1, 1, ..., 0, 0, 0], dtype=int64)))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limit the vocabulary to the top 500 words using num_words\n",
    "imdb.load_data(num_words=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([list([2, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 2, 173, 36, 256, 2, 25, 100, 43, 838, 112, 50, 670, 2, 2, 35, 480, 284, 2, 150, 2, 172, 112, 167, 2, 336, 385, 39, 2, 172, 2, 2, 17, 546, 38, 13, 447, 2, 192, 50, 16, 2, 147, 2, 19, 14, 22, 2, 2, 2, 469, 2, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 2, 22, 17, 515, 17, 12, 16, 626, 18, 2, 2, 62, 386, 12, 2, 316, 2, 106, 2, 2, 2, 2, 16, 480, 66, 2, 33, 2, 130, 12, 16, 38, 619, 2, 25, 124, 51, 36, 135, 48, 25, 2, 33, 2, 22, 12, 215, 28, 77, 52, 2, 14, 407, 16, 82, 2, 2, 2, 107, 117, 2, 15, 256, 2, 2, 2, 2, 2, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 2, 2, 2, 2, 13, 104, 88, 2, 381, 15, 297, 98, 32, 2, 56, 26, 141, 2, 194, 2, 18, 2, 226, 22, 21, 134, 476, 26, 480, 2, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 2, 226, 65, 16, 38, 2, 88, 12, 16, 283, 2, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]),\n",
       "         list([2, 194, 2, 194, 2, 78, 228, 2, 2, 2, 2, 2, 134, 26, 2, 715, 2, 118, 2, 14, 394, 20, 13, 119, 954, 189, 102, 2, 207, 110, 2, 21, 14, 69, 188, 2, 30, 23, 2, 2, 249, 126, 93, 2, 114, 2, 2, 2, 2, 647, 2, 116, 2, 35, 2, 2, 229, 2, 340, 2, 2, 118, 2, 2, 130, 2, 19, 2, 2, 2, 89, 29, 952, 46, 37, 2, 455, 2, 45, 43, 38, 2, 2, 398, 2, 2, 26, 2, 2, 163, 11, 2, 2, 2, 2, 2, 194, 775, 2, 2, 2, 349, 2, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 2, 98, 2, 2, 228, 2, 43, 2, 2, 15, 299, 120, 2, 120, 174, 11, 220, 175, 136, 50, 2, 2, 228, 2, 2, 2, 656, 245, 2, 2, 2, 2, 131, 152, 491, 18, 2, 32, 2, 2, 14, 2, 2, 371, 78, 22, 625, 64, 2, 2, 2, 168, 145, 23, 2, 2, 15, 16, 2, 2, 2, 28, 2, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "         list([2, 14, 47, 2, 30, 31, 2, 2, 249, 108, 2, 2, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 2, 2, 311, 12, 16, 2, 33, 75, 43, 2, 296, 2, 86, 320, 35, 534, 19, 263, 2, 2, 2, 2, 33, 89, 78, 12, 66, 16, 2, 360, 2, 2, 58, 316, 334, 11, 2, 2, 43, 645, 662, 2, 257, 85, 2, 42, 2, 2, 83, 68, 2, 15, 36, 165, 2, 278, 36, 69, 2, 780, 2, 106, 14, 2, 2, 18, 2, 22, 12, 215, 28, 610, 40, 2, 87, 326, 23, 2, 21, 23, 22, 12, 272, 40, 57, 31, 11, 2, 22, 47, 2, 2, 51, 2, 170, 23, 595, 116, 595, 2, 13, 191, 79, 638, 89, 2, 14, 2, 2, 106, 607, 624, 35, 534, 2, 227, 2, 129, 113]),\n",
       "         ...,\n",
       "         list([2, 11, 2, 230, 245, 2, 2, 2, 2, 446, 2, 45, 2, 84, 2, 2, 21, 2, 912, 84, 2, 325, 725, 134, 2, 2, 84, 2, 36, 28, 57, 2, 21, 2, 140, 2, 703, 2, 2, 84, 56, 18, 2, 14, 2, 31, 2, 2, 2, 2, 2, 2, 2, 18, 2, 20, 207, 110, 563, 12, 2, 2, 2, 2, 97, 2, 20, 53, 2, 74, 2, 460, 364, 2, 29, 270, 11, 960, 108, 45, 40, 29, 2, 395, 11, 2, 2, 500, 2, 2, 89, 364, 70, 29, 140, 2, 64, 2, 11, 2, 2, 26, 178, 2, 529, 443, 2, 2, 27, 710, 117, 2, 2, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 2, 2, 10, 10, 288, 2, 2, 34, 2, 2, 2, 65, 496, 2, 231, 2, 790, 2, 2, 320, 234, 2, 234, 2, 2, 2, 496, 2, 139, 929, 2, 2, 2, 2, 2, 18, 2, 2, 2, 250, 11, 2, 2, 2, 2, 2, 747, 2, 372, 2, 2, 541, 2, 2, 2, 59, 2, 2, 2, 2]),\n",
       "         list([2, 2, 2, 69, 72, 2, 13, 610, 930, 2, 12, 582, 23, 2, 16, 484, 685, 54, 349, 11, 2, 2, 45, 58, 2, 13, 197, 12, 16, 43, 23, 2, 2, 62, 30, 145, 402, 11, 2, 51, 575, 32, 61, 369, 71, 66, 770, 12, 2, 75, 100, 2, 2, 2, 105, 37, 69, 147, 712, 75, 2, 44, 257, 390, 2, 69, 263, 514, 105, 50, 286, 2, 23, 2, 123, 13, 161, 40, 2, 421, 2, 116, 16, 897, 13, 2, 40, 319, 2, 112, 2, 11, 2, 121, 25, 70, 2, 2, 719, 2, 13, 18, 31, 62, 40, 2, 2, 2, 2, 2, 14, 123, 2, 942, 25, 2, 721, 12, 145, 2, 202, 12, 160, 580, 202, 12, 2, 52, 58, 2, 92, 401, 728, 12, 39, 14, 251, 2, 15, 251, 2, 2, 12, 38, 84, 80, 124, 12, 2, 23]),\n",
       "         list([2, 17, 2, 194, 337, 2, 2, 204, 22, 45, 254, 2, 106, 14, 123, 2, 2, 270, 2, 2, 2, 2, 732, 2, 101, 405, 39, 14, 2, 2, 2, 2, 115, 50, 305, 12, 47, 2, 168, 2, 235, 2, 38, 111, 699, 102, 2, 2, 2, 2, 2, 24, 2, 78, 2, 17, 2, 2, 21, 27, 2, 2, 2, 2, 2, 92, 2, 2, 2, 2, 2, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 2, 97, 12, 157, 21, 2, 2, 2, 2, 66, 78, 2, 2, 631, 2, 2, 2, 272, 191, 2, 2, 2, 2, 2, 2, 2, 544, 2, 383, 2, 848, 2, 2, 497, 2, 2, 2, 2, 2, 21, 60, 27, 239, 2, 43, 2, 209, 405, 10, 10, 12, 764, 40, 2, 248, 20, 12, 16, 2, 174, 2, 72, 2, 51, 2, 2, 22, 2, 204, 131, 2])],\n",
       "        dtype=object),\n",
       "  array([1, 0, 0, ..., 0, 1, 0], dtype=int64)),\n",
       " (array([list([2, 591, 202, 14, 31, 2, 717, 10, 10, 2, 2, 2, 2, 360, 2, 2, 177, 2, 394, 354, 2, 123, 2, 2, 2, 2, 10, 10, 13, 92, 124, 89, 488, 2, 100, 28, 2, 14, 31, 23, 27, 2, 29, 220, 468, 2, 124, 14, 286, 170, 2, 157, 46, 2, 27, 239, 16, 179, 2, 38, 32, 25, 2, 451, 202, 14, 2, 717]),\n",
       "         list([2, 14, 22, 2, 2, 176, 2, 2, 88, 12, 2, 23, 2, 2, 109, 943, 2, 114, 2, 55, 606, 2, 111, 2, 2, 139, 193, 273, 23, 2, 172, 270, 11, 2, 2, 2, 2, 2, 109, 2, 21, 2, 22, 2, 2, 2, 2, 2, 10, 10, 2, 105, 987, 35, 841, 2, 19, 861, 2, 2, 2, 2, 45, 55, 221, 15, 670, 2, 526, 14, 2, 2, 405, 2, 2, 2, 27, 85, 108, 131, 2, 2, 2, 2, 405, 2, 2, 133, 2, 50, 13, 104, 51, 66, 166, 14, 22, 157, 2, 2, 530, 239, 34, 2, 2, 45, 407, 31, 2, 41, 2, 105, 21, 59, 299, 12, 38, 950, 2, 2, 15, 45, 629, 488, 2, 127, 2, 52, 292, 17, 2, 2, 185, 132, 2, 2, 2, 488, 2, 47, 2, 392, 173, 2, 2, 2, 270, 2, 2, 2, 2, 2, 65, 55, 73, 11, 346, 14, 20, 2, 2, 976, 2, 2, 2, 861, 2, 2, 2, 30, 2, 2, 56, 2, 841, 2, 990, 692, 2, 2, 2, 398, 229, 10, 10, 13, 2, 670, 2, 14, 2, 31, 2, 27, 111, 108, 15, 2, 19, 2, 2, 875, 551, 14, 22, 2, 2, 21, 45, 2, 2, 45, 252, 2, 2, 2, 565, 921, 2, 39, 2, 529, 48, 25, 181, 2, 67, 35, 2, 22, 49, 238, 60, 135, 2, 14, 2, 290, 2, 58, 10, 10, 472, 45, 55, 878, 2, 169, 11, 374, 2, 25, 203, 28, 2, 818, 12, 125, 2, 2]),\n",
       "         list([2, 111, 748, 2, 2, 2, 2, 2, 87, 2, 2, 2, 31, 318, 2, 2, 2, 498, 2, 748, 63, 29, 2, 220, 686, 2, 2, 17, 12, 575, 220, 2, 17, 2, 185, 132, 2, 16, 53, 928, 11, 2, 74, 2, 438, 21, 27, 2, 589, 2, 22, 107, 2, 2, 997, 2, 2, 35, 2, 2, 11, 22, 231, 54, 29, 2, 29, 100, 2, 2, 34, 2, 2, 2, 2, 2, 98, 31, 2, 33, 2, 58, 14, 2, 2, 2, 2, 365, 2, 2, 2, 356, 346, 2, 2, 2, 63, 29, 93, 11, 2, 11, 2, 33, 2, 58, 54, 2, 431, 748, 2, 32, 2, 16, 11, 94, 2, 10, 10, 2, 993, 2, 2, 2, 2, 2, 2, 2, 2, 847, 2, 2, 121, 31, 2, 27, 86, 2, 2, 16, 2, 465, 993, 2, 2, 573, 17, 2, 42, 2, 2, 37, 473, 2, 711, 2, 2, 2, 328, 212, 70, 30, 258, 11, 220, 32, 2, 108, 21, 133, 12, 2, 55, 465, 849, 2, 53, 33, 2, 2, 37, 70, 2, 2, 2, 2, 74, 476, 37, 62, 91, 2, 169, 2, 2, 2, 146, 655, 2, 2, 258, 12, 184, 2, 546, 2, 849, 2, 2, 2, 22, 2, 18, 631, 2, 797, 2, 2, 2, 71, 348, 425, 2, 2, 19, 2, 2, 2, 11, 661, 2, 339, 2, 2, 2, 2, 2, 2, 2, 10, 10, 263, 787, 2, 270, 11, 2, 2, 2, 2, 2, 121, 2, 2, 26, 2, 19, 68, 2, 2, 28, 446, 2, 318, 2, 2, 67, 51, 36, 70, 81, 2, 2, 2, 36, 2, 2, 2, 2, 18, 2, 711, 2, 2, 26, 2, 2, 11, 14, 636, 720, 12, 426, 28, 77, 776, 2, 97, 38, 111, 2, 2, 168, 2, 2, 137, 2, 18, 27, 173, 2, 2, 17, 2, 2, 428, 2, 232, 11, 2, 2, 37, 272, 40, 2, 247, 30, 656, 2, 2, 54, 2, 2, 98, 2, 2, 40, 558, 37, 2, 98, 2, 2, 2, 15, 14, 2, 57, 2, 2, 2, 2, 275, 711, 2, 2, 2, 98, 2, 2, 10, 10, 2, 19, 14, 2, 267, 162, 711, 37, 2, 752, 98, 2, 2, 2, 90, 19, 2, 2, 2, 2, 2, 2, 2, 2, 2, 930, 2, 508, 90, 2, 2, 2, 2, 2, 17, 2, 2, 2, 2, 2, 2, 2, 189, 2, 2, 2, 2, 2, 2, 2, 95, 271, 23, 2, 2, 2, 2, 2, 33, 2, 2, 425, 2, 2, 2, 2, 2, 2, 2, 2, 469, 2, 2, 54, 2, 150, 2, 2, 280, 53, 2, 2, 18, 339, 29, 2, 27, 2, 2, 2, 68, 2, 19, 2, 2, 2, 2, 2, 263, 65, 2, 34, 2, 2, 2, 43, 159, 29, 2, 2, 2, 387, 73, 195, 584, 10, 10, 2, 2, 58, 810, 54, 14, 2, 117, 22, 16, 93, 2, 2, 2, 192, 15, 12, 16, 93, 34, 2, 2, 2, 33, 2, 2, 2, 15, 2, 2, 2, 325, 12, 62, 30, 776, 2, 67, 14, 17, 2, 2, 44, 148, 687, 2, 203, 42, 203, 24, 28, 69, 2, 2, 11, 330, 54, 29, 93, 2, 21, 845, 2, 27, 2, 2, 819, 2, 22, 2, 17, 2, 2, 787, 2, 2, 2, 2, 100, 30, 2, 2, 2, 2, 2, 42, 2, 11, 2, 2, 42, 101, 704, 2, 101, 999, 15, 2, 94, 2, 180, 2, 2, 2, 34, 2, 45, 2, 2, 22, 60, 2, 2, 31, 11, 94, 2, 96, 21, 94, 749, 2, 57, 975]),\n",
       "         ...,\n",
       "         list([2, 13, 2, 15, 2, 135, 14, 2, 35, 32, 46, 394, 20, 62, 30, 2, 21, 45, 184, 78, 2, 2, 910, 769, 2, 2, 395, 2, 2, 2, 11, 119, 2, 89, 2, 2, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 2, 185, 2, 284, 2, 2, 37, 315, 2, 226, 20, 272, 2, 40, 29, 152, 60, 181, 2, 30, 50, 553, 362, 80, 119, 12, 21, 846, 2]),\n",
       "         list([2, 11, 119, 241, 2, 2, 840, 20, 12, 468, 15, 94, 2, 562, 791, 39, 2, 86, 107, 2, 97, 14, 31, 33, 2, 2, 2, 743, 46, 2, 2, 2, 2, 2, 768, 47, 2, 79, 90, 145, 164, 162, 50, 2, 501, 119, 2, 2, 2, 78, 232, 15, 16, 224, 11, 2, 333, 20, 2, 985, 200, 2, 2, 2, 2, 2, 2, 79, 357, 2, 20, 47, 220, 57, 206, 139, 11, 12, 2, 55, 117, 212, 13, 2, 92, 124, 51, 45, 2, 71, 536, 13, 520, 14, 20, 2, 2, 2, 470]),\n",
       "         list([2, 2, 52, 2, 430, 22, 2, 220, 2, 2, 28, 2, 519, 2, 2, 769, 15, 47, 2, 2, 2, 2, 114, 2, 33, 222, 31, 55, 184, 704, 2, 2, 19, 346, 2, 2, 2, 364, 350, 2, 184, 2, 2, 133, 2, 11, 2, 2, 21, 2, 2, 2, 570, 50, 2, 2, 2, 2, 2, 17, 2, 2, 2, 21, 17, 2, 2, 232, 2, 2, 29, 266, 56, 96, 346, 194, 308, 2, 194, 21, 29, 218, 2, 19, 2, 78, 173, 2, 27, 2, 2, 2, 718, 2, 2, 2, 2, 17, 210, 2, 2, 2, 47, 77, 395, 14, 172, 173, 18, 2, 2, 2, 82, 127, 27, 173, 11, 2, 392, 217, 21, 50, 2, 57, 65, 12, 2, 53, 40, 35, 390, 2, 11, 2, 2, 2, 2, 314, 74, 2, 792, 22, 2, 19, 714, 727, 2, 382, 2, 91, 2, 439, 19, 14, 20, 2, 2, 2, 2, 2, 756, 25, 124, 2, 31, 12, 16, 93, 804, 34, 2, 2])],\n",
       "        dtype=object),\n",
       "  array([0, 1, 1, ..., 0, 0, 0], dtype=int64)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ignore the top 10 most frequent words\n",
    "imdb.load_data(skip_top=10, num_words=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
       "         list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "         list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
       "         ...,\n",
       "         list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 86527, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 14532, 325, 725, 134, 15271, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 11656, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 26094, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 17793, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 14492, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 17793, 5, 27, 710, 117, 74936, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 17793, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 17793, 7750, 5, 4241, 18, 4, 8497, 13164, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 11027, 4, 3586, 22459]),\n",
       "         list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 21469, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 40691, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 29455, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 11418, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 21213, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
       "         list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 12815, 270, 14437, 5, 16923, 12255, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 16553, 21, 27, 9685, 6139, 5, 29043, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 85010, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 70907, 10755, 544, 5, 383, 1271, 848, 1468, 12183, 497, 16876, 8, 1597, 8778, 19280, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
       "        dtype=object),\n",
       "  array([1, 0, 0, ..., 0, 1, 0], dtype=int64)),\n",
       " (array([list([1, 591, 202, 14, 31, 6, 717, 10, 10, 18142, 10698, 5, 4, 360, 7, 4, 177, 5760, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 7944, 100, 28, 1668, 14, 31, 23, 27, 7479, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 15387, 38, 32, 25, 7944, 451, 202, 14, 6, 717]),\n",
       "         list([1, 14, 22, 3443, 6, 176, 7, 5063, 88, 12, 2679, 23, 1310, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 7216, 10626, 4, 8463, 2801, 109, 1603, 21, 4, 22, 3861, 8, 6, 1193, 1330, 10, 10, 4, 105, 987, 35, 841, 16873, 19, 861, 1074, 5, 1987, 17975, 45, 55, 221, 15, 670, 5304, 526, 14, 1069, 4, 405, 5, 2438, 7, 27, 85, 108, 131, 4, 5045, 5304, 3884, 405, 9, 3523, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 8463, 2801, 45, 407, 31, 7, 41, 3778, 105, 21, 59, 299, 12, 38, 950, 5, 4521, 15, 45, 629, 488, 2733, 127, 6, 52, 292, 17, 4, 6936, 185, 132, 1988, 5304, 1799, 488, 2693, 47, 6, 392, 173, 4, 21686, 4378, 270, 2352, 4, 1500, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2078, 7, 5293, 861, 12746, 5, 4182, 30, 3127, 23651, 56, 4, 841, 5, 990, 692, 8, 4, 1669, 398, 229, 10, 10, 13, 2822, 670, 5304, 14, 9, 31, 7, 27, 111, 108, 15, 2033, 19, 7836, 1429, 875, 551, 14, 22, 9, 1193, 21, 45, 4829, 5, 45, 252, 8, 12508, 6, 565, 921, 3639, 39, 4, 529, 48, 25, 181, 8, 67, 35, 1732, 22, 49, 238, 60, 135, 1162, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 5687, 25, 203, 28, 8, 818, 12, 125, 4, 3077]),\n",
       "         list([1, 13, 1228, 119, 14, 552, 7, 20, 190, 14, 58, 13, 258, 546, 1786, 8, 1968, 4, 268, 237, 13, 191, 81, 15, 13, 80, 43, 3824, 44, 12, 14, 16, 427, 3192, 4, 183, 15, 593, 19, 4, 351, 362, 26, 55, 646, 21, 4, 1239, 84, 26, 1557, 3755, 13, 244, 6, 2071, 132, 184, 194, 5, 13, 70, 4478, 546, 73, 190, 13, 62, 24, 81, 320, 4, 538, 4, 117, 250, 127, 11, 14, 20, 82, 4, 452, 11, 14, 20, 9, 8654, 19, 41, 476, 8, 4, 213, 7, 9185, 13, 657, 13, 286, 38, 1612, 44, 41, 5, 41, 1729, 88, 13, 62, 28, 900, 510, 4, 509, 51, 6, 612, 59, 16, 193, 61, 4666, 5, 702, 930, 143, 285, 25, 67, 41, 81, 366, 4, 130, 82, 9, 259, 334, 397, 1195, 7, 149, 102, 15, 26, 814, 38, 465, 1627, 31, 70, 983, 67, 51, 9, 112, 814, 17, 35, 311, 75, 26, 11649, 574, 19, 4, 1729, 23, 4, 268, 38, 95, 138, 4, 609, 191, 75, 28, 314, 1772]),\n",
       "         ...,\n",
       "         list([1, 13, 1408, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 5093, 21, 45, 184, 78, 4, 1492, 910, 769, 2290, 2515, 395, 4257, 5, 1454, 11, 119, 16946, 89, 1036, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2280, 284, 1842, 60664, 37, 315, 4, 226, 20, 272, 2942, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 5518]),\n",
       "         list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 3684, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2960, 7, 743, 46, 1028, 9, 3531, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 28739, 5, 9, 1861, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 1276, 92, 124, 51, 45, 1188, 71, 536, 13, 520, 14, 20, 6, 2302, 7, 470]),\n",
       "         list([1, 6, 52, 7465, 430, 22, 9, 220, 2594, 8, 28, 24357, 519, 3227, 6, 769, 15, 47, 6, 3482, 4067, 8, 114, 5, 33, 222, 31, 55, 184, 704, 5586, 18020, 19, 346, 3153, 5, 6, 364, 350, 4, 184, 5586, 9, 133, 1810, 11, 5417, 13226, 21, 4, 7298, 42657, 570, 50, 2005, 2643, 9, 6, 1249, 17, 6, 25194, 27803, 21, 17, 6, 1211, 232, 1138, 2249, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 1078, 19, 4, 78, 173, 7, 27, 20067, 5698, 3406, 718, 21264, 9, 6, 6907, 17, 210, 5, 3281, 5677, 47, 77, 395, 14, 172, 173, 18, 2740, 2931, 4517, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 14274, 53, 40, 35, 390, 7, 11, 4, 3567, 7, 4, 314, 74, 6, 792, 22, 16261, 19, 714, 727, 5205, 382, 4, 91, 6533, 439, 19, 14, 20, 9, 1441, 5805, 1118, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2005, 2643])],\n",
       "        dtype=object),\n",
       "  array([0, 1, 0, ..., 0, 0, 0], dtype=int64)))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.load_data(maxlen=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
       "         list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "         list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
       "         ...,\n",
       "         list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 86527, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 14532, 325, 725, 134, 15271, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 11656, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 26094, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 17793, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 14492, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 17793, 5, 27, 710, 117, 74936, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 17793, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 17793, 7750, 5, 4241, 18, 4, 8497, 13164, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 11027, 4, 3586, 22459]),\n",
       "         list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 21469, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 40691, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 29455, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 11418, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 21213, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
       "         list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 12815, 270, 14437, 5, 16923, 12255, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 16553, 21, 27, 9685, 6139, 5, 29043, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 85010, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 70907, 10755, 544, 5, 383, 1271, 848, 1468, 12183, 497, 16876, 8, 1597, 8778, 19280, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
       "        dtype=object),\n",
       "  array([1, 0, 0, ..., 0, 1, 0], dtype=int64)),\n",
       " (array([list([1, 591, 202, 14, 31, 6, 717, 10, 10, 18142, 10698, 5, 4, 360, 7, 4, 177, 5760, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 7944, 100, 28, 1668, 14, 31, 23, 27, 7479, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 15387, 38, 32, 25, 7944, 451, 202, 14, 6, 717]),\n",
       "         list([1, 14, 22, 3443, 6, 176, 7, 5063, 88, 12, 2679, 23, 1310, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 7216, 10626, 4, 8463, 2801, 109, 1603, 21, 4, 22, 3861, 8, 6, 1193, 1330, 10, 10, 4, 105, 987, 35, 841, 16873, 19, 861, 1074, 5, 1987, 17975, 45, 55, 221, 15, 670, 5304, 526, 14, 1069, 4, 405, 5, 2438, 7, 27, 85, 108, 131, 4, 5045, 5304, 3884, 405, 9, 3523, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 8463, 2801, 45, 407, 31, 7, 41, 3778, 105, 21, 59, 299, 12, 38, 950, 5, 4521, 15, 45, 629, 488, 2733, 127, 6, 52, 292, 17, 4, 6936, 185, 132, 1988, 5304, 1799, 488, 2693, 47, 6, 392, 173, 4, 21686, 4378, 270, 2352, 4, 1500, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2078, 7, 5293, 861, 12746, 5, 4182, 30, 3127, 23651, 56, 4, 841, 5, 990, 692, 8, 4, 1669, 398, 229, 10, 10, 13, 2822, 670, 5304, 14, 9, 31, 7, 27, 111, 108, 15, 2033, 19, 7836, 1429, 875, 551, 14, 22, 9, 1193, 21, 45, 4829, 5, 45, 252, 8, 12508, 6, 565, 921, 3639, 39, 4, 529, 48, 25, 181, 8, 67, 35, 1732, 22, 49, 238, 60, 135, 1162, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 5687, 25, 203, 28, 8, 818, 12, 125, 4, 3077]),\n",
       "         list([1, 111, 748, 4368, 1133, 33782, 24563, 4, 87, 1551, 1262, 7, 31, 318, 9459, 7, 4, 498, 5076, 748, 63, 29, 5161, 220, 686, 10941, 5, 17, 12, 575, 220, 2507, 17, 6, 185, 132, 24563, 16, 53, 928, 11, 51278, 74, 4, 438, 21, 27, 10044, 589, 8, 22, 107, 20123, 19550, 997, 1638, 8, 35, 2076, 9019, 11, 22, 231, 54, 29, 1706, 29, 100, 18995, 2425, 34, 12998, 8738, 48078, 5, 19353, 98, 31, 2122, 33, 6, 58, 14, 3808, 1638, 8, 4, 365, 7, 2789, 3761, 356, 346, 4, 27608, 1060, 63, 29, 93, 11, 5421, 11, 15236, 33, 6, 58, 54, 1270, 431, 748, 7, 32, 2580, 16, 11, 94, 19469, 10, 10, 4, 993, 45222, 7, 4, 1766, 2634, 2164, 24563, 8, 847, 8, 1450, 121, 31, 7, 27, 86, 2663, 10760, 16, 6, 465, 993, 2006, 30995, 573, 17, 61862, 42, 4, 17345, 37, 473, 6, 711, 6, 8869, 7, 328, 212, 70, 30, 258, 11, 220, 32, 7, 108, 21, 133, 12, 9, 55, 465, 849, 3711, 53, 33, 2071, 1969, 37, 70, 1144, 4, 5940, 1409, 74, 476, 37, 62, 91, 1329, 169, 4, 1330, 10104, 146, 655, 2212, 5, 258, 12, 184, 10104, 546, 5, 849, 10333, 7, 4, 22, 1436, 18, 631, 1386, 797, 7, 4, 8712, 71, 348, 425, 4320, 1061, 19, 10288, 5, 12141, 11, 661, 8, 339, 17863, 4, 2455, 11434, 7, 4, 1962, 10, 10, 263, 787, 9, 270, 11, 6, 9466, 4, 61862, 48414, 121, 4, 5437, 26, 4434, 19, 68, 1372, 5, 28, 446, 6, 318, 7149, 8, 67, 51, 36, 70, 81, 8, 4392, 2294, 36, 1197, 8, 68411, 25399, 18, 6, 711, 4, 9909, 26, 10296, 1125, 11, 14, 636, 720, 12, 426, 28, 77, 776, 8, 97, 38, 111, 7489, 6175, 168, 1239, 5189, 137, 25399, 18, 27, 173, 9, 2399, 17, 6, 12397, 428, 14657, 232, 11, 4, 8014, 37, 272, 40, 2708, 247, 30, 656, 6, 13182, 54, 25399, 3292, 98, 6, 2840, 40, 558, 37, 6093, 98, 4, 17345, 1197, 15, 14, 9, 57, 4893, 5, 4659, 6, 275, 711, 7937, 25399, 3292, 98, 6, 31036, 10, 10, 6639, 19, 14, 10241, 267, 162, 711, 37, 5900, 752, 98, 4, 17345, 2378, 90, 19, 6, 73284, 7, 36744, 1810, 77553, 4, 4770, 3183, 930, 8, 508, 90, 4, 1317, 8, 4, 48414, 17, 15454, 3965, 1853, 4, 1494, 8, 4468, 189, 4, 31036, 6287, 5774, 4, 4770, 5, 95, 271, 23, 6, 7742, 6063, 21627, 5437, 33, 1526, 6, 425, 3155, 33697, 4535, 1636, 7, 4, 4669, 11966, 469, 4, 4552, 54, 4, 150, 5664, 17345, 280, 53, 68411, 25399, 18, 339, 29, 1978, 27, 7885, 5, 17303, 68, 1830, 19, 6571, 14605, 4, 1515, 7, 263, 65, 2132, 34, 6, 5680, 7489, 43, 159, 29, 9, 4706, 9, 387, 73, 195, 584, 10, 10, 1069, 4, 58, 810, 54, 14, 6078, 117, 22, 16, 93, 5, 1069, 4, 192, 15, 12, 16, 93, 34, 6, 1766, 28228, 33, 4, 5673, 7, 15, 18760, 9252, 3286, 325, 12, 62, 30, 776, 8, 67, 14, 17, 6, 12214, 44, 148, 687, 24563, 203, 42, 203, 24, 28, 69, 32157, 6676, 11, 330, 54, 29, 93, 61862, 21, 845, 14148, 27, 1099, 7, 819, 4, 22, 1407, 17, 6, 14967, 787, 7, 2460, 19569, 61862, 100, 30, 4, 3737, 3617, 3169, 2321, 42, 1898, 11, 4, 3814, 42, 101, 704, 7, 101, 999, 15, 1625, 94, 2926, 180, 5, 9, 9101, 34, 15205, 45, 6, 1429, 22, 60, 6, 1220, 31, 11, 94, 6408, 96, 21, 94, 749, 9, 57, 975]),\n",
       "         ...,\n",
       "         list([1, 13, 1408, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 5093, 21, 45, 184, 78, 4, 1492, 910, 769, 2290, 2515, 395, 4257, 5, 1454, 11, 119, 16946, 89, 1036, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2280, 284, 1842, 60664, 37, 315, 4, 226, 20, 272, 2942, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 5518]),\n",
       "         list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 3684, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2960, 7, 743, 46, 1028, 9, 3531, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 28739, 5, 9, 1861, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 1276, 92, 124, 51, 45, 1188, 71, 536, 13, 520, 14, 20, 6, 2302, 7, 470]),\n",
       "         list([1, 6, 52, 7465, 430, 22, 9, 220, 2594, 8, 28, 24357, 519, 3227, 6, 769, 15, 47, 6, 3482, 4067, 8, 114, 5, 33, 222, 31, 55, 184, 704, 5586, 18020, 19, 346, 3153, 5, 6, 364, 350, 4, 184, 5586, 9, 133, 1810, 11, 5417, 13226, 21, 4, 7298, 42657, 570, 50, 2005, 2643, 9, 6, 1249, 17, 6, 25194, 27803, 21, 17, 6, 1211, 232, 1138, 2249, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 1078, 19, 4, 78, 173, 7, 27, 20067, 5698, 3406, 718, 21264, 9, 6, 6907, 17, 210, 5, 3281, 5677, 47, 77, 395, 14, 172, 173, 18, 2740, 2931, 4517, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 14274, 53, 40, 35, 390, 7, 11, 4, 3567, 7, 4, 314, 74, 6, 792, 22, 16261, 19, 714, 727, 5205, 382, 4, 91, 6533, 439, 19, 14, 20, 9, 1441, 5805, 1118, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2005, 2643])],\n",
       "        dtype=object),\n",
       "  array([0, 1, 1, ..., 0, 0, 0], dtype=int64)))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use '1' as the character that indicates the start of a sequence\n",
    "imdb.load_data(start_char=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the dataset word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the word index as a dictionary\n",
    "# Accounting for index_from\n",
    "index_from = 3\n",
    "test = {key: value + index_from for key, value in imdb_word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fawn': 34704,\n",
       " 'tsukino': 52009,\n",
       " 'nunnery': 52010,\n",
       " 'sonja': 16819,\n",
       " 'vani': 63954,\n",
       " 'woods': 1411,\n",
       " 'spiders': 16118,\n",
       " 'hanging': 2348,\n",
       " 'woody': 2292,\n",
       " 'trawling': 52011,\n",
       " \"hold's\": 52012,\n",
       " 'comically': 11310,\n",
       " 'localized': 40833,\n",
       " 'disobeying': 30571,\n",
       " \"'royale\": 52013,\n",
       " \"harpo's\": 40834,\n",
       " 'canet': 52014,\n",
       " 'aileen': 19316,\n",
       " 'acurately': 52015,\n",
       " \"diplomat's\": 52016,\n",
       " 'rickman': 25245,\n",
       " 'arranged': 6749,\n",
       " 'rumbustious': 52017,\n",
       " 'familiarness': 52018,\n",
       " \"spider'\": 52019,\n",
       " 'hahahah': 68807,\n",
       " \"wood'\": 52020,\n",
       " 'transvestism': 40836,\n",
       " \"hangin'\": 34705,\n",
       " 'bringing': 2341,\n",
       " 'seamier': 40837,\n",
       " 'wooded': 34706,\n",
       " 'bravora': 52021,\n",
       " 'grueling': 16820,\n",
       " 'wooden': 1639,\n",
       " 'wednesday': 16821,\n",
       " \"'prix\": 52022,\n",
       " 'altagracia': 34707,\n",
       " 'circuitry': 52023,\n",
       " 'crotch': 11588,\n",
       " 'busybody': 57769,\n",
       " \"tart'n'tangy\": 52024,\n",
       " 'burgade': 14132,\n",
       " 'thrace': 52026,\n",
       " \"tom's\": 11041,\n",
       " 'snuggles': 52028,\n",
       " 'francesco': 29117,\n",
       " 'complainers': 52030,\n",
       " 'templarios': 52128,\n",
       " '272': 40838,\n",
       " '273': 52031,\n",
       " 'zaniacs': 52133,\n",
       " '275': 34709,\n",
       " 'consenting': 27634,\n",
       " 'snuggled': 40839,\n",
       " 'inanimate': 15495,\n",
       " 'uality': 52033,\n",
       " 'bronte': 11929,\n",
       " 'errors': 4013,\n",
       " 'dialogs': 3233,\n",
       " \"yomada's\": 52034,\n",
       " \"madman's\": 34710,\n",
       " 'dialoge': 30588,\n",
       " 'usenet': 52036,\n",
       " 'videodrome': 40840,\n",
       " \"kid'\": 26341,\n",
       " 'pawed': 52037,\n",
       " \"'girlfriend'\": 30572,\n",
       " \"'pleasure\": 52038,\n",
       " \"'reloaded'\": 52039,\n",
       " \"kazakos'\": 40842,\n",
       " 'rocque': 52040,\n",
       " 'mailings': 52041,\n",
       " 'brainwashed': 11930,\n",
       " 'mcanally': 16822,\n",
       " \"tom''\": 52042,\n",
       " 'kurupt': 25246,\n",
       " 'affiliated': 21908,\n",
       " 'babaganoosh': 52043,\n",
       " \"noe's\": 40843,\n",
       " 'quart': 40844,\n",
       " 'kids': 362,\n",
       " 'uplifting': 5037,\n",
       " 'controversy': 7096,\n",
       " 'kida': 21909,\n",
       " 'kidd': 23382,\n",
       " \"error'\": 52044,\n",
       " 'neurologist': 52045,\n",
       " 'spotty': 18513,\n",
       " 'cobblers': 30573,\n",
       " 'projection': 9881,\n",
       " 'fastforwarding': 40845,\n",
       " 'sters': 52046,\n",
       " \"eggar's\": 52047,\n",
       " 'etherything': 52048,\n",
       " 'gateshead': 40846,\n",
       " 'airball': 34711,\n",
       " 'unsinkable': 25247,\n",
       " 'stern': 7183,\n",
       " \"cervi's\": 52049,\n",
       " 'dnd': 40847,\n",
       " 'dna': 11589,\n",
       " 'insecurity': 20601,\n",
       " \"'reboot'\": 52050,\n",
       " 'trelkovsky': 11040,\n",
       " 'jaekel': 52051,\n",
       " 'sidebars': 52052,\n",
       " \"sforza's\": 52053,\n",
       " 'distortions': 17636,\n",
       " 'mutinies': 52054,\n",
       " 'sermons': 30605,\n",
       " '7ft': 40849,\n",
       " 'boobage': 52055,\n",
       " \"o'bannon's\": 52056,\n",
       " 'populations': 23383,\n",
       " 'chulak': 52057,\n",
       " 'mesmerize': 27636,\n",
       " 'quinnell': 52058,\n",
       " 'yahoo': 10310,\n",
       " 'meteorologist': 52060,\n",
       " 'beswick': 42580,\n",
       " 'boorman': 15496,\n",
       " 'voicework': 40850,\n",
       " \"ster'\": 52061,\n",
       " 'blustering': 22925,\n",
       " 'hj': 52062,\n",
       " 'intake': 27637,\n",
       " 'morally': 5624,\n",
       " 'jumbling': 40852,\n",
       " 'bowersock': 52063,\n",
       " \"'porky's'\": 52064,\n",
       " 'gershon': 16824,\n",
       " 'ludicrosity': 40853,\n",
       " 'coprophilia': 52065,\n",
       " 'expressively': 40854,\n",
       " \"india's\": 19503,\n",
       " \"post's\": 34713,\n",
       " 'wana': 52066,\n",
       " 'wang': 5286,\n",
       " 'wand': 30574,\n",
       " 'wane': 25248,\n",
       " 'edgeways': 52324,\n",
       " 'titanium': 34714,\n",
       " 'pinta': 40855,\n",
       " 'want': 181,\n",
       " 'pinto': 30575,\n",
       " 'whoopdedoodles': 52068,\n",
       " 'tchaikovsky': 21911,\n",
       " 'travel': 2106,\n",
       " \"'victory'\": 52069,\n",
       " 'copious': 11931,\n",
       " 'gouge': 22436,\n",
       " \"chapters'\": 52070,\n",
       " 'barbra': 6705,\n",
       " 'uselessness': 30576,\n",
       " \"wan'\": 52071,\n",
       " 'assimilated': 27638,\n",
       " 'petiot': 16119,\n",
       " 'most\\x85and': 52072,\n",
       " 'dinosaurs': 3933,\n",
       " 'wrong': 355,\n",
       " 'seda': 52073,\n",
       " 'stollen': 52074,\n",
       " 'sentencing': 34715,\n",
       " 'ouroboros': 40856,\n",
       " 'assimilates': 40857,\n",
       " 'colorfully': 40858,\n",
       " 'glenne': 27639,\n",
       " 'dongen': 52075,\n",
       " 'subplots': 4763,\n",
       " 'kiloton': 52076,\n",
       " 'chandon': 23384,\n",
       " \"effect'\": 34716,\n",
       " 'snugly': 27640,\n",
       " 'kuei': 40859,\n",
       " 'welcomed': 9095,\n",
       " 'dishonor': 30074,\n",
       " 'concurrence': 52078,\n",
       " 'stoicism': 23385,\n",
       " \"guys'\": 14899,\n",
       " \"beroemd'\": 52080,\n",
       " 'butcher': 6706,\n",
       " \"melfi's\": 40860,\n",
       " 'aargh': 30626,\n",
       " 'playhouse': 20602,\n",
       " 'wickedly': 11311,\n",
       " 'fit': 1183,\n",
       " 'labratory': 52081,\n",
       " 'lifeline': 40862,\n",
       " 'screaming': 1930,\n",
       " 'fix': 4290,\n",
       " 'cineliterate': 52082,\n",
       " 'fic': 52083,\n",
       " 'fia': 52084,\n",
       " 'fig': 34717,\n",
       " 'fmvs': 52085,\n",
       " 'fie': 52086,\n",
       " 'reentered': 52087,\n",
       " 'fin': 30577,\n",
       " 'doctresses': 52088,\n",
       " 'fil': 52089,\n",
       " 'zucker': 12609,\n",
       " 'ached': 31934,\n",
       " 'counsil': 52091,\n",
       " 'paterfamilias': 52092,\n",
       " 'songwriter': 13888,\n",
       " 'shivam': 34718,\n",
       " 'hurting': 9657,\n",
       " 'effects': 302,\n",
       " 'slauther': 52093,\n",
       " \"'flame'\": 52094,\n",
       " 'sommerset': 52095,\n",
       " 'interwhined': 52096,\n",
       " 'whacking': 27641,\n",
       " 'bartok': 52097,\n",
       " 'barton': 8778,\n",
       " 'frewer': 21912,\n",
       " \"fi'\": 52098,\n",
       " 'ingrid': 6195,\n",
       " 'stribor': 30578,\n",
       " 'approporiately': 52099,\n",
       " 'wobblyhand': 52100,\n",
       " 'tantalisingly': 52101,\n",
       " 'ankylosaurus': 52102,\n",
       " 'parasites': 17637,\n",
       " 'childen': 52103,\n",
       " \"jenkins'\": 52104,\n",
       " 'metafiction': 52105,\n",
       " 'golem': 17638,\n",
       " 'indiscretion': 40863,\n",
       " \"reeves'\": 23386,\n",
       " \"inamorata's\": 57784,\n",
       " 'brittannica': 52107,\n",
       " 'adapt': 7919,\n",
       " \"russo's\": 30579,\n",
       " 'guitarists': 48249,\n",
       " 'abbott': 10556,\n",
       " 'abbots': 40864,\n",
       " 'lanisha': 17652,\n",
       " 'magickal': 40866,\n",
       " 'mattter': 52108,\n",
       " \"'willy\": 52109,\n",
       " 'pumpkins': 34719,\n",
       " 'stuntpeople': 52110,\n",
       " 'estimate': 30580,\n",
       " 'ugghhh': 40867,\n",
       " 'gameplay': 11312,\n",
       " \"wern't\": 52111,\n",
       " \"n'sync\": 40868,\n",
       " 'sickeningly': 16120,\n",
       " 'chiara': 40869,\n",
       " 'disturbed': 4014,\n",
       " 'portmanteau': 40870,\n",
       " 'ineffectively': 52112,\n",
       " \"duchonvey's\": 82146,\n",
       " \"nasty'\": 37522,\n",
       " 'purpose': 1288,\n",
       " 'lazers': 52115,\n",
       " 'lightened': 28108,\n",
       " 'kaliganj': 52116,\n",
       " 'popularism': 52117,\n",
       " \"damme's\": 18514,\n",
       " 'stylistics': 30581,\n",
       " 'mindgaming': 52118,\n",
       " 'spoilerish': 46452,\n",
       " \"'corny'\": 52120,\n",
       " 'boerner': 34721,\n",
       " 'olds': 6795,\n",
       " 'bakelite': 52121,\n",
       " 'renovated': 27642,\n",
       " 'forrester': 27643,\n",
       " \"lumiere's\": 52122,\n",
       " 'gaskets': 52027,\n",
       " 'needed': 887,\n",
       " 'smight': 34722,\n",
       " 'master': 1300,\n",
       " \"edie's\": 25908,\n",
       " 'seeber': 40871,\n",
       " 'hiya': 52123,\n",
       " 'fuzziness': 52124,\n",
       " 'genesis': 14900,\n",
       " 'rewards': 12610,\n",
       " 'enthrall': 30582,\n",
       " \"'about\": 40872,\n",
       " \"recollection's\": 52125,\n",
       " 'mutilated': 11042,\n",
       " 'fatherlands': 52126,\n",
       " \"fischer's\": 52127,\n",
       " 'positively': 5402,\n",
       " '270': 34708,\n",
       " 'ahmed': 34723,\n",
       " 'zatoichi': 9839,\n",
       " 'bannister': 13889,\n",
       " 'anniversaries': 52130,\n",
       " \"helm's\": 30583,\n",
       " \"'work'\": 52131,\n",
       " 'exclaimed': 34724,\n",
       " \"'unfunny'\": 52132,\n",
       " '274': 52032,\n",
       " 'feeling': 547,\n",
       " \"wanda's\": 52134,\n",
       " 'dolan': 33269,\n",
       " '278': 52136,\n",
       " 'peacoat': 52137,\n",
       " 'brawny': 40873,\n",
       " 'mishra': 40874,\n",
       " 'worlders': 40875,\n",
       " 'protags': 52138,\n",
       " 'skullcap': 52139,\n",
       " 'dastagir': 57599,\n",
       " 'affairs': 5625,\n",
       " 'wholesome': 7802,\n",
       " 'hymen': 52140,\n",
       " 'paramedics': 25249,\n",
       " 'unpersons': 52141,\n",
       " 'heavyarms': 52142,\n",
       " 'affaire': 52143,\n",
       " 'coulisses': 52144,\n",
       " 'hymer': 40876,\n",
       " 'kremlin': 52145,\n",
       " 'shipments': 30584,\n",
       " 'pixilated': 52146,\n",
       " \"'00s\": 30585,\n",
       " 'diminishing': 18515,\n",
       " 'cinematic': 1360,\n",
       " 'resonates': 14901,\n",
       " 'simplify': 40877,\n",
       " \"nature'\": 40878,\n",
       " 'temptresses': 40879,\n",
       " 'reverence': 16825,\n",
       " 'resonated': 19505,\n",
       " 'dailey': 34725,\n",
       " '2\\x85': 52147,\n",
       " 'treize': 27644,\n",
       " 'majo': 52148,\n",
       " 'kiya': 21913,\n",
       " 'woolnough': 52149,\n",
       " 'thanatos': 39800,\n",
       " 'sandoval': 35734,\n",
       " 'dorama': 40882,\n",
       " \"o'shaughnessy\": 52150,\n",
       " 'tech': 4991,\n",
       " 'fugitives': 32021,\n",
       " 'teck': 30586,\n",
       " \"'e'\": 76128,\n",
       " 'doesnt': 40884,\n",
       " 'purged': 52152,\n",
       " 'saying': 660,\n",
       " \"martians'\": 41098,\n",
       " 'norliss': 23421,\n",
       " 'dickey': 27645,\n",
       " 'dicker': 52155,\n",
       " \"'sependipity\": 52156,\n",
       " 'padded': 8425,\n",
       " 'ordell': 57795,\n",
       " \"sturges'\": 40885,\n",
       " 'independentcritics': 52157,\n",
       " 'tempted': 5748,\n",
       " \"atkinson's\": 34727,\n",
       " 'hounded': 25250,\n",
       " 'apace': 52158,\n",
       " 'clicked': 15497,\n",
       " \"'humor'\": 30587,\n",
       " \"martino's\": 17180,\n",
       " \"'supporting\": 52159,\n",
       " 'warmongering': 52035,\n",
       " \"zemeckis's\": 34728,\n",
       " 'lube': 21914,\n",
       " 'shocky': 52160,\n",
       " 'plate': 7479,\n",
       " 'plata': 40886,\n",
       " 'sturgess': 40887,\n",
       " \"nerds'\": 40888,\n",
       " 'plato': 20603,\n",
       " 'plath': 34729,\n",
       " 'platt': 40889,\n",
       " 'mcnab': 52162,\n",
       " 'clumsiness': 27646,\n",
       " 'altogether': 3902,\n",
       " 'massacring': 42587,\n",
       " 'bicenntinial': 52163,\n",
       " 'skaal': 40890,\n",
       " 'droning': 14363,\n",
       " 'lds': 8779,\n",
       " 'jaguar': 21915,\n",
       " \"cale's\": 34730,\n",
       " 'nicely': 1780,\n",
       " 'mummy': 4591,\n",
       " \"lot's\": 18516,\n",
       " 'patch': 10089,\n",
       " 'kerkhof': 50205,\n",
       " \"leader's\": 52164,\n",
       " \"'movie\": 27647,\n",
       " 'uncomfirmed': 52165,\n",
       " 'heirloom': 40891,\n",
       " 'wrangle': 47363,\n",
       " 'emotion\\x85': 52166,\n",
       " \"'stargate'\": 52167,\n",
       " 'pinoy': 40892,\n",
       " 'conchatta': 40893,\n",
       " 'broeke': 41131,\n",
       " 'advisedly': 40894,\n",
       " \"barker's\": 17639,\n",
       " 'descours': 52169,\n",
       " 'lots': 775,\n",
       " 'lotr': 9262,\n",
       " 'irs': 9882,\n",
       " 'lott': 52170,\n",
       " 'xvi': 40895,\n",
       " 'irk': 34731,\n",
       " 'irl': 52171,\n",
       " 'ira': 6890,\n",
       " 'belzer': 21916,\n",
       " 'irc': 52172,\n",
       " 'ire': 27648,\n",
       " 'requisites': 40896,\n",
       " 'discipline': 7696,\n",
       " 'lyoko': 52964,\n",
       " 'extend': 11313,\n",
       " 'nature': 876,\n",
       " \"'dickie'\": 52173,\n",
       " 'optimist': 40897,\n",
       " 'lapping': 30589,\n",
       " 'superficial': 3903,\n",
       " 'vestment': 52174,\n",
       " 'extent': 2826,\n",
       " 'tendons': 52175,\n",
       " \"heller's\": 52176,\n",
       " 'quagmires': 52177,\n",
       " 'miyako': 52178,\n",
       " 'moocow': 20604,\n",
       " \"coles'\": 52179,\n",
       " 'lookit': 40898,\n",
       " 'ravenously': 52180,\n",
       " 'levitating': 40899,\n",
       " 'perfunctorily': 52181,\n",
       " 'lookin': 30590,\n",
       " \"lot'\": 40901,\n",
       " 'lookie': 52182,\n",
       " 'fearlessly': 34873,\n",
       " 'libyan': 52184,\n",
       " 'fondles': 40902,\n",
       " 'gopher': 35717,\n",
       " 'wearying': 40904,\n",
       " \"nz's\": 52185,\n",
       " 'minuses': 27649,\n",
       " 'puposelessly': 52186,\n",
       " 'shandling': 52187,\n",
       " 'decapitates': 31271,\n",
       " 'humming': 11932,\n",
       " \"'nother\": 40905,\n",
       " 'smackdown': 21917,\n",
       " 'underdone': 30591,\n",
       " 'frf': 40906,\n",
       " 'triviality': 52188,\n",
       " 'fro': 25251,\n",
       " 'bothers': 8780,\n",
       " \"'kensington\": 52189,\n",
       " 'much': 76,\n",
       " 'muco': 34733,\n",
       " 'wiseguy': 22618,\n",
       " \"richie's\": 27651,\n",
       " 'tonino': 40907,\n",
       " 'unleavened': 52190,\n",
       " 'fry': 11590,\n",
       " \"'tv'\": 40908,\n",
       " 'toning': 40909,\n",
       " 'obese': 14364,\n",
       " 'sensationalized': 30592,\n",
       " 'spiv': 40910,\n",
       " 'spit': 6262,\n",
       " 'arkin': 7367,\n",
       " 'charleton': 21918,\n",
       " 'jeon': 16826,\n",
       " 'boardroom': 21919,\n",
       " 'doubts': 4992,\n",
       " 'spin': 3087,\n",
       " 'hepo': 53086,\n",
       " 'wildcat': 27652,\n",
       " 'venoms': 10587,\n",
       " 'misconstrues': 52194,\n",
       " 'mesmerising': 18517,\n",
       " 'misconstrued': 40911,\n",
       " 'rescinds': 52195,\n",
       " 'prostrate': 52196,\n",
       " 'majid': 40912,\n",
       " 'climbed': 16482,\n",
       " 'canoeing': 34734,\n",
       " 'majin': 52198,\n",
       " 'animie': 57807,\n",
       " 'sylke': 40913,\n",
       " 'conditioned': 14902,\n",
       " 'waddell': 40914,\n",
       " '3\\x85': 52199,\n",
       " 'hyperdrive': 41191,\n",
       " 'conditioner': 34735,\n",
       " 'bricklayer': 53156,\n",
       " 'hong': 2579,\n",
       " 'memoriam': 52201,\n",
       " 'inventively': 30595,\n",
       " \"levant's\": 25252,\n",
       " 'portobello': 20641,\n",
       " 'remand': 52203,\n",
       " 'mummified': 19507,\n",
       " 'honk': 27653,\n",
       " 'spews': 19508,\n",
       " 'visitations': 40915,\n",
       " 'mummifies': 52204,\n",
       " 'cavanaugh': 25253,\n",
       " 'zeon': 23388,\n",
       " \"jungle's\": 40916,\n",
       " 'viertel': 34736,\n",
       " 'frenchmen': 27654,\n",
       " 'torpedoes': 52205,\n",
       " 'schlessinger': 52206,\n",
       " 'torpedoed': 34737,\n",
       " 'blister': 69879,\n",
       " 'cinefest': 52207,\n",
       " 'furlough': 34738,\n",
       " 'mainsequence': 52208,\n",
       " 'mentors': 40917,\n",
       " 'academic': 9097,\n",
       " 'stillness': 20605,\n",
       " 'academia': 40918,\n",
       " 'lonelier': 52209,\n",
       " 'nibby': 52210,\n",
       " \"losers'\": 52211,\n",
       " 'cineastes': 40919,\n",
       " 'corporate': 4452,\n",
       " 'massaging': 40920,\n",
       " 'bellow': 30596,\n",
       " 'absurdities': 19509,\n",
       " 'expetations': 53244,\n",
       " 'nyfiken': 40921,\n",
       " 'mehras': 75641,\n",
       " 'lasse': 52212,\n",
       " 'visability': 52213,\n",
       " 'militarily': 33949,\n",
       " \"elder'\": 52214,\n",
       " 'gainsbourg': 19026,\n",
       " 'hah': 20606,\n",
       " 'hai': 13423,\n",
       " 'haj': 34739,\n",
       " 'hak': 25254,\n",
       " 'hal': 4314,\n",
       " 'ham': 4895,\n",
       " 'duffer': 53262,\n",
       " 'haa': 52216,\n",
       " 'had': 69,\n",
       " 'advancement': 11933,\n",
       " 'hag': 16828,\n",
       " \"hand'\": 25255,\n",
       " 'hay': 13424,\n",
       " 'mcnamara': 20607,\n",
       " \"mozart's\": 52217,\n",
       " 'duffel': 30734,\n",
       " 'haq': 30597,\n",
       " 'har': 13890,\n",
       " 'has': 47,\n",
       " 'hat': 2404,\n",
       " 'hav': 40922,\n",
       " 'haw': 30598,\n",
       " 'figtings': 52218,\n",
       " 'elders': 15498,\n",
       " 'underpanted': 52219,\n",
       " 'pninson': 52220,\n",
       " 'unequivocally': 27655,\n",
       " \"barbara's\": 23676,\n",
       " \"bello'\": 52222,\n",
       " 'indicative': 13000,\n",
       " 'yawnfest': 40923,\n",
       " 'hexploitation': 52223,\n",
       " \"loder's\": 52224,\n",
       " 'sleuthing': 27656,\n",
       " \"justin's\": 32625,\n",
       " \"'ball\": 52225,\n",
       " \"'summer\": 52226,\n",
       " \"'demons'\": 34938,\n",
       " \"mormon's\": 52228,\n",
       " \"laughton's\": 34740,\n",
       " 'debell': 52229,\n",
       " 'shipyard': 39727,\n",
       " 'unabashedly': 30600,\n",
       " 'disks': 40404,\n",
       " 'crowd': 2293,\n",
       " 'crowe': 10090,\n",
       " \"vancouver's\": 56437,\n",
       " 'mosques': 34741,\n",
       " 'crown': 6630,\n",
       " 'culpas': 52230,\n",
       " 'crows': 27657,\n",
       " 'surrell': 53347,\n",
       " 'flowless': 52232,\n",
       " 'sheirk': 52233,\n",
       " \"'three\": 40926,\n",
       " \"peterson'\": 52234,\n",
       " 'ooverall': 52235,\n",
       " 'perchance': 40927,\n",
       " 'bottom': 1324,\n",
       " 'chabert': 53366,\n",
       " 'sneha': 52236,\n",
       " 'inhuman': 13891,\n",
       " 'ichii': 52237,\n",
       " 'ursla': 52238,\n",
       " 'completly': 30601,\n",
       " 'moviedom': 40928,\n",
       " 'raddick': 52239,\n",
       " 'brundage': 51998,\n",
       " 'brigades': 40929,\n",
       " 'starring': 1184,\n",
       " \"'goal'\": 52240,\n",
       " 'caskets': 52241,\n",
       " 'willcock': 52242,\n",
       " \"threesome's\": 52243,\n",
       " \"mosque'\": 52244,\n",
       " \"cover's\": 52245,\n",
       " 'spaceships': 17640,\n",
       " 'anomalous': 40930,\n",
       " 'ptsd': 27658,\n",
       " 'shirdan': 52246,\n",
       " 'obscenity': 21965,\n",
       " 'lemmings': 30602,\n",
       " 'duccio': 30603,\n",
       " \"levene's\": 52247,\n",
       " \"'gorby'\": 52248,\n",
       " \"teenager's\": 25258,\n",
       " 'marshall': 5343,\n",
       " 'honeymoon': 9098,\n",
       " 'shoots': 3234,\n",
       " 'despised': 12261,\n",
       " 'okabasho': 52249,\n",
       " 'fabric': 8292,\n",
       " 'cannavale': 18518,\n",
       " 'raped': 3540,\n",
       " \"tutt's\": 52250,\n",
       " 'grasping': 17641,\n",
       " 'despises': 18519,\n",
       " \"thief's\": 40931,\n",
       " 'rapes': 8929,\n",
       " 'raper': 52251,\n",
       " \"eyre'\": 27659,\n",
       " 'walchek': 52252,\n",
       " \"elmo's\": 23389,\n",
       " 'perfumes': 40932,\n",
       " 'spurting': 21921,\n",
       " \"exposition'\\x85\": 52253,\n",
       " 'denoting': 52254,\n",
       " 'thesaurus': 34743,\n",
       " \"shoot'\": 40933,\n",
       " 'bonejack': 49762,\n",
       " 'simpsonian': 52256,\n",
       " 'hebetude': 30604,\n",
       " \"hallow's\": 34744,\n",
       " 'desperation\\x85': 52257,\n",
       " 'incinerator': 34745,\n",
       " 'congratulations': 10311,\n",
       " 'humbled': 52258,\n",
       " \"else's\": 5927,\n",
       " 'trelkovski': 40848,\n",
       " \"rape'\": 52259,\n",
       " \"'chapters'\": 59389,\n",
       " '1600s': 52260,\n",
       " 'martian': 7256,\n",
       " 'nicest': 25259,\n",
       " 'eyred': 52262,\n",
       " 'passenger': 9460,\n",
       " 'disgrace': 6044,\n",
       " 'moderne': 52263,\n",
       " 'barrymore': 5123,\n",
       " 'yankovich': 52264,\n",
       " 'moderns': 40934,\n",
       " 'studliest': 52265,\n",
       " 'bedsheet': 52266,\n",
       " 'decapitation': 14903,\n",
       " 'slurring': 52267,\n",
       " \"'nunsploitation'\": 52268,\n",
       " \"'character'\": 34746,\n",
       " 'cambodia': 9883,\n",
       " 'rebelious': 52269,\n",
       " 'pasadena': 27660,\n",
       " 'crowne': 40935,\n",
       " \"'bedchamber\": 52270,\n",
       " 'conjectural': 52271,\n",
       " 'appologize': 52272,\n",
       " 'halfassing': 52273,\n",
       " 'paycheque': 57819,\n",
       " 'palms': 20609,\n",
       " \"'islands\": 52274,\n",
       " 'hawked': 40936,\n",
       " 'palme': 21922,\n",
       " 'conservatively': 40937,\n",
       " 'larp': 64010,\n",
       " 'palma': 5561,\n",
       " 'smelling': 21923,\n",
       " 'aragorn': 13001,\n",
       " 'hawker': 52275,\n",
       " 'hawkes': 52276,\n",
       " 'explosions': 3978,\n",
       " 'loren': 8062,\n",
       " \"pyle's\": 52277,\n",
       " 'shootout': 6707,\n",
       " \"mike's\": 18520,\n",
       " \"driscoll's\": 52278,\n",
       " 'cogsworth': 40938,\n",
       " \"britian's\": 52279,\n",
       " 'childs': 34747,\n",
       " \"portrait's\": 52280,\n",
       " 'chain': 3629,\n",
       " 'whoever': 2500,\n",
       " 'puttered': 52281,\n",
       " 'childe': 52282,\n",
       " 'maywether': 52283,\n",
       " 'chair': 3039,\n",
       " \"rance's\": 52284,\n",
       " 'machu': 34748,\n",
       " 'ballet': 4520,\n",
       " 'grapples': 34749,\n",
       " 'summerize': 76155,\n",
       " 'freelance': 30606,\n",
       " \"andrea's\": 52286,\n",
       " '\\x91very': 52287,\n",
       " 'coolidge': 45882,\n",
       " 'mache': 18521,\n",
       " 'balled': 52288,\n",
       " 'grappled': 40940,\n",
       " 'macha': 18522,\n",
       " 'underlining': 21924,\n",
       " 'macho': 5626,\n",
       " 'oversight': 19510,\n",
       " 'machi': 25260,\n",
       " 'verbally': 11314,\n",
       " 'tenacious': 21925,\n",
       " 'windshields': 40941,\n",
       " 'paychecks': 18560,\n",
       " 'jerk': 3399,\n",
       " \"good'\": 11934,\n",
       " 'prancer': 34751,\n",
       " 'prances': 21926,\n",
       " 'olympus': 52289,\n",
       " 'lark': 21927,\n",
       " 'embark': 10788,\n",
       " 'gloomy': 7368,\n",
       " 'jehaan': 52290,\n",
       " 'turaqui': 52291,\n",
       " \"child'\": 20610,\n",
       " 'locked': 2897,\n",
       " 'pranced': 52292,\n",
       " 'exact': 2591,\n",
       " 'unattuned': 52293,\n",
       " 'minute': 786,\n",
       " 'skewed': 16121,\n",
       " 'hodgins': 40943,\n",
       " 'skewer': 34752,\n",
       " 'think\\x85': 52294,\n",
       " 'rosenstein': 38768,\n",
       " 'helmit': 52295,\n",
       " 'wrestlemanias': 34753,\n",
       " 'hindered': 16829,\n",
       " \"martha's\": 30607,\n",
       " 'cheree': 52296,\n",
       " \"pluckin'\": 52297,\n",
       " 'ogles': 40944,\n",
       " 'heavyweight': 11935,\n",
       " 'aada': 82193,\n",
       " 'chopping': 11315,\n",
       " 'strongboy': 61537,\n",
       " 'hegemonic': 41345,\n",
       " 'adorns': 40945,\n",
       " 'xxth': 41349,\n",
       " 'nobuhiro': 34754,\n",
       " 'capites': 52301,\n",
       " 'kavogianni': 52302,\n",
       " 'antwerp': 13425,\n",
       " 'celebrated': 6541,\n",
       " 'roarke': 52303,\n",
       " 'baggins': 40946,\n",
       " 'cheeseburgers': 31273,\n",
       " 'matras': 52304,\n",
       " \"nineties'\": 52305,\n",
       " \"'craig'\": 52306,\n",
       " 'celebrates': 13002,\n",
       " 'unintentionally': 3386,\n",
       " 'drafted': 14365,\n",
       " 'climby': 52307,\n",
       " '303': 52308,\n",
       " 'oldies': 18523,\n",
       " 'climbs': 9099,\n",
       " 'honour': 9658,\n",
       " 'plucking': 34755,\n",
       " '305': 30077,\n",
       " 'address': 5517,\n",
       " 'menjou': 40947,\n",
       " \"'freak'\": 42595,\n",
       " 'dwindling': 19511,\n",
       " 'benson': 9461,\n",
       " 'whites': 52310,\n",
       " 'shamelessness': 40948,\n",
       " 'impacted': 21928,\n",
       " 'upatz': 52311,\n",
       " 'cusack': 3843,\n",
       " \"flavia's\": 37570,\n",
       " 'effette': 52312,\n",
       " 'influx': 34756,\n",
       " 'boooooooo': 52313,\n",
       " 'dimitrova': 52314,\n",
       " 'houseman': 13426,\n",
       " 'bigas': 25262,\n",
       " 'boylen': 52315,\n",
       " 'phillipenes': 52316,\n",
       " 'fakery': 40949,\n",
       " \"grandpa's\": 27661,\n",
       " 'darnell': 27662,\n",
       " 'undergone': 19512,\n",
       " 'handbags': 52318,\n",
       " 'perished': 21929,\n",
       " 'pooped': 37781,\n",
       " 'vigour': 27663,\n",
       " 'opposed': 3630,\n",
       " 'etude': 52319,\n",
       " \"caine's\": 11802,\n",
       " 'doozers': 52320,\n",
       " 'photojournals': 34757,\n",
       " 'perishes': 52321,\n",
       " 'constrains': 34758,\n",
       " 'migenes': 40951,\n",
       " 'consoled': 30608,\n",
       " 'alastair': 16830,\n",
       " 'wvs': 52322,\n",
       " 'ooooooh': 52323,\n",
       " 'approving': 34759,\n",
       " 'consoles': 40952,\n",
       " 'disparagement': 52067,\n",
       " 'futureistic': 52325,\n",
       " 'rebounding': 52326,\n",
       " \"'date\": 52327,\n",
       " 'gregoire': 52328,\n",
       " 'rutherford': 21930,\n",
       " 'americanised': 34760,\n",
       " 'novikov': 82199,\n",
       " 'following': 1045,\n",
       " 'munroe': 34761,\n",
       " \"morita'\": 52329,\n",
       " 'christenssen': 52330,\n",
       " 'oatmeal': 23109,\n",
       " 'fossey': 25263,\n",
       " 'livered': 40953,\n",
       " 'listens': 13003,\n",
       " \"'marci\": 76167,\n",
       " \"otis's\": 52333,\n",
       " 'thanking': 23390,\n",
       " 'maude': 16022,\n",
       " 'extensions': 34762,\n",
       " 'ameteurish': 52335,\n",
       " \"commender's\": 52336,\n",
       " 'agricultural': 27664,\n",
       " 'convincingly': 4521,\n",
       " 'fueled': 17642,\n",
       " 'mahattan': 54017,\n",
       " \"paris's\": 40955,\n",
       " 'vulkan': 52339,\n",
       " 'stapes': 52340,\n",
       " 'odysessy': 52341,\n",
       " 'harmon': 12262,\n",
       " 'surfing': 4255,\n",
       " 'halloran': 23497,\n",
       " 'unbelieveably': 49583,\n",
       " \"'offed'\": 52342,\n",
       " 'quadrant': 30610,\n",
       " 'inhabiting': 19513,\n",
       " 'nebbish': 34763,\n",
       " 'forebears': 40956,\n",
       " 'skirmish': 34764,\n",
       " 'ocassionally': 52343,\n",
       " \"'resist\": 52344,\n",
       " 'impactful': 21931,\n",
       " 'spicier': 52345,\n",
       " 'touristy': 40957,\n",
       " \"'football'\": 52346,\n",
       " 'webpage': 40958,\n",
       " 'exurbia': 52348,\n",
       " 'jucier': 52349,\n",
       " 'professors': 14904,\n",
       " 'structuring': 34765,\n",
       " 'jig': 30611,\n",
       " 'overlord': 40959,\n",
       " 'disconnect': 25264,\n",
       " 'sniffle': 82204,\n",
       " 'slimeball': 40960,\n",
       " 'jia': 40961,\n",
       " 'milked': 16831,\n",
       " 'banjoes': 40962,\n",
       " 'jim': 1240,\n",
       " 'workforces': 52351,\n",
       " 'jip': 52352,\n",
       " 'rotweiller': 52353,\n",
       " 'mundaneness': 34766,\n",
       " \"'ninja'\": 52354,\n",
       " \"dead'\": 11043,\n",
       " \"cipriani's\": 40963,\n",
       " 'modestly': 20611,\n",
       " \"professor'\": 52355,\n",
       " 'shacked': 40964,\n",
       " 'bashful': 34767,\n",
       " 'sorter': 23391,\n",
       " 'overpowering': 16123,\n",
       " 'workmanlike': 18524,\n",
       " 'henpecked': 27665,\n",
       " 'sorted': 18525,\n",
       " \"jb's\": 52357,\n",
       " \"'always\": 52358,\n",
       " \"'baptists\": 34768,\n",
       " 'dreamcatchers': 52359,\n",
       " \"'silence'\": 52360,\n",
       " 'hickory': 21932,\n",
       " 'fun\\x97yet': 52361,\n",
       " 'breakumentary': 52362,\n",
       " 'didn': 15499,\n",
       " 'didi': 52363,\n",
       " 'pealing': 52364,\n",
       " 'dispite': 40965,\n",
       " \"italy's\": 25265,\n",
       " 'instability': 21933,\n",
       " 'quarter': 6542,\n",
       " 'quartet': 12611,\n",
       " 'padm': 52365,\n",
       " \"'bleedmedry\": 52366,\n",
       " 'pahalniuk': 52367,\n",
       " 'honduras': 52368,\n",
       " 'bursting': 10789,\n",
       " \"pablo's\": 41468,\n",
       " 'irremediably': 52370,\n",
       " 'presages': 40966,\n",
       " 'bowlegged': 57835,\n",
       " 'dalip': 65186,\n",
       " 'entering': 6263,\n",
       " 'newsradio': 76175,\n",
       " 'presaged': 54153,\n",
       " \"giallo's\": 27666,\n",
       " 'bouyant': 40967,\n",
       " 'amerterish': 52371,\n",
       " 'rajni': 18526,\n",
       " 'leeves': 30613,\n",
       " 'macauley': 34770,\n",
       " 'seriously': 615,\n",
       " 'sugercoma': 52372,\n",
       " 'grimstead': 52373,\n",
       " \"'fairy'\": 52374,\n",
       " 'zenda': 30614,\n",
       " \"'twins'\": 52375,\n",
       " 'realisation': 17643,\n",
       " 'highsmith': 27667,\n",
       " 'raunchy': 7820,\n",
       " 'incentives': 40968,\n",
       " 'flatson': 52377,\n",
       " 'snooker': 35100,\n",
       " 'crazies': 16832,\n",
       " 'crazier': 14905,\n",
       " 'grandma': 7097,\n",
       " 'napunsaktha': 52378,\n",
       " 'workmanship': 30615,\n",
       " 'reisner': 52379,\n",
       " \"sanford's\": 61309,\n",
       " '\\x91doa': 52380,\n",
       " 'modest': 6111,\n",
       " \"everything's\": 19156,\n",
       " 'hamer': 40969,\n",
       " \"couldn't'\": 52382,\n",
       " 'quibble': 13004,\n",
       " 'socking': 52383,\n",
       " 'tingler': 21934,\n",
       " 'gutman': 52384,\n",
       " 'lachlan': 40970,\n",
       " 'tableaus': 52385,\n",
       " 'headbanger': 52386,\n",
       " 'spoken': 2850,\n",
       " 'cerebrally': 34771,\n",
       " \"'road\": 23493,\n",
       " 'tableaux': 21935,\n",
       " \"proust's\": 40971,\n",
       " 'periodical': 40972,\n",
       " \"shoveller's\": 52388,\n",
       " 'tamara': 25266,\n",
       " 'affords': 17644,\n",
       " 'concert': 3252,\n",
       " \"yara's\": 87958,\n",
       " 'someome': 52389,\n",
       " 'lingering': 8427,\n",
       " \"abraham's\": 41514,\n",
       " 'beesley': 34772,\n",
       " 'cherbourg': 34773,\n",
       " 'kagan': 28627,\n",
       " 'snatch': 9100,\n",
       " \"miyazaki's\": 9263,\n",
       " 'absorbs': 25267,\n",
       " \"koltai's\": 40973,\n",
       " 'tingled': 64030,\n",
       " 'crossroads': 19514,\n",
       " 'rehab': 16124,\n",
       " 'falworth': 52392,\n",
       " 'sequals': 52393,\n",
       " ...}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52256"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['simpsonian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'film',\n",
       " 'was',\n",
       " 'just',\n",
       " 'brilliant',\n",
       " 'casting',\n",
       " 'location',\n",
       " 'scenery',\n",
       " 'story',\n",
       " 'direction',\n",
       " \"everyone's\",\n",
       " 'really',\n",
       " 'suited',\n",
       " 'the',\n",
       " 'part',\n",
       " 'they',\n",
       " 'played',\n",
       " 'and',\n",
       " 'you',\n",
       " 'could',\n",
       " 'just',\n",
       " 'imagine',\n",
       " 'being',\n",
       " 'there',\n",
       " 'robert',\n",
       " \"redford's\",\n",
       " 'is',\n",
       " 'an',\n",
       " 'amazing',\n",
       " 'actor',\n",
       " 'and',\n",
       " 'now',\n",
       " 'the',\n",
       " 'same',\n",
       " 'being',\n",
       " 'director',\n",
       " \"norman's\",\n",
       " 'father',\n",
       " 'came',\n",
       " 'from',\n",
       " 'the',\n",
       " 'same',\n",
       " 'scottish',\n",
       " 'island',\n",
       " 'as',\n",
       " 'myself',\n",
       " 'so',\n",
       " 'i',\n",
       " 'loved',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'there',\n",
       " 'was',\n",
       " 'a',\n",
       " 'real',\n",
       " 'connection',\n",
       " 'with',\n",
       " 'this',\n",
       " 'film',\n",
       " 'the',\n",
       " 'witty',\n",
       " 'remarks',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'film',\n",
       " 'were',\n",
       " 'great',\n",
       " 'it',\n",
       " 'was',\n",
       " 'just',\n",
       " 'brilliant',\n",
       " 'so',\n",
       " 'much',\n",
       " 'that',\n",
       " 'i',\n",
       " 'bought',\n",
       " 'the',\n",
       " 'film',\n",
       " 'as',\n",
       " 'soon',\n",
       " 'as',\n",
       " 'it',\n",
       " 'was',\n",
       " 'released',\n",
       " 'for',\n",
       " 'retail',\n",
       " 'and',\n",
       " 'would',\n",
       " 'recommend',\n",
       " 'it',\n",
       " 'to',\n",
       " 'everyone',\n",
       " 'to',\n",
       " 'watch',\n",
       " 'and',\n",
       " 'the',\n",
       " 'fly',\n",
       " 'fishing',\n",
       " 'was',\n",
       " 'amazing',\n",
       " 'really',\n",
       " 'cried',\n",
       " 'at',\n",
       " 'the',\n",
       " 'end',\n",
       " 'it',\n",
       " 'was',\n",
       " 'so',\n",
       " 'sad',\n",
       " 'and',\n",
       " 'you',\n",
       " 'know',\n",
       " 'what',\n",
       " 'they',\n",
       " 'say',\n",
       " 'if',\n",
       " 'you',\n",
       " 'cry',\n",
       " 'at',\n",
       " 'a',\n",
       " 'film',\n",
       " 'it',\n",
       " 'must',\n",
       " 'have',\n",
       " 'been',\n",
       " 'good',\n",
       " 'and',\n",
       " 'this',\n",
       " 'definitely',\n",
       " 'was',\n",
       " 'also',\n",
       " 'congratulations',\n",
       " 'to',\n",
       " 'the',\n",
       " 'two',\n",
       " 'little',\n",
       " \"boy's\",\n",
       " 'that',\n",
       " 'played',\n",
       " 'the',\n",
       " \"part's\",\n",
       " 'of',\n",
       " 'norman',\n",
       " 'and',\n",
       " 'paul',\n",
       " 'they',\n",
       " 'were',\n",
       " 'just',\n",
       " 'brilliant',\n",
       " 'children',\n",
       " 'are',\n",
       " 'often',\n",
       " 'left',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'praising',\n",
       " 'list',\n",
       " 'i',\n",
       " 'think',\n",
       " 'because',\n",
       " 'the',\n",
       " 'stars',\n",
       " 'that',\n",
       " 'play',\n",
       " 'them',\n",
       " 'all',\n",
       " 'grown',\n",
       " 'up',\n",
       " 'are',\n",
       " 'such',\n",
       " 'a',\n",
       " 'big',\n",
       " 'profile',\n",
       " 'for',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'film',\n",
       " 'but',\n",
       " 'these',\n",
       " 'children',\n",
       " 'are',\n",
       " 'amazing',\n",
       " 'and',\n",
       " 'should',\n",
       " 'be',\n",
       " 'praised',\n",
       " 'for',\n",
       " 'what',\n",
       " 'they',\n",
       " 'have',\n",
       " 'done',\n",
       " \"don't\",\n",
       " 'you',\n",
       " 'think',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'story',\n",
       " 'was',\n",
       " 'so',\n",
       " 'lovely',\n",
       " 'because',\n",
       " 'it',\n",
       " 'was',\n",
       " 'true',\n",
       " 'and',\n",
       " 'was',\n",
       " \"someone's\",\n",
       " 'life',\n",
       " 'after',\n",
       " 'all',\n",
       " 'that',\n",
       " 'was',\n",
       " 'shared',\n",
       " 'with',\n",
       " 'us',\n",
       " 'all']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View an input sentence\n",
    "inv_imdb_word_index = {value: key for key, value in test.items()}\n",
    "[inv_imdb_word_index[index] for index in X_train[0] if index > index_from]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding and masking sequence data - Coding tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data with padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad the inputs to the maximum length using maxlen\n",
    "padded_X_train = pad_sequences(X_train, maxlen=300, padding='post', truncating='pre')\n",
    "padded_X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Masking layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([25000, 300, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Masking expects to see (batch, sequence, features)\n",
    "# Create a dummy feature dimension using expand_dims\n",
    "\n",
    "padded_X_train = tf.expand_dims(padded_X_train, -1)\n",
    "padded_X_train = tf.cast(padded_X_train, 'float32')\n",
    "padded_X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can expand it with `np.expand_dims` then, convert it to tensor with `tf.convert_to_tensor()`\n",
    "\n",
    "```python\n",
    "padded_X_train = np.expand_dims(padded_X_train, -1)\n",
    "tf_X_train = tf.convert_to_tensor(padded_X_train, dtype='float32')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a masking layer\n",
    "masking_layer = Masking(mask_value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass padded_X_train to it\n",
    "masked_X_train = masking_layer(padded_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(25000, 300, 1), dtype=float32, numpy=\n",
       "array([[[1.000e+00],\n",
       "        [1.400e+01],\n",
       "        [2.200e+01],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.940e+02],\n",
       "        [1.153e+03],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.400e+01],\n",
       "        [4.700e+01],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.100e+01],\n",
       "        [6.000e+00],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.446e+03],\n",
       "        [7.079e+03],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.700e+01],\n",
       "        [6.000e+00],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]]], dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(25000, 300, 1), dtype=float32, numpy=\n",
       "array([[[1.000e+00],\n",
       "        [1.400e+01],\n",
       "        [2.200e+01],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.940e+02],\n",
       "        [1.153e+03],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.400e+01],\n",
       "        [4.700e+01],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.100e+01],\n",
       "        [6.000e+00],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.446e+03],\n",
       "        [7.079e+03],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.700e+01],\n",
       "        [6.000e+00],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]]], dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object has no attribute '_keras_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-b0f845e22e46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpadded_X_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keras_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute '_keras_mask'"
     ]
    }
   ],
   "source": [
    "padded_X_train._keras_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(25000, 300), dtype=bool, numpy=\n",
       "array([[ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       ...,\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False]])>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_X_train._keras_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Text Data\n",
    "\n",
    "In this section, you will learn how to tokenise text data using `tf.keras.preprocessing.text.Tokenizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The text dataset\n",
    "\n",
    "The text we will work with in this notebook is Three Men in a Boat by Jerome K. Jerome, a comical short story about the perils of going outside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "with open('./dataset/ThreeMenInABoat.txt', 'r', encoding='utf-8') as f:\n",
    "    text_string = f.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform some simple preprocessing, replacing dashes with empty spaces\n",
    "text_string = text_string.replace('', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHAPTER I.   Three invalids.Sufferings of George and Harris.A victim to one hundred and seven fatal maladies.Useful prescriptions.Cure for liver complaint in children.We agree that we are overworked, and need rest.A week on the rolling deep?George suggests the River.Montmorency lodges an objection.Original motion carried by majority of three to one.  There were four of usGeorge, and William Samuel Harris, and myself, and Montmorency.  We were sitting in my room, smoking, and talking about how bad we werebad from a medical point of view I mean, of course.  We were all feeling seedy, and we were getting quite nervous about it. Harris said he felt such extraordinary fits of giddiness come over him at times, that he hardly knew what he was doing; and then George said that _he_ had fits of giddiness too, and hardly knew what _he_ was doing. With me, it was my liver that was out of order.  I knew it was my liver that was out of order, because I had just been reading a patent liver-pill circular, in which were detailed the various symptoms by which a man could tell when his liver was out of order.  I had them all.  It is a most extraordinary thing, but I never read a patent medicine advertisement without being impelled to the conclusion that I am suffering from the particular disease therein dealt with in its most virulent form.  The diagnosis seems in every case to correspond exactly with all the sensations that I have ever felt.  [Picture: Man reading book] I remember going to the British Museum one day to read up the treatment for some slight ailment of which I had a touchhay fever, I fancy it was.  I got down the book, and read all I came to read; and then, in an unthinking moment, I idly turned the leaves, and began to indolently study diseases, generally.  I forget which was the first distemper I plunged intosome fearful, devastating scourge, I knowand, before I had glanced half down the list of premonitory symptoms, it was borne in upon me that I had fairly got it.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_string[:2001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  I got down the book, and read all I came to read; and then, in an unthinking moment, I idly turned the leaves, and began to indolently study diseases, generally',\n",
       " '  I forget which was the first distemper I plunged intosome fearful, devastating scourge, I knowand, before I had glanced half down the list of premonitory symptoms, it was borne in upon me that I had fairly got it',\n",
       " '  I sat for awhile, frozen with horror; and then, in the listlessness of despair, I again turned over the pages',\n",
       " '  I came to typhoid feverread the symptomsdiscovered that I had typhoid fever, must have had it for months without knowing itwondered what else I had got; turned up St',\n",
       " ' Vituss Dancefound, as I expected, that I had that too,began to get interested in my case, and determined to sift it to the bottom, and so started alphabeticallyread up ague, and learnt that I was sickening for it, and that the acute stage would commence in about another fortnight',\n",
       " '  Brights disease, I was relieved to find, I had only in a modified form, and, so far as that was concerned, I might live for years',\n",
       " '  Cholera I had, with severe complications; and diphtheria I seemed to have been born with',\n",
       " '  I plodded conscientiously through the twenty-six letters, and the only malady I could conclude I had not got was housemaids knee',\n",
       " '  I felt rather hurt about this at first; it seemed somehow to be a sort of slight',\n",
       " '  Why hadnt I got housemaids knee?  Why this invidious reservation?  After a while, however, less grasping feelings prevailed']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the text into sentences\n",
    "sentence_strings = text_string.split('.')\n",
    "sentence_strings[20:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Tokenizer object\n",
    "\n",
    "The `Tokenizer` object allows you to easily tokenise words or characters from a text document. It has several options to allow you to adjust the tokenisation process. Documentation is available for the `Tokenizer` [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define any additional characters that we want to filter out (ignore) from the text\n",
    "additional_filters = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tokenizer has a `filters` keyword argument, that determines which characters will be filtered out from the text. The cell below shows the default characters that are filtered, to which we are adding our additional filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=None,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n' + additional_filters,\n",
    "    lower=True,\n",
    "    split=' ',\n",
    "    char_level=False,\n",
    "    oov_token='<UNK>',\n",
    "    document_count=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all, the `Tokenizer` has the following keyword arguments:\n",
    "\n",
    "`num_words`: int. the maximum number of words to keep, based on word frequency. Only the most common `num_words-1` words will be kept. If set to `None`, all words are kept.\n",
    "    \n",
    "`filters`: str. Each element is a character that will be filtered from the texts. Defaults to all punctuation (inc. tabs and line breaks), except `'`.\n",
    "\n",
    "`lower`: bool. Whether to convert the texts to lowercase. Defaults to `True`.\n",
    "\n",
    "`split`: str. Separator for word splitting. Defaults to `' '`.\n",
    "    \n",
    "`char_level`: bool. if True, every character will be treated as a token. Defaults to `False`.\n",
    "\n",
    "`oov_token`: if given, it will be added to word_index and used to replace out-of-vocabulary words during sequence_to_text calls. Defaults to `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Tokenizer to the text\n",
    "\n",
    "We can now tokenize our text using the `fit_on_texts` method. This method takes a list of strings to tokenize, as we have prepared with `sentence_strings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Tokenizer vocabulary\n",
    "tokenizer.fit_on_texts(sentence_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit_on_texts` method could also take a list of lists of strings, and in this case it would recognise each element of each sublist as an individual token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Tokenizer configuration\n",
    "\n",
    "Now that the Tokenizer has ingested the data, we can see what it has extracted from the text by viewing its configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['num_words', 'filters', 'lower', 'split', 'char_level', 'oov_token', 'document_count', 'word_counts', 'word_docs', 'index_docs', 'index_word', 'word_index'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the tokenizer config as a python dict\n",
    "tokenizer_config = tokenizer.get_config()\n",
    "tokenizer_config.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"chapter\": 19, \"i\": 1195, \"three\": 79, \"invalids\": 1, \"sufferings\": 2, \"of\": 1487, \"george\": 306, \"and\": 3375, \"harris\": 314, \"a\": 1696, \"victim\": 3, \"to\": 1785, \"one\": 241, \"hundred\": 19, \"seven\": 15, \"fatal\": 1, \"maladies\": 2, \"useful\": 2, \"prescriptions\": 1, \"cure\": 1, \"for\": 525, \"liver\": 8, \"complaint\": 2, \"in\": 976, \"children\": 13, \"we\": 866, \"agree\": 2, \"that\": 944, \"are\": 181, \"overworked\": 1, \"need\": 7, \"rest\": 14, \"week\": 19, \"on\": 501, \"the\": 3603, \"rolling\": 1, \"deep\": 18, \"suggests'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_config['word_counts'][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is the number of times each word appears in the corpus. As you can see, the word counts dictionaries in the config are serialized into plain JSON. The `loads()` method in the Python library `json` can be used to convert this JSON string into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "word_counts = json.loads(tokenizer_config['word_counts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word index is derived from the `word_counts`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"<UNK>\": 1, \"the\": 2, \"and\": 3, \"to\": 4, \"a\": 5, \"of\": 6, \"it\": 7, \"i\": 8, \"in\": 9, \"that\": 10, \"he\": 11, \"we\": 12, \"was\": 13, \"you\": 14, \"had\": 15, \"for\": 16, \"at\": 17, \"on\": 18, \"with\": 19, \"up\": 20, \"they\": 21, \"is\": 22, \"as\": 23, \"not\": 24, \"his\": 25, \"said\": 26, \"but\": 27, \"would\": 28, \"all\": 29, \"s\": 30, \"have\": 31, \"him\": 32, \"there\": 33, \"be\": 34, \"harris\": 35, \"george\": 36, \"out\": 37, \"t\": 38, \"so\": 39, \"then\": 40, \"when\": 41, \"them\": 42, \"one\": 43, \"were\": 44, \"about\": 45, \"us\": 46, \"'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_config['word_index'][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word = json.loads(tokenizer_config['index_word'])\n",
    "word_index = json.loads(tokenizer_config['word_index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the sentences to tokens\n",
    "\n",
    "You can map each sentence to a sequence of integer tokens using the Tokenizer's `texts_to_sequences()` method. As was the case for the IMDb data set, the number corresponding to a word is that word's frequency rank in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHAPTER I',\n",
       " '   Three invalids',\n",
       " 'Sufferings of George and Harris',\n",
       " 'A victim to one hundred and seven fatal maladies',\n",
       " 'Useful prescriptions']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_strings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the data\n",
    "sentence_seq = tokenizer.texts_to_sequences(sentence_strings)\n",
    "type(sentence_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[362, 8],\n",
       " [126, 3362],\n",
       " [2319, 6, 36, 3, 35],\n",
       " [5, 1779, 4, 43, 363, 3, 468, 3363, 2320],\n",
       " [2321, 3364]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_seq[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362 8\n",
      "126 3362\n",
      "2319 6 36 3 35\n",
      "5 1779 4 43 363 3 468 3363 2320\n",
      "2321 3364\n"
     ]
    }
   ],
   "source": [
    "print(word_index['chapter'], word_index['i'])\n",
    "print(word_index['three'], word_index['invalids'])\n",
    "print(word_index['sufferings'], word_index['of'], word_index['george'], word_index['and'], word_index['harris'])\n",
    "print(word_index['a'], word_index['victim'], word_index['to'], word_index['one'], word_index['hundred'], word_index['and'], word_index['seven'], word_index['fatal'], word_index['maladies'])\n",
    "print(word_index['useful'], word_index['prescriptions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the tokens to sentences\n",
    "\n",
    "You can map the tokens back to sentences using the Tokenizer's `sequences_to_texts` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[362, 8],\n",
       " [126, 3362],\n",
       " [2319, 6, 36, 3, 35],\n",
       " [5, 1779, 4, 43, 363, 3, 468, 3363, 2320],\n",
       " [2321, 3364]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_seq[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chapter i',\n",
       " 'three invalids',\n",
       " 'sufferings of george and harris',\n",
       " 'a victim to one hundred and seven fatal maladies',\n",
       " 'useful prescriptions']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map the token sequences back to sentences\n",
    "tokenizer.sequences_to_texts(sentence_seq)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chapter i\n",
      "three invalids\n",
      "sufferings of george and harris\n",
      "a victim to one hundred and seven fatal maladies\n",
      "useful prescriptions\n"
     ]
    }
   ],
   "source": [
    "# Verify the mappings in the config\n",
    "print(index_word['362'], index_word['8'])\n",
    "print(index_word['126'], index_word['3362'])\n",
    "print(index_word['2319'], index_word['6'], index_word['36'], index_word['3'], index_word['35'])\n",
    "print(index_word['5'], index_word['1779'], index_word['4'], index_word['43'], index_word['363'], index_word['3'], index_word['468'], index_word['3363'], index_word['2320'])\n",
    "print(index_word['2321'], index_word['3364'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good day world', 'montmorency bit my finger']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Any valida sequence of tokens can be converted to text\n",
    "tokenizer.sequences_to_texts([[92, 104, 241], [152, 169, 53, 2491]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a word is not featured in the Tokenizer's word index, then it will be mapped to the value of the Tokenizer's `oov_token` property. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8, 28, 78, 1, 1]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize unrecognized words\n",
    "tokenizer.texts_to_sequences(['i would like goobleydoobly hobbledyho'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<UNK>'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the OOV token\n",
    "index_word['1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further reading and resources\n",
    "\n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "* https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(input_dim=1000, output_dim=32, input_length=64, mask_zero=True)\n",
    "test_input = np.random.randint(1000, size=(16, 64))\n",
    "\n",
    "embedded_inputs = embedding_layer(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([16, 64, 32])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (batch_size, sequence, embedding_dim)\n",
    "embedded_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16, 64, 32), dtype=float32, numpy=\n",
       "array([[[ 2.81567499e-03,  2.10131891e-02, -3.96898761e-02, ...,\n",
       "         -2.66116615e-02,  3.62089388e-02, -3.65935266e-04],\n",
       "        [-4.30470705e-02,  2.24783309e-02, -2.29980238e-02, ...,\n",
       "          1.06098540e-02, -2.00324543e-02, -4.90301624e-02],\n",
       "        [-4.51265946e-02,  4.26606089e-03, -2.22358853e-03, ...,\n",
       "          4.41974513e-02,  3.86998095e-02,  2.54563130e-02],\n",
       "        ...,\n",
       "        [-2.10099705e-02, -2.06712727e-02, -1.95646640e-02, ...,\n",
       "         -1.05483644e-02, -3.03806905e-02,  8.59250873e-03],\n",
       "        [ 6.08798116e-03,  6.44494221e-03,  3.61385234e-02, ...,\n",
       "          2.71921232e-03, -6.62819296e-03,  2.06886642e-02],\n",
       "        [ 1.70254447e-02,  3.95854749e-02,  2.80608423e-02, ...,\n",
       "          4.87455241e-02,  2.42841579e-02,  2.53848173e-02]],\n",
       "\n",
       "       [[-3.00672408e-02,  2.30912082e-02,  3.05684693e-02, ...,\n",
       "         -1.23296864e-02,  3.15528549e-02,  1.78706534e-02],\n",
       "        [-2.61569619e-02,  2.96578147e-02, -1.74473524e-02, ...,\n",
       "          1.13152862e-02,  1.23596676e-02, -1.79197788e-02],\n",
       "        [-7.13679940e-03, -3.80343683e-02,  3.08545679e-03, ...,\n",
       "         -3.69194373e-02,  2.51399390e-02,  3.28949578e-02],\n",
       "        ...,\n",
       "        [ 4.16709296e-02, -4.20799963e-02, -4.10032980e-02, ...,\n",
       "         -2.42502224e-02, -3.21334749e-02,  2.14416422e-02],\n",
       "        [ 3.40003707e-02, -1.99676156e-02, -4.49220091e-03, ...,\n",
       "          6.67306036e-03,  1.26620047e-02, -1.05860010e-02],\n",
       "        [-2.56809592e-02, -4.73610424e-02,  7.88015127e-03, ...,\n",
       "          2.77104490e-02,  1.66305155e-03,  1.72116645e-02]],\n",
       "\n",
       "       [[-9.01782513e-03,  1.72769539e-02, -4.78108041e-02, ...,\n",
       "         -2.06303596e-02, -2.41944082e-02,  2.86352672e-02],\n",
       "        [ 7.21943378e-03,  4.65213992e-02,  3.49917747e-02, ...,\n",
       "          3.38277705e-02,  3.99518721e-02, -2.60806326e-02],\n",
       "        [ 1.84006579e-02,  3.10549773e-02,  2.86107548e-02, ...,\n",
       "         -3.10319550e-02, -4.87898365e-02,  3.53809446e-03],\n",
       "        ...,\n",
       "        [ 3.25400755e-03, -4.69002128e-02,  2.46371664e-02, ...,\n",
       "          2.54724175e-03, -1.20333321e-02,  9.28021967e-04],\n",
       "        [ 8.12009722e-03, -4.95495461e-02, -3.82835642e-02, ...,\n",
       "          4.99497317e-02,  3.19850780e-02,  4.95400093e-02],\n",
       "        [-1.81210153e-02, -1.73208714e-02,  1.84181966e-02, ...,\n",
       "         -1.74571201e-03, -1.06853247e-03,  3.33114713e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-6.93267584e-03, -1.98975336e-02,  1.47152692e-04, ...,\n",
       "         -3.74492407e-02,  3.39955725e-02, -3.09745073e-02],\n",
       "        [ 2.94211619e-02,  2.52574682e-06,  2.71571539e-02, ...,\n",
       "          9.48901102e-03,  4.74815033e-02, -3.97741795e-04],\n",
       "        [-2.12484729e-02,  6.72491640e-03, -2.22875122e-02, ...,\n",
       "          3.63655798e-02,  2.69971751e-02, -1.99613571e-02],\n",
       "        ...,\n",
       "        [ 2.64538638e-02, -1.84864290e-02, -4.78597656e-02, ...,\n",
       "         -2.33052969e-02, -2.19485648e-02,  3.23286764e-02],\n",
       "        [ 1.63775571e-02, -4.31616232e-03,  3.00465710e-02, ...,\n",
       "         -2.81502493e-02,  1.03712082e-03,  4.81733195e-02],\n",
       "        [-3.86986248e-02,  5.48709184e-04,  3.75761874e-02, ...,\n",
       "         -3.21803689e-02, -9.50942189e-03, -1.41590610e-02]],\n",
       "\n",
       "       [[ 2.16421373e-02,  3.85268219e-02,  4.18144204e-02, ...,\n",
       "         -4.29583304e-02, -2.41656899e-02, -2.39517447e-02],\n",
       "        [ 3.41323763e-03,  2.44268812e-02, -4.38218229e-02, ...,\n",
       "         -2.66598109e-02, -1.32768042e-02, -8.44930485e-03],\n",
       "        [-1.39304027e-02, -3.63248810e-02,  4.29272912e-02, ...,\n",
       "          2.95040347e-02,  8.04245472e-03, -3.16418633e-02],\n",
       "        ...,\n",
       "        [-3.83144617e-02, -6.01769611e-03, -2.04911232e-02, ...,\n",
       "          3.74309681e-02,  4.47872318e-02,  1.49408840e-02],\n",
       "        [ 9.05939192e-03,  1.20387673e-02, -6.44896179e-03, ...,\n",
       "          2.08939798e-02,  6.71445206e-03, -3.70318294e-02],\n",
       "        [-2.61569619e-02,  2.96578147e-02, -1.74473524e-02, ...,\n",
       "          1.13152862e-02,  1.23596676e-02, -1.79197788e-02]],\n",
       "\n",
       "       [[-5.82249090e-03, -1.11140385e-02,  3.94122638e-02, ...,\n",
       "          1.86590813e-02, -3.28295976e-02,  3.30119617e-02],\n",
       "        [-4.00455706e-02,  2.24814676e-02, -3.22625265e-02, ...,\n",
       "         -4.33975942e-02, -6.90705702e-03, -2.71656048e-02],\n",
       "        [-7.96044990e-03, -4.73088622e-02,  4.88929488e-02, ...,\n",
       "          3.35184820e-02, -4.99430075e-02, -3.06885596e-02],\n",
       "        ...,\n",
       "        [ 3.84024419e-02,  8.60388204e-03,  2.90149711e-02, ...,\n",
       "          3.75747122e-02,  1.21750347e-02, -2.26478335e-02],\n",
       "        [-5.84471971e-04, -1.82863325e-03,  1.94530599e-02, ...,\n",
       "         -3.40794399e-03,  1.97619312e-02,  2.54274942e-02],\n",
       "        [-4.16493192e-02, -8.46403837e-03,  1.75357200e-02, ...,\n",
       "          4.99112122e-02, -1.91973336e-02, -2.26595048e-02]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16, 64), dtype=bool, numpy=\n",
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]])>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_inputs._keras_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and apply an Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding layer using layers.Embedding\n",
    "# Specify input_dim, output_dim, input_length\n",
    "embedding_layer = Embedding(input_dim=501, output_dim=16, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4, 1, 16), dtype=float32, numpy=\n",
       "array([[[[ 0.01946225, -0.02479618,  0.01827569, -0.04427719,\n",
       "           0.03377054,  0.0257794 ,  0.03393973, -0.03865162,\n",
       "          -0.00638528, -0.04464755, -0.04334855,  0.03885332,\n",
       "           0.02062796,  0.03248196, -0.04557872, -0.02933714]],\n",
       "\n",
       "        [[ 0.04105124, -0.02397075, -0.03625159, -0.01970358,\n",
       "          -0.03417058,  0.03670056,  0.00848722, -0.04011478,\n",
       "          -0.0148047 , -0.04938881,  0.0183324 , -0.0035557 ,\n",
       "           0.00230777, -0.01670351,  0.00925242, -0.02079442]],\n",
       "\n",
       "        [[ 0.02852968,  0.0055367 ,  0.04847244,  0.00098878,\n",
       "           0.00494426,  0.02346985, -0.01505352,  0.0183579 ,\n",
       "           0.00381358,  0.0285494 ,  0.01183083, -0.01646228,\n",
       "           0.02696766, -0.00062073,  0.03969332,  0.03658456]],\n",
       "\n",
       "        [[ 0.00924232, -0.03488412,  0.02969712,  0.03395958,\n",
       "          -0.04848244,  0.0122721 ,  0.02069709, -0.02639389,\n",
       "          -0.02024229, -0.04675431,  0.02512041, -0.02461251,\n",
       "           0.00205641, -0.01812328, -0.04241917,  0.00648856]]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect an Embedding layer output for a fixed input\n",
    "sequence_of_indices = tf.constant([[[0], [1], [5], [500]]])\n",
    "sequence_of_embeddings = embedding_layer(sequence_of_indices)\n",
    "sequence_of_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.01946225, -0.02479618,  0.01827569, ...,  0.03248196,\n",
       "         -0.04557872, -0.02933714],\n",
       "        [ 0.04105124, -0.02397075, -0.03625159, ..., -0.01670351,\n",
       "          0.00925242, -0.02079442],\n",
       "        [ 0.02708561,  0.03803079, -0.02646797, ...,  0.00310315,\n",
       "         -0.0261147 ,  0.02336366],\n",
       "        ...,\n",
       "        [-0.00200007, -0.01418354,  0.03642055, ...,  0.03924714,\n",
       "         -0.02593886, -0.04372653],\n",
       "        [-0.04381604,  0.03934881,  0.00856291, ..., -0.01755339,\n",
       "         -0.04893535,  0.00029821],\n",
       "        [ 0.00924232, -0.03488412,  0.02969712, ..., -0.01812328,\n",
       "         -0.04241917,  0.00648856]], dtype=float32)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03645971,  0.03282349, -0.02996556, -0.00856809,  0.00849456,\n",
       "       -0.04225675, -0.03091792, -0.01070333, -0.03835428,  0.00934491,\n",
       "       -0.04702229, -0.03091754,  0.02984029,  0.04235016,  0.00289961,\n",
       "       -0.01689137], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the embedding for the 14th index\n",
    "embedding_layer.get_weights()[0][14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a layer that uses the mask_zero kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "masking_embdding_layer = Embedding(input_dim=501, output_dim=16, mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4, 1), dtype=bool, numpy=\n",
       "array([[[False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True]]])>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply this layer to the sequence and see the _keras_mask property\n",
    "masked_sequence_of_embeddings = masking_embdding_layer(sequence_of_indices)\n",
    "masked_sequence_of_embeddings._keras_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Embedding Projector - Coding Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess the IMDb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to load and preprocess the IMDB dataset\n",
    "\n",
    "def get_and_pad_imdb_dataset(num_words=10000, maxlen=None, index_from=2):\n",
    "    from tensorflow.keras.datasets import imdb\n",
    "\n",
    "    # Load the reviews\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(path='imdb.npz',\n",
    "                                                          num_words=num_words,\n",
    "                                                          skip_top=0,\n",
    "                                                          maxlen=maxlen,\n",
    "                                                          start_char=1,\n",
    "                                                          oov_char=2,\n",
    "                                                          index_from=index_from)\n",
    "\n",
    "    x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        maxlen=None,\n",
    "                                                        padding='pre',\n",
    "                                                        truncating='pre',\n",
    "                                                        value=0)\n",
    "    \n",
    "    x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                           maxlen=None,\n",
    "                                                           padding='pre',\n",
    "                                                           truncating='pre',\n",
    "                                                           value=0)\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = get_and_pad_imdb_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to get the dataset word index\n",
    "def get_imdb_word_index(num_words=10000, index_from=2):\n",
    "    imdb_word_index = tf.keras.datasets.imdb.get_word_index(\n",
    "                                        path='imdb_word_index.json')\n",
    "    imdb_word_index = {key: value + index_from for\n",
    "                       key, value in imdb_word_index.items() if value <= num_words-index_from}\n",
    "    return imdb_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word index\n",
    "imdb_word_index = get_imdb_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swap the keys and values of the word index\n",
    "inv_imdb_word_index = {value: key for key, value in imdb_word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'am',\n",
       " 'a',\n",
       " 'great',\n",
       " 'fan',\n",
       " 'of',\n",
       " 'david',\n",
       " 'lynch',\n",
       " 'and',\n",
       " 'have',\n",
       " 'everything',\n",
       " 'that',\n",
       " \"he's\",\n",
       " 'made',\n",
       " 'on',\n",
       " 'dvd',\n",
       " 'except',\n",
       " 'for',\n",
       " 'hotel',\n",
       " 'room',\n",
       " 'the',\n",
       " '2',\n",
       " 'hour',\n",
       " 'twin',\n",
       " 'peaks',\n",
       " 'movie',\n",
       " 'so',\n",
       " 'when',\n",
       " 'i',\n",
       " 'found',\n",
       " 'out',\n",
       " 'about',\n",
       " 'this',\n",
       " 'i',\n",
       " 'immediately',\n",
       " 'grabbed',\n",
       " 'it',\n",
       " 'and',\n",
       " 'and',\n",
       " 'what',\n",
       " 'is',\n",
       " 'this',\n",
       " \"it's\",\n",
       " 'a',\n",
       " 'bunch',\n",
       " 'of',\n",
       " 'drawn',\n",
       " 'black',\n",
       " 'and',\n",
       " 'white',\n",
       " 'cartoons',\n",
       " 'that',\n",
       " 'are',\n",
       " 'loud',\n",
       " 'and',\n",
       " 'foul',\n",
       " 'mouthed',\n",
       " 'and',\n",
       " 'unfunny',\n",
       " 'maybe',\n",
       " 'i',\n",
       " \"don't\",\n",
       " 'know',\n",
       " \"what's\",\n",
       " 'good',\n",
       " 'but',\n",
       " 'maybe',\n",
       " 'this',\n",
       " 'is',\n",
       " 'just',\n",
       " 'a',\n",
       " 'bunch',\n",
       " 'of',\n",
       " 'crap',\n",
       " 'that',\n",
       " 'was',\n",
       " 'on',\n",
       " 'the',\n",
       " 'public',\n",
       " 'under',\n",
       " 'the',\n",
       " 'name',\n",
       " 'of',\n",
       " 'david',\n",
       " 'lynch',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'few',\n",
       " 'bucks',\n",
       " 'too',\n",
       " 'let',\n",
       " 'me',\n",
       " 'make',\n",
       " 'it',\n",
       " 'clear',\n",
       " 'that',\n",
       " 'i',\n",
       " \"didn't\",\n",
       " 'care',\n",
       " 'about',\n",
       " 'the',\n",
       " 'foul',\n",
       " 'language',\n",
       " 'part',\n",
       " 'but',\n",
       " 'had',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'the',\n",
       " 'sound',\n",
       " 'because',\n",
       " 'my',\n",
       " 'neighbors',\n",
       " 'might',\n",
       " 'have',\n",
       " 'all',\n",
       " 'in',\n",
       " 'all',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'highly',\n",
       " 'disappointing',\n",
       " 'release',\n",
       " 'and',\n",
       " 'may',\n",
       " 'well',\n",
       " 'have',\n",
       " 'just',\n",
       " 'been',\n",
       " 'left',\n",
       " 'in',\n",
       " 'the',\n",
       " 'box',\n",
       " 'set',\n",
       " 'as',\n",
       " 'a',\n",
       " 'curiosity',\n",
       " 'i',\n",
       " 'highly',\n",
       " 'recommend',\n",
       " 'you',\n",
       " \"don't\",\n",
       " 'spend',\n",
       " 'your',\n",
       " 'money',\n",
       " 'on',\n",
       " 'this',\n",
       " '2',\n",
       " 'out',\n",
       " 'of',\n",
       " '10']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inv_imdb_word_index[index] for index in X_train[100] if index > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an Embedding layer into a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum index value\n",
    "max_index_value = max(imdb_word_index.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify an embedding dimension\n",
    "embedding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, Dense\n",
    "\n",
    "# Build a model using sequential\n",
    "# 1. Embedding layer\n",
    "# 2. GlobalAveragePooling1D\n",
    "# 3. Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_index_value+1, output_dim=embedding_dim, mask_zero=False),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 16)          160016    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Functional API refresher.\n",
    "\n",
    "inputs = Input((None, ))\n",
    "embedding_sequence = Embedding(input_dim=max_index_value+1, output_dim=embedding_dim, mask_zero=False)(inputs)\n",
    "average_embedding = GlobalAveragePooling1D()(embedding_sequence)\n",
    "positive_probability = Dense(1, activation='sigmoid')(average_embedding)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=positive_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, None, 16)          160016    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile, train, and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.6896 - accuracy: 0.5606 - val_loss: 0.6815 - val_accuracy: 0.5922\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.6671 - accuracy: 0.6950 - val_loss: 0.6443 - val_accuracy: 0.7422\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.6212 - accuracy: 0.7617 - val_loss: 0.5995 - val_accuracy: 0.7328\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.5693 - accuracy: 0.7955 - val_loss: 0.5473 - val_accuracy: 0.7719\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.5208 - accuracy: 0.8200 - val_loss: 0.5023 - val_accuracy: 0.8047\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test), validation_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, marker='.', label='Training acc')\n",
    "plt.plot(epochs, val_acc, marker='.', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification acc')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow embedding projector\n",
    "\n",
    "The Tensorflow embedding projector can be found [here](https://projector.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the embedding layer's weights from the trained model\n",
    "weights = model.layers[1].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the word Embeddings to tsv files\n",
    "# Two files: \n",
    "#     one contains the embedding labels (meta.tsv),\n",
    "#     one contains the embeddings (vecs.tsv)\n",
    "\n",
    "k = 0\n",
    "\n",
    "for word, token in word_index.items():\n",
    "    if k != 0:\n",
    "        with open('meta.tsv', 'w', encoding='utf-8') as out_m:\n",
    "            out_m.write('\\n')\n",
    "        with open('vecs.tsv', 'w', encoding='utf-8') as out_v:\n",
    "            out_v.write('\\n')\n",
    "    \n",
    "    with open('vecs.tsv', 'w', encoding='utf-8') as out_v:\n",
    "        out_v.write('\\t'.join([str(x) for x in weights[token]]))\n",
    "    with open('meta.tsv', 'w', encoding='utf-8') as out_m:\n",
    "        out_m.write(word)\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins import projector\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(embedding=tf.Variable(model.layers[1].get_weights()[0][1:]))\n",
    "checkpoint.save('./embedding.ckpt')\n",
    "\n",
    "config = projector.ProjectorConfig()\n",
    "embedding_config = config.embeddings.add()\n",
    "\n",
    "embedding_config.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "embedding_config.metadata_path = 'meta.tsv'\n",
    "projector.visualize_embeddings('.', config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 31676."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir . --host 0.0.0.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.01242078, -0.03183878,  0.00668573, ..., -0.0365598 ,\n",
       "         -0.0086023 ,  0.02383696],\n",
       "        [-0.02388966,  0.05638453,  0.05363629, ..., -0.03258812,\n",
       "          0.00874781,  0.01883143],\n",
       "        [-0.04551405, -0.02130688,  0.0483591 , ..., -0.00594715,\n",
       "          0.01125759, -0.00276546],\n",
       "        ...,\n",
       "        [-0.32413834,  0.2561377 ,  0.3572946 , ..., -0.27774405,\n",
       "         -0.3043125 , -0.26406413],\n",
       "        [-0.13741532,  0.1337254 ,  0.16523331, ..., -0.1524726 ,\n",
       "         -0.14197849, -0.15352522],\n",
       "        [ 0.02836016, -0.01367629, -0.02100376, ...,  0.01533603,\n",
       "         -0.0300089 ,  0.04289247]], dtype=float32)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural network layers\n",
    "\n",
    "### Example - Fixed input length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 64, 32)            32000     \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 64)                6208      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 38,533\n",
      "Trainable params: 38,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import SimpleRNN\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(1000, 32, input_length=64),\n",
    "    SimpleRNN(64, activation='tanh'),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - Variable input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, None, 32)          32000     \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 64)                6208      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 38,533\n",
      "Trainable params: 38,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(1000, 32),\n",
    "    SimpleRNN(64, activation='tanh'),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, None, 32)          32000     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 57,157\n",
      "Trainable params: 57,157\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(1000, 32),\n",
    "    LSTM(64, activation='tanh'),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, None, 32)          32000     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 64)                18816     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 51,141\n",
      "Trainable params: 51,141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(1000, 32),\n",
    "    GRU(64, activation='tanh'),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize and pass an input to a SimpleRNN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SimpleRNN layer and test it\n",
    "simplernn_layer = SimpleRNN(units=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 16), dtype=float32, numpy=\n",
       "array([[ 1.       ,  1.       ,  1.       , -1.       ,  1.       ,\n",
       "        -1.       ,  1.       ,  1.       , -1.       , -1.       ,\n",
       "        -1.       , -1.       ,  1.       ,  1.       , -0.9999994,\n",
       "        -1.       ]], dtype=float32)>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = tf.constant([[[1., 1.], [2., 2.], [56., -100.]]])\n",
    "layer_output = simplernn_layer(sequence)\n",
    "layer_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and transform the IMDB review sentiment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = get_and_pad_imdb_dataset(maxlen=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_word_index = get_imdb_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a recurrent neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum index value\n",
    "max_index_value = max(imdb_word_index.values())\n",
    "embedding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, None, 16)          160016    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 16)                2112      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 162,145\n",
      "Trainable params: 162,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Using sequential, build the model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_index_value+1, output_dim=embedding_dim, mask_zero=True),\n",
    "    LSTM(units=16),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "536/536 [==============================] - 260s 485ms/step - loss: 0.4128 - accuracy: 0.8119 - val_loss: 0.2967 - val_accuracy: 0.8719\n",
      "Epoch 2/3\n",
      "536/536 [==============================] - 260s 485ms/step - loss: 0.2199 - accuracy: 0.9195 - val_loss: 0.2993 - val_accuracy: 0.8656\n",
      "Epoch 3/3\n",
      "487/536 [==========================>...] - ETA: 23s - loss: 0.1579 - accuracy: 0.9427"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=3, batch_size=32, \n",
    "                    validation_data=(X_test, y_test), validation_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, marker='.', label='Training acc')\n",
    "plt.plot(epochs, val_acc, marker='.', label='Validation acc')\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification acc')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_imdb_word_index = {value: key for key, value in imdb_word_index.items()}\n",
    "[inv_imdb_word_index[index] for index in X_test[0] if index > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model prediction \n",
    "model.predict(X_test[None, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the corresponding label\n",
    "y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked RNNs and the Bidirectional wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(None, 10))\n",
    "h = Masking(mask_value=0)(inputs)\n",
    "h = LSTM(units=64)(h)\n",
    "outputs = Dense(5, activation='softmax')(h)\n",
    "\n",
    "model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with Stacked RNN model\n",
    "\n",
    "If the `return_sequences` argument sets to True, then the layer returns an output for each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(None, 10))\n",
    "h = Masking(mask_value=0)(inputs)\n",
    "h = LSTM(units=32, return_sequences=True)(h)\n",
    "h = LSTM(units=64)(h)\n",
    "outputs = Dense(5, activation='softmax')(h)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with Bidirectional layer\n",
    "\n",
    "We use bidirection model to take account future context as well as past contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "inputs = Input(shape=(None, 10))\n",
    "h = Masking(mask_value=0)(inputs)\n",
    "h = Bidirectional(LSTM(units=32, return_sequences=True))(h)\n",
    "h = LSTM(units=64)(h)\n",
    "outputs = Dense(5, activation='softmax')(h)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can use it with normal LSTM layer, not set `return_sequences`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(None, 10))\n",
    "h = Masking(mask_value=0)(inputs)\n",
    "h = Bidirectional(LSTM(units=32, return_sequences=True))(h)\n",
    "h = Bidirectional(LSTM(units=64))(h)\n",
    "outputs = Dense(5, activation='softmax')(h)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also set the `merge_mode` parameter in Bidirectional layer. (default operation is `'concat'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(None, 10))\n",
    "h = Masking(mask_value=0)(inputs)\n",
    "h = Bidirectional(LSTM(units=32, return_sequences=True), merge_mode='sum')(h)\n",
    "h = Bidirectional(LSTM(units=64))(h)\n",
    "outputs = Dense(5, activation='softmax')(h)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and transform the IMDB review sentiment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = get_and_pad_imdb_dataset()\n",
    "imdb_word_index = get_imdb_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build stacked and bidirectional recurrent models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum index value and specify an embedding dimension\n",
    "max_index_value = max(imdb_word_index.values())\n",
    "embedding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Sequential model\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_index_value+1, output_dim=embedding_dim, mask_zero=True),\n",
    "    LSTM(units=32, return_sequences=True),\n",
    "    LSTM(units=32, return_sequences=False),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Sequential API, build a bidirectional RNN with merge_mode='sum'\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_index_value+1, output_dim=embedding_dim, mask_zero=True),\n",
    "    Bidirectional(layer=LSTM(units=8), merge_mode='sum',\n",
    "                  backward_layer=GRU(units=8, go_backwards=True)),\n",
    "    Dense(units=1, activation='sigmoid')   \n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model featuring both stacked recurrent layers and a bidirectional layer\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_index_value+1, output_dim=embedding_dim),\n",
    "    Bidirectional(layer=LSTM(units=8, return_sequences=True), merge_mode='concat'),\n",
    "    GRU(units=8, return_sequences=False),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=3, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
