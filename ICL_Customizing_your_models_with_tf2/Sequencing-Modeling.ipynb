{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequencing Modeling\n",
    "\n",
    "> In this post, it will cover various network architectures and layers that we can use to make predictions from sequence data. This is the summary of lecture \"Customizing your model with Tensorflow 2\" from Coursera.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Chanseok Kang\n",
    "- categories: [Python, Coursera, Deep_Learning, Tensorflow]\n",
    "- image: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (16, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing sequence data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  4, 12, 33, 18],\n",
       "       [63, 23, 54, 30, 19,  3],\n",
       "       [ 0, 43, 37, 11, 33, 15]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "test_data = [\n",
    "    [4, 12, 33, 18],\n",
    "    [63, 23, 54, 30, 19, 3],\n",
    "    [43, 37, 11, 33, 15]\n",
    "]\n",
    "\n",
    "preprocessed_data = pad_sequences(test_data, padding='pre')\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### post padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4, 12, 33, 18,  0,  0],\n",
       "       [63, 23, 54, 30, 19,  3],\n",
       "       [43, 37, 11, 33, 15,  0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data = pad_sequences(test_data, padding='post')\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post padding with maximum length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4, 12, 33, 18,  0],\n",
       "       [23, 54, 30, 19,  3],\n",
       "       [43, 37, 11, 33, 15]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data = pad_sequences(test_data, padding='post', maxlen=5)\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post padding with truncating (Default: 'pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4, 12, 33, 18,  0],\n",
       "       [63, 23, 54, 30, 19],\n",
       "       [43, 37, 11, 33, 15]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data = pad_sequences(test_data, padding='post', maxlen=5, truncating='post')\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post padding with truncating, then filled with value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4, 12, 33, 18, -1],\n",
       "       [63, 23, 54, 30, 19],\n",
       "       [43, 37, 11, 33, 15]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data = pad_sequences(test_data, padding='post', maxlen=5, truncating='post', value=-1)\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example in 2d array sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2, 1],\n",
       "        [3, 3],\n",
       "        [0, 0]],\n",
       "\n",
       "       [[4, 3],\n",
       "        [2, 4],\n",
       "        [1, 1]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = [\n",
    "    [[2, 1], [3, 3]],\n",
    "    [[4, 3], [2, 4], [1, 1]]\n",
    "]\n",
    "\n",
    "preprocessed_data = pad_sequences(test_input, padding='post')\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masking Layer for mask specific sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 6, 1), dtype=int32, numpy=\n",
       "array([[[ 4],\n",
       "        [12],\n",
       "        [33],\n",
       "        [18],\n",
       "        [ 0],\n",
       "        [ 0]],\n",
       "\n",
       "       [[63],\n",
       "        [23],\n",
       "        [54],\n",
       "        [30],\n",
       "        [19],\n",
       "        [ 3]],\n",
       "\n",
       "       [[43],\n",
       "        [37],\n",
       "        [11],\n",
       "        [33],\n",
       "        [15],\n",
       "        [ 0]]])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Masking\n",
    "\n",
    "preprocessed_data = pad_sequences(test_data, padding='post')\n",
    "\n",
    "masking_layer = Masking(mask_value=0)\n",
    "preprocessed_data = preprocessed_data[..., tf.newaxis] # (batch_size, seq_length, features)\n",
    "masked_input = masking_layer(preprocessed_data)\n",
    "masked_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 6), dtype=bool, numpy=\n",
       "array([[ True,  True,  True,  True, False, False],\n",
       "       [ True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True, False]])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_input._keras_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the IMDB review sentiment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 22665,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 21631,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 31050,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset with different options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([list([1, 14, 22, 16, 43, 2, 2, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 2, 112, 50, 2, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 2, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 2, 38, 76, 15, 13, 2, 4, 22, 17, 2, 17, 12, 16, 2, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 2, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 2, 36, 71, 43, 2, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]),\n",
       "         list([1, 194, 2, 194, 2, 78, 228, 5, 6, 2, 2, 2, 134, 26, 4, 2, 8, 118, 2, 14, 394, 20, 13, 119, 2, 189, 102, 5, 207, 110, 2, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2, 2, 5, 2, 4, 116, 9, 35, 2, 4, 229, 9, 340, 2, 4, 118, 9, 4, 130, 2, 19, 4, 2, 5, 89, 29, 2, 46, 37, 4, 455, 9, 45, 43, 38, 2, 2, 398, 4, 2, 26, 2, 5, 163, 11, 2, 2, 4, 2, 9, 194, 2, 7, 2, 2, 349, 2, 148, 2, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 2, 98, 5, 4, 228, 9, 43, 2, 2, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 2, 228, 2, 5, 2, 2, 245, 2, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 2, 14, 9, 6, 371, 78, 22, 2, 64, 2, 9, 8, 168, 145, 23, 4, 2, 15, 16, 4, 2, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "         list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2, 311, 12, 16, 2, 33, 75, 43, 2, 296, 4, 86, 320, 35, 2, 19, 263, 2, 2, 4, 2, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 2, 43, 2, 2, 8, 257, 85, 2, 42, 2, 2, 83, 68, 2, 15, 36, 165, 2, 278, 36, 69, 2, 2, 8, 106, 14, 2, 2, 18, 6, 22, 12, 215, 28, 2, 40, 6, 87, 326, 23, 2, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2, 51, 9, 170, 23, 2, 116, 2, 2, 13, 191, 79, 2, 89, 2, 14, 9, 8, 106, 2, 2, 35, 2, 6, 227, 7, 129, 113]),\n",
       "         ...,\n",
       "         list([1, 11, 6, 230, 245, 2, 9, 6, 2, 446, 2, 45, 2, 84, 2, 2, 21, 4, 2, 84, 2, 325, 2, 134, 2, 2, 84, 5, 36, 28, 57, 2, 21, 8, 140, 8, 2, 5, 2, 84, 56, 18, 2, 14, 9, 31, 7, 4, 2, 2, 2, 2, 2, 18, 6, 20, 207, 110, 2, 12, 8, 2, 2, 8, 97, 6, 20, 53, 2, 74, 4, 460, 364, 2, 29, 270, 11, 2, 108, 45, 40, 29, 2, 395, 11, 6, 2, 2, 7, 2, 89, 364, 70, 29, 140, 4, 64, 2, 11, 4, 2, 26, 178, 4, 2, 443, 2, 5, 27, 2, 117, 2, 2, 165, 47, 84, 37, 131, 2, 14, 2, 10, 10, 61, 2, 2, 10, 10, 288, 2, 2, 34, 2, 2, 4, 65, 496, 4, 231, 7, 2, 5, 6, 320, 234, 2, 234, 2, 2, 7, 496, 4, 139, 2, 2, 2, 2, 5, 2, 18, 4, 2, 2, 250, 11, 2, 2, 4, 2, 2, 2, 2, 372, 2, 2, 2, 2, 7, 4, 59, 2, 4, 2, 2]),\n",
       "         list([1, 2, 2, 69, 72, 2, 13, 2, 2, 8, 12, 2, 23, 5, 16, 484, 2, 54, 349, 11, 2, 2, 45, 58, 2, 13, 197, 12, 16, 43, 23, 2, 5, 62, 30, 145, 402, 11, 2, 51, 2, 32, 61, 369, 71, 66, 2, 12, 2, 75, 100, 2, 8, 4, 105, 37, 69, 147, 2, 75, 2, 44, 257, 390, 5, 69, 263, 2, 105, 50, 286, 2, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 2, 13, 2, 40, 319, 2, 112, 2, 11, 2, 121, 25, 70, 2, 4, 2, 2, 13, 18, 31, 62, 40, 8, 2, 4, 2, 7, 14, 123, 5, 2, 25, 8, 2, 12, 145, 5, 202, 12, 160, 2, 202, 12, 6, 52, 58, 2, 92, 401, 2, 12, 39, 14, 251, 8, 15, 251, 5, 2, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
       "         list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 2, 270, 2, 5, 2, 2, 2, 2, 101, 405, 39, 14, 2, 4, 2, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 2, 102, 7, 4, 2, 2, 9, 24, 6, 78, 2, 17, 2, 2, 21, 27, 2, 2, 5, 2, 2, 92, 2, 4, 2, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 2, 2, 9, 6, 66, 78, 2, 4, 2, 2, 5, 2, 272, 191, 2, 6, 2, 8, 2, 2, 2, 2, 5, 383, 2, 2, 2, 2, 497, 2, 8, 2, 2, 2, 21, 60, 27, 239, 9, 43, 2, 209, 405, 10, 10, 12, 2, 40, 4, 248, 20, 12, 16, 5, 174, 2, 72, 7, 51, 6, 2, 22, 4, 204, 131, 9])],\n",
       "        dtype=object),\n",
       "  array([1, 0, 0, ..., 0, 1, 0], dtype=int64)),\n",
       " (array([list([1, 2, 202, 14, 31, 6, 2, 10, 10, 2, 2, 5, 4, 360, 7, 4, 177, 2, 394, 354, 4, 123, 9, 2, 2, 2, 10, 10, 13, 92, 124, 89, 488, 2, 100, 28, 2, 14, 31, 23, 27, 2, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 2, 38, 32, 25, 2, 451, 202, 14, 6, 2]),\n",
       "         list([1, 14, 22, 2, 6, 176, 7, 2, 88, 12, 2, 23, 2, 5, 109, 2, 4, 114, 9, 55, 2, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 2, 2, 4, 2, 2, 109, 2, 21, 4, 22, 2, 8, 6, 2, 2, 10, 10, 4, 105, 2, 35, 2, 2, 19, 2, 2, 5, 2, 2, 45, 55, 221, 15, 2, 2, 2, 14, 2, 4, 405, 5, 2, 7, 27, 85, 108, 131, 4, 2, 2, 2, 405, 9, 2, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 2, 239, 34, 2, 2, 45, 407, 31, 7, 41, 2, 105, 21, 59, 299, 12, 38, 2, 5, 2, 15, 45, 2, 488, 2, 127, 6, 52, 292, 17, 4, 2, 185, 132, 2, 2, 2, 488, 2, 47, 6, 392, 173, 4, 2, 2, 270, 2, 4, 2, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 2, 2, 7, 2, 2, 2, 5, 2, 30, 2, 2, 56, 4, 2, 5, 2, 2, 8, 4, 2, 398, 229, 10, 10, 13, 2, 2, 2, 14, 9, 31, 7, 27, 111, 108, 15, 2, 19, 2, 2, 2, 2, 14, 22, 9, 2, 21, 45, 2, 5, 45, 252, 8, 2, 6, 2, 2, 2, 39, 4, 2, 48, 25, 181, 8, 67, 35, 2, 22, 49, 238, 60, 135, 2, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 2, 8, 169, 11, 374, 2, 25, 203, 28, 8, 2, 12, 125, 4, 2]),\n",
       "         list([1, 111, 2, 2, 2, 2, 2, 4, 87, 2, 2, 7, 31, 318, 2, 7, 4, 498, 2, 2, 63, 29, 2, 220, 2, 2, 5, 17, 12, 2, 220, 2, 17, 6, 185, 132, 2, 16, 53, 2, 11, 2, 74, 4, 438, 21, 27, 2, 2, 8, 22, 107, 2, 2, 2, 2, 8, 35, 2, 2, 11, 22, 231, 54, 29, 2, 29, 100, 2, 2, 34, 2, 2, 2, 5, 2, 98, 31, 2, 33, 6, 58, 14, 2, 2, 8, 4, 365, 7, 2, 2, 356, 346, 4, 2, 2, 63, 29, 93, 11, 2, 11, 2, 33, 6, 58, 54, 2, 431, 2, 7, 32, 2, 16, 11, 94, 2, 10, 10, 4, 2, 2, 7, 4, 2, 2, 2, 2, 8, 2, 8, 2, 121, 31, 7, 27, 86, 2, 2, 16, 6, 465, 2, 2, 2, 2, 17, 2, 42, 4, 2, 37, 473, 6, 2, 6, 2, 7, 328, 212, 70, 30, 258, 11, 220, 32, 7, 108, 21, 133, 12, 9, 55, 465, 2, 2, 53, 33, 2, 2, 37, 70, 2, 4, 2, 2, 74, 476, 37, 62, 91, 2, 169, 4, 2, 2, 146, 2, 2, 5, 258, 12, 184, 2, 2, 5, 2, 2, 7, 4, 22, 2, 18, 2, 2, 2, 7, 4, 2, 71, 348, 425, 2, 2, 19, 2, 5, 2, 11, 2, 8, 339, 2, 4, 2, 2, 7, 4, 2, 10, 10, 263, 2, 9, 270, 11, 6, 2, 4, 2, 2, 121, 4, 2, 26, 2, 19, 68, 2, 5, 28, 446, 6, 318, 2, 8, 67, 51, 36, 70, 81, 8, 2, 2, 36, 2, 8, 2, 2, 18, 6, 2, 4, 2, 26, 2, 2, 11, 14, 2, 2, 12, 426, 28, 77, 2, 8, 97, 38, 111, 2, 2, 168, 2, 2, 137, 2, 18, 27, 173, 9, 2, 17, 6, 2, 428, 2, 232, 11, 4, 2, 37, 272, 40, 2, 247, 30, 2, 6, 2, 54, 2, 2, 98, 6, 2, 40, 2, 37, 2, 98, 4, 2, 2, 15, 14, 9, 57, 2, 5, 2, 6, 275, 2, 2, 2, 2, 98, 6, 2, 10, 10, 2, 19, 14, 2, 267, 162, 2, 37, 2, 2, 98, 4, 2, 2, 90, 19, 6, 2, 7, 2, 2, 2, 4, 2, 2, 2, 8, 2, 90, 4, 2, 8, 4, 2, 17, 2, 2, 2, 4, 2, 8, 2, 189, 4, 2, 2, 2, 4, 2, 5, 95, 271, 23, 6, 2, 2, 2, 2, 33, 2, 6, 425, 2, 2, 2, 2, 7, 4, 2, 2, 469, 4, 2, 54, 4, 150, 2, 2, 280, 53, 2, 2, 18, 339, 29, 2, 27, 2, 5, 2, 68, 2, 19, 2, 2, 4, 2, 7, 263, 65, 2, 34, 6, 2, 2, 43, 159, 29, 9, 2, 9, 387, 73, 195, 2, 10, 10, 2, 4, 58, 2, 54, 14, 2, 117, 22, 16, 93, 5, 2, 4, 192, 15, 12, 16, 93, 34, 6, 2, 2, 33, 4, 2, 7, 15, 2, 2, 2, 325, 12, 62, 30, 2, 8, 67, 14, 17, 6, 2, 44, 148, 2, 2, 203, 42, 203, 24, 28, 69, 2, 2, 11, 330, 54, 29, 93, 2, 21, 2, 2, 27, 2, 7, 2, 4, 22, 2, 17, 6, 2, 2, 7, 2, 2, 2, 100, 30, 4, 2, 2, 2, 2, 42, 2, 11, 4, 2, 42, 101, 2, 7, 101, 2, 15, 2, 94, 2, 180, 5, 9, 2, 34, 2, 45, 6, 2, 22, 60, 6, 2, 31, 11, 94, 2, 96, 21, 94, 2, 9, 57, 2]),\n",
       "         ...,\n",
       "         list([1, 13, 2, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 2, 21, 45, 184, 78, 4, 2, 2, 2, 2, 2, 395, 2, 5, 2, 11, 119, 2, 89, 2, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2, 284, 2, 2, 37, 315, 4, 226, 20, 272, 2, 40, 29, 152, 60, 181, 8, 30, 50, 2, 362, 80, 119, 12, 21, 2, 2]),\n",
       "         list([1, 11, 119, 241, 9, 4, 2, 20, 12, 468, 15, 94, 2, 2, 2, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2, 7, 2, 46, 2, 9, 2, 5, 4, 2, 47, 8, 79, 90, 145, 164, 162, 50, 6, 2, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 2, 200, 5, 2, 5, 9, 2, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 2, 92, 124, 51, 45, 2, 71, 2, 13, 2, 14, 20, 6, 2, 7, 470]),\n",
       "         list([1, 6, 52, 2, 430, 22, 9, 220, 2, 8, 28, 2, 2, 2, 6, 2, 15, 47, 6, 2, 2, 8, 114, 5, 33, 222, 31, 55, 184, 2, 2, 2, 19, 346, 2, 5, 6, 364, 350, 4, 184, 2, 9, 133, 2, 11, 2, 2, 21, 4, 2, 2, 2, 50, 2, 2, 9, 6, 2, 17, 6, 2, 2, 21, 17, 6, 2, 232, 2, 2, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 2, 19, 4, 78, 173, 7, 27, 2, 2, 2, 2, 2, 9, 6, 2, 17, 210, 5, 2, 2, 47, 77, 395, 14, 172, 173, 18, 2, 2, 2, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 2, 53, 40, 35, 390, 7, 11, 4, 2, 7, 4, 314, 74, 6, 2, 22, 2, 19, 2, 2, 2, 382, 4, 91, 2, 439, 19, 14, 20, 9, 2, 2, 2, 4, 2, 25, 124, 4, 31, 12, 16, 93, 2, 34, 2, 2])],\n",
       "        dtype=object),\n",
       "  array([0, 1, 1, ..., 0, 0, 0], dtype=int64)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limit the vocabulary to the top 500 words using num_words\n",
    "imdb.load_data(num_words=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([list([2, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 2, 173, 36, 256, 2, 25, 100, 43, 838, 112, 50, 670, 2, 2, 35, 480, 284, 2, 150, 2, 172, 112, 167, 2, 336, 385, 39, 2, 172, 2, 2, 17, 546, 38, 13, 447, 2, 192, 50, 16, 2, 147, 2, 19, 14, 22, 2, 2, 2, 469, 2, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 2, 22, 17, 515, 17, 12, 16, 626, 18, 2, 2, 62, 386, 12, 2, 316, 2, 106, 2, 2, 2, 2, 16, 480, 66, 2, 33, 2, 130, 12, 16, 38, 619, 2, 25, 124, 51, 36, 135, 48, 25, 2, 33, 2, 22, 12, 215, 28, 77, 52, 2, 14, 407, 16, 82, 2, 2, 2, 107, 117, 2, 15, 256, 2, 2, 2, 2, 2, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 2, 2, 2, 2, 13, 104, 88, 2, 381, 15, 297, 98, 32, 2, 56, 26, 141, 2, 194, 2, 18, 2, 226, 22, 21, 134, 476, 26, 480, 2, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 2, 226, 65, 16, 38, 2, 88, 12, 16, 283, 2, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]),\n",
       "         list([2, 194, 2, 194, 2, 78, 228, 2, 2, 2, 2, 2, 134, 26, 2, 715, 2, 118, 2, 14, 394, 20, 13, 119, 954, 189, 102, 2, 207, 110, 2, 21, 14, 69, 188, 2, 30, 23, 2, 2, 249, 126, 93, 2, 114, 2, 2, 2, 2, 647, 2, 116, 2, 35, 2, 2, 229, 2, 340, 2, 2, 118, 2, 2, 130, 2, 19, 2, 2, 2, 89, 29, 952, 46, 37, 2, 455, 2, 45, 43, 38, 2, 2, 398, 2, 2, 26, 2, 2, 163, 11, 2, 2, 2, 2, 2, 194, 775, 2, 2, 2, 349, 2, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 2, 98, 2, 2, 228, 2, 43, 2, 2, 15, 299, 120, 2, 120, 174, 11, 220, 175, 136, 50, 2, 2, 228, 2, 2, 2, 656, 245, 2, 2, 2, 2, 131, 152, 491, 18, 2, 32, 2, 2, 14, 2, 2, 371, 78, 22, 625, 64, 2, 2, 2, 168, 145, 23, 2, 2, 15, 16, 2, 2, 2, 28, 2, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "         list([2, 14, 47, 2, 30, 31, 2, 2, 249, 108, 2, 2, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 2, 2, 311, 12, 16, 2, 33, 75, 43, 2, 296, 2, 86, 320, 35, 534, 19, 263, 2, 2, 2, 2, 33, 89, 78, 12, 66, 16, 2, 360, 2, 2, 58, 316, 334, 11, 2, 2, 43, 645, 662, 2, 257, 85, 2, 42, 2, 2, 83, 68, 2, 15, 36, 165, 2, 278, 36, 69, 2, 780, 2, 106, 14, 2, 2, 18, 2, 22, 12, 215, 28, 610, 40, 2, 87, 326, 23, 2, 21, 23, 22, 12, 272, 40, 57, 31, 11, 2, 22, 47, 2, 2, 51, 2, 170, 23, 595, 116, 595, 2, 13, 191, 79, 638, 89, 2, 14, 2, 2, 106, 607, 624, 35, 534, 2, 227, 2, 129, 113]),\n",
       "         ...,\n",
       "         list([2, 11, 2, 230, 245, 2, 2, 2, 2, 446, 2, 45, 2, 84, 2, 2, 21, 2, 912, 84, 2, 325, 725, 134, 2, 2, 84, 2, 36, 28, 57, 2, 21, 2, 140, 2, 703, 2, 2, 84, 56, 18, 2, 14, 2, 31, 2, 2, 2, 2, 2, 2, 2, 18, 2, 20, 207, 110, 563, 12, 2, 2, 2, 2, 97, 2, 20, 53, 2, 74, 2, 460, 364, 2, 29, 270, 11, 960, 108, 45, 40, 29, 2, 395, 11, 2, 2, 500, 2, 2, 89, 364, 70, 29, 140, 2, 64, 2, 11, 2, 2, 26, 178, 2, 529, 443, 2, 2, 27, 710, 117, 2, 2, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 2, 2, 10, 10, 288, 2, 2, 34, 2, 2, 2, 65, 496, 2, 231, 2, 790, 2, 2, 320, 234, 2, 234, 2, 2, 2, 496, 2, 139, 929, 2, 2, 2, 2, 2, 18, 2, 2, 2, 250, 11, 2, 2, 2, 2, 2, 747, 2, 372, 2, 2, 541, 2, 2, 2, 59, 2, 2, 2, 2]),\n",
       "         list([2, 2, 2, 69, 72, 2, 13, 610, 930, 2, 12, 582, 23, 2, 16, 484, 685, 54, 349, 11, 2, 2, 45, 58, 2, 13, 197, 12, 16, 43, 23, 2, 2, 62, 30, 145, 402, 11, 2, 51, 575, 32, 61, 369, 71, 66, 770, 12, 2, 75, 100, 2, 2, 2, 105, 37, 69, 147, 712, 75, 2, 44, 257, 390, 2, 69, 263, 514, 105, 50, 286, 2, 23, 2, 123, 13, 161, 40, 2, 421, 2, 116, 16, 897, 13, 2, 40, 319, 2, 112, 2, 11, 2, 121, 25, 70, 2, 2, 719, 2, 13, 18, 31, 62, 40, 2, 2, 2, 2, 2, 14, 123, 2, 942, 25, 2, 721, 12, 145, 2, 202, 12, 160, 580, 202, 12, 2, 52, 58, 2, 92, 401, 728, 12, 39, 14, 251, 2, 15, 251, 2, 2, 12, 38, 84, 80, 124, 12, 2, 23]),\n",
       "         list([2, 17, 2, 194, 337, 2, 2, 204, 22, 45, 254, 2, 106, 14, 123, 2, 2, 270, 2, 2, 2, 2, 732, 2, 101, 405, 39, 14, 2, 2, 2, 2, 115, 50, 305, 12, 47, 2, 168, 2, 235, 2, 38, 111, 699, 102, 2, 2, 2, 2, 2, 24, 2, 78, 2, 17, 2, 2, 21, 27, 2, 2, 2, 2, 2, 92, 2, 2, 2, 2, 2, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 2, 97, 12, 157, 21, 2, 2, 2, 2, 66, 78, 2, 2, 631, 2, 2, 2, 272, 191, 2, 2, 2, 2, 2, 2, 2, 544, 2, 383, 2, 848, 2, 2, 497, 2, 2, 2, 2, 2, 21, 60, 27, 239, 2, 43, 2, 209, 405, 10, 10, 12, 764, 40, 2, 248, 20, 12, 16, 2, 174, 2, 72, 2, 51, 2, 2, 22, 2, 204, 131, 2])],\n",
       "        dtype=object),\n",
       "  array([1, 0, 0, ..., 0, 1, 0], dtype=int64)),\n",
       " (array([list([2, 591, 202, 14, 31, 2, 717, 10, 10, 2, 2, 2, 2, 360, 2, 2, 177, 2, 394, 354, 2, 123, 2, 2, 2, 2, 10, 10, 13, 92, 124, 89, 488, 2, 100, 28, 2, 14, 31, 23, 27, 2, 29, 220, 468, 2, 124, 14, 286, 170, 2, 157, 46, 2, 27, 239, 16, 179, 2, 38, 32, 25, 2, 451, 202, 14, 2, 717]),\n",
       "         list([2, 14, 22, 2, 2, 176, 2, 2, 88, 12, 2, 23, 2, 2, 109, 943, 2, 114, 2, 55, 606, 2, 111, 2, 2, 139, 193, 273, 23, 2, 172, 270, 11, 2, 2, 2, 2, 2, 109, 2, 21, 2, 22, 2, 2, 2, 2, 2, 10, 10, 2, 105, 987, 35, 841, 2, 19, 861, 2, 2, 2, 2, 45, 55, 221, 15, 670, 2, 526, 14, 2, 2, 405, 2, 2, 2, 27, 85, 108, 131, 2, 2, 2, 2, 405, 2, 2, 133, 2, 50, 13, 104, 51, 66, 166, 14, 22, 157, 2, 2, 530, 239, 34, 2, 2, 45, 407, 31, 2, 41, 2, 105, 21, 59, 299, 12, 38, 950, 2, 2, 15, 45, 629, 488, 2, 127, 2, 52, 292, 17, 2, 2, 185, 132, 2, 2, 2, 488, 2, 47, 2, 392, 173, 2, 2, 2, 270, 2, 2, 2, 2, 2, 65, 55, 73, 11, 346, 14, 20, 2, 2, 976, 2, 2, 2, 861, 2, 2, 2, 30, 2, 2, 56, 2, 841, 2, 990, 692, 2, 2, 2, 398, 229, 10, 10, 13, 2, 670, 2, 14, 2, 31, 2, 27, 111, 108, 15, 2, 19, 2, 2, 875, 551, 14, 22, 2, 2, 21, 45, 2, 2, 45, 252, 2, 2, 2, 565, 921, 2, 39, 2, 529, 48, 25, 181, 2, 67, 35, 2, 22, 49, 238, 60, 135, 2, 14, 2, 290, 2, 58, 10, 10, 472, 45, 55, 878, 2, 169, 11, 374, 2, 25, 203, 28, 2, 818, 12, 125, 2, 2]),\n",
       "         list([2, 111, 748, 2, 2, 2, 2, 2, 87, 2, 2, 2, 31, 318, 2, 2, 2, 498, 2, 748, 63, 29, 2, 220, 686, 2, 2, 17, 12, 575, 220, 2, 17, 2, 185, 132, 2, 16, 53, 928, 11, 2, 74, 2, 438, 21, 27, 2, 589, 2, 22, 107, 2, 2, 997, 2, 2, 35, 2, 2, 11, 22, 231, 54, 29, 2, 29, 100, 2, 2, 34, 2, 2, 2, 2, 2, 98, 31, 2, 33, 2, 58, 14, 2, 2, 2, 2, 365, 2, 2, 2, 356, 346, 2, 2, 2, 63, 29, 93, 11, 2, 11, 2, 33, 2, 58, 54, 2, 431, 748, 2, 32, 2, 16, 11, 94, 2, 10, 10, 2, 993, 2, 2, 2, 2, 2, 2, 2, 2, 847, 2, 2, 121, 31, 2, 27, 86, 2, 2, 16, 2, 465, 993, 2, 2, 573, 17, 2, 42, 2, 2, 37, 473, 2, 711, 2, 2, 2, 328, 212, 70, 30, 258, 11, 220, 32, 2, 108, 21, 133, 12, 2, 55, 465, 849, 2, 53, 33, 2, 2, 37, 70, 2, 2, 2, 2, 74, 476, 37, 62, 91, 2, 169, 2, 2, 2, 146, 655, 2, 2, 258, 12, 184, 2, 546, 2, 849, 2, 2, 2, 22, 2, 18, 631, 2, 797, 2, 2, 2, 71, 348, 425, 2, 2, 19, 2, 2, 2, 11, 661, 2, 339, 2, 2, 2, 2, 2, 2, 2, 10, 10, 263, 787, 2, 270, 11, 2, 2, 2, 2, 2, 121, 2, 2, 26, 2, 19, 68, 2, 2, 28, 446, 2, 318, 2, 2, 67, 51, 36, 70, 81, 2, 2, 2, 36, 2, 2, 2, 2, 18, 2, 711, 2, 2, 26, 2, 2, 11, 14, 636, 720, 12, 426, 28, 77, 776, 2, 97, 38, 111, 2, 2, 168, 2, 2, 137, 2, 18, 27, 173, 2, 2, 17, 2, 2, 428, 2, 232, 11, 2, 2, 37, 272, 40, 2, 247, 30, 656, 2, 2, 54, 2, 2, 98, 2, 2, 40, 558, 37, 2, 98, 2, 2, 2, 15, 14, 2, 57, 2, 2, 2, 2, 275, 711, 2, 2, 2, 98, 2, 2, 10, 10, 2, 19, 14, 2, 267, 162, 711, 37, 2, 752, 98, 2, 2, 2, 90, 19, 2, 2, 2, 2, 2, 2, 2, 2, 2, 930, 2, 508, 90, 2, 2, 2, 2, 2, 17, 2, 2, 2, 2, 2, 2, 2, 189, 2, 2, 2, 2, 2, 2, 2, 95, 271, 23, 2, 2, 2, 2, 2, 33, 2, 2, 425, 2, 2, 2, 2, 2, 2, 2, 2, 469, 2, 2, 54, 2, 150, 2, 2, 280, 53, 2, 2, 18, 339, 29, 2, 27, 2, 2, 2, 68, 2, 19, 2, 2, 2, 2, 2, 263, 65, 2, 34, 2, 2, 2, 43, 159, 29, 2, 2, 2, 387, 73, 195, 584, 10, 10, 2, 2, 58, 810, 54, 14, 2, 117, 22, 16, 93, 2, 2, 2, 192, 15, 12, 16, 93, 34, 2, 2, 2, 33, 2, 2, 2, 15, 2, 2, 2, 325, 12, 62, 30, 776, 2, 67, 14, 17, 2, 2, 44, 148, 687, 2, 203, 42, 203, 24, 28, 69, 2, 2, 11, 330, 54, 29, 93, 2, 21, 845, 2, 27, 2, 2, 819, 2, 22, 2, 17, 2, 2, 787, 2, 2, 2, 2, 100, 30, 2, 2, 2, 2, 2, 42, 2, 11, 2, 2, 42, 101, 704, 2, 101, 999, 15, 2, 94, 2, 180, 2, 2, 2, 34, 2, 45, 2, 2, 22, 60, 2, 2, 31, 11, 94, 2, 96, 21, 94, 749, 2, 57, 975]),\n",
       "         ...,\n",
       "         list([2, 13, 2, 15, 2, 135, 14, 2, 35, 32, 46, 394, 20, 62, 30, 2, 21, 45, 184, 78, 2, 2, 910, 769, 2, 2, 395, 2, 2, 2, 11, 119, 2, 89, 2, 2, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 2, 185, 2, 284, 2, 2, 37, 315, 2, 226, 20, 272, 2, 40, 29, 152, 60, 181, 2, 30, 50, 553, 362, 80, 119, 12, 21, 846, 2]),\n",
       "         list([2, 11, 119, 241, 2, 2, 840, 20, 12, 468, 15, 94, 2, 562, 791, 39, 2, 86, 107, 2, 97, 14, 31, 33, 2, 2, 2, 743, 46, 2, 2, 2, 2, 2, 768, 47, 2, 79, 90, 145, 164, 162, 50, 2, 501, 119, 2, 2, 2, 78, 232, 15, 16, 224, 11, 2, 333, 20, 2, 985, 200, 2, 2, 2, 2, 2, 2, 79, 357, 2, 20, 47, 220, 57, 206, 139, 11, 12, 2, 55, 117, 212, 13, 2, 92, 124, 51, 45, 2, 71, 536, 13, 520, 14, 20, 2, 2, 2, 470]),\n",
       "         list([2, 2, 52, 2, 430, 22, 2, 220, 2, 2, 28, 2, 519, 2, 2, 769, 15, 47, 2, 2, 2, 2, 114, 2, 33, 222, 31, 55, 184, 704, 2, 2, 19, 346, 2, 2, 2, 364, 350, 2, 184, 2, 2, 133, 2, 11, 2, 2, 21, 2, 2, 2, 570, 50, 2, 2, 2, 2, 2, 17, 2, 2, 2, 21, 17, 2, 2, 232, 2, 2, 29, 266, 56, 96, 346, 194, 308, 2, 194, 21, 29, 218, 2, 19, 2, 78, 173, 2, 27, 2, 2, 2, 718, 2, 2, 2, 2, 17, 210, 2, 2, 2, 47, 77, 395, 14, 172, 173, 18, 2, 2, 2, 82, 127, 27, 173, 11, 2, 392, 217, 21, 50, 2, 57, 65, 12, 2, 53, 40, 35, 390, 2, 11, 2, 2, 2, 2, 314, 74, 2, 792, 22, 2, 19, 714, 727, 2, 382, 2, 91, 2, 439, 19, 14, 20, 2, 2, 2, 2, 2, 756, 25, 124, 2, 31, 12, 16, 93, 804, 34, 2, 2])],\n",
       "        dtype=object),\n",
       "  array([0, 1, 1, ..., 0, 0, 0], dtype=int64)))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ignore the top 10 most frequent words\n",
    "imdb.load_data(skip_top=10, num_words=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
       "         list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "         list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
       "         ...,\n",
       "         list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 86527, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 14532, 325, 725, 134, 15271, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 11656, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 26094, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 17793, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 14492, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 17793, 5, 27, 710, 117, 74936, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 17793, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 17793, 7750, 5, 4241, 18, 4, 8497, 13164, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 11027, 4, 3586, 22459]),\n",
       "         list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 21469, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 40691, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 29455, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 11418, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 21213, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
       "         list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 12815, 270, 14437, 5, 16923, 12255, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 16553, 21, 27, 9685, 6139, 5, 29043, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 85010, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 70907, 10755, 544, 5, 383, 1271, 848, 1468, 12183, 497, 16876, 8, 1597, 8778, 19280, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
       "        dtype=object),\n",
       "  array([1, 0, 0, ..., 0, 1, 0], dtype=int64)),\n",
       " (array([list([1, 591, 202, 14, 31, 6, 717, 10, 10, 18142, 10698, 5, 4, 360, 7, 4, 177, 5760, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 7944, 100, 28, 1668, 14, 31, 23, 27, 7479, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 15387, 38, 32, 25, 7944, 451, 202, 14, 6, 717]),\n",
       "         list([1, 14, 22, 3443, 6, 176, 7, 5063, 88, 12, 2679, 23, 1310, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 7216, 10626, 4, 8463, 2801, 109, 1603, 21, 4, 22, 3861, 8, 6, 1193, 1330, 10, 10, 4, 105, 987, 35, 841, 16873, 19, 861, 1074, 5, 1987, 17975, 45, 55, 221, 15, 670, 5304, 526, 14, 1069, 4, 405, 5, 2438, 7, 27, 85, 108, 131, 4, 5045, 5304, 3884, 405, 9, 3523, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 8463, 2801, 45, 407, 31, 7, 41, 3778, 105, 21, 59, 299, 12, 38, 950, 5, 4521, 15, 45, 629, 488, 2733, 127, 6, 52, 292, 17, 4, 6936, 185, 132, 1988, 5304, 1799, 488, 2693, 47, 6, 392, 173, 4, 21686, 4378, 270, 2352, 4, 1500, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2078, 7, 5293, 861, 12746, 5, 4182, 30, 3127, 23651, 56, 4, 841, 5, 990, 692, 8, 4, 1669, 398, 229, 10, 10, 13, 2822, 670, 5304, 14, 9, 31, 7, 27, 111, 108, 15, 2033, 19, 7836, 1429, 875, 551, 14, 22, 9, 1193, 21, 45, 4829, 5, 45, 252, 8, 12508, 6, 565, 921, 3639, 39, 4, 529, 48, 25, 181, 8, 67, 35, 1732, 22, 49, 238, 60, 135, 1162, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 5687, 25, 203, 28, 8, 818, 12, 125, 4, 3077]),\n",
       "         list([1, 13, 1228, 119, 14, 552, 7, 20, 190, 14, 58, 13, 258, 546, 1786, 8, 1968, 4, 268, 237, 13, 191, 81, 15, 13, 80, 43, 3824, 44, 12, 14, 16, 427, 3192, 4, 183, 15, 593, 19, 4, 351, 362, 26, 55, 646, 21, 4, 1239, 84, 26, 1557, 3755, 13, 244, 6, 2071, 132, 184, 194, 5, 13, 70, 4478, 546, 73, 190, 13, 62, 24, 81, 320, 4, 538, 4, 117, 250, 127, 11, 14, 20, 82, 4, 452, 11, 14, 20, 9, 8654, 19, 41, 476, 8, 4, 213, 7, 9185, 13, 657, 13, 286, 38, 1612, 44, 41, 5, 41, 1729, 88, 13, 62, 28, 900, 510, 4, 509, 51, 6, 612, 59, 16, 193, 61, 4666, 5, 702, 930, 143, 285, 25, 67, 41, 81, 366, 4, 130, 82, 9, 259, 334, 397, 1195, 7, 149, 102, 15, 26, 814, 38, 465, 1627, 31, 70, 983, 67, 51, 9, 112, 814, 17, 35, 311, 75, 26, 11649, 574, 19, 4, 1729, 23, 4, 268, 38, 95, 138, 4, 609, 191, 75, 28, 314, 1772]),\n",
       "         ...,\n",
       "         list([1, 13, 1408, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 5093, 21, 45, 184, 78, 4, 1492, 910, 769, 2290, 2515, 395, 4257, 5, 1454, 11, 119, 16946, 89, 1036, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2280, 284, 1842, 60664, 37, 315, 4, 226, 20, 272, 2942, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 5518]),\n",
       "         list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 3684, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2960, 7, 743, 46, 1028, 9, 3531, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 28739, 5, 9, 1861, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 1276, 92, 124, 51, 45, 1188, 71, 536, 13, 520, 14, 20, 6, 2302, 7, 470]),\n",
       "         list([1, 6, 52, 7465, 430, 22, 9, 220, 2594, 8, 28, 24357, 519, 3227, 6, 769, 15, 47, 6, 3482, 4067, 8, 114, 5, 33, 222, 31, 55, 184, 704, 5586, 18020, 19, 346, 3153, 5, 6, 364, 350, 4, 184, 5586, 9, 133, 1810, 11, 5417, 13226, 21, 4, 7298, 42657, 570, 50, 2005, 2643, 9, 6, 1249, 17, 6, 25194, 27803, 21, 17, 6, 1211, 232, 1138, 2249, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 1078, 19, 4, 78, 173, 7, 27, 20067, 5698, 3406, 718, 21264, 9, 6, 6907, 17, 210, 5, 3281, 5677, 47, 77, 395, 14, 172, 173, 18, 2740, 2931, 4517, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 14274, 53, 40, 35, 390, 7, 11, 4, 3567, 7, 4, 314, 74, 6, 792, 22, 16261, 19, 714, 727, 5205, 382, 4, 91, 6533, 439, 19, 14, 20, 9, 1441, 5805, 1118, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2005, 2643])],\n",
       "        dtype=object),\n",
       "  array([0, 1, 0, ..., 0, 0, 0], dtype=int64)))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.load_data(maxlen=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
       "         list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "         list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
       "         ...,\n",
       "         list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 86527, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 14532, 325, 725, 134, 15271, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 11656, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 26094, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 17793, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 14492, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 17793, 5, 27, 710, 117, 74936, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 17793, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 17793, 7750, 5, 4241, 18, 4, 8497, 13164, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 11027, 4, 3586, 22459]),\n",
       "         list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 21469, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 40691, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 29455, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 11418, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 21213, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
       "         list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 12815, 270, 14437, 5, 16923, 12255, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 16553, 21, 27, 9685, 6139, 5, 29043, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 85010, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 70907, 10755, 544, 5, 383, 1271, 848, 1468, 12183, 497, 16876, 8, 1597, 8778, 19280, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
       "        dtype=object),\n",
       "  array([1, 0, 0, ..., 0, 1, 0], dtype=int64)),\n",
       " (array([list([1, 591, 202, 14, 31, 6, 717, 10, 10, 18142, 10698, 5, 4, 360, 7, 4, 177, 5760, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 7944, 100, 28, 1668, 14, 31, 23, 27, 7479, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 15387, 38, 32, 25, 7944, 451, 202, 14, 6, 717]),\n",
       "         list([1, 14, 22, 3443, 6, 176, 7, 5063, 88, 12, 2679, 23, 1310, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 7216, 10626, 4, 8463, 2801, 109, 1603, 21, 4, 22, 3861, 8, 6, 1193, 1330, 10, 10, 4, 105, 987, 35, 841, 16873, 19, 861, 1074, 5, 1987, 17975, 45, 55, 221, 15, 670, 5304, 526, 14, 1069, 4, 405, 5, 2438, 7, 27, 85, 108, 131, 4, 5045, 5304, 3884, 405, 9, 3523, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 8463, 2801, 45, 407, 31, 7, 41, 3778, 105, 21, 59, 299, 12, 38, 950, 5, 4521, 15, 45, 629, 488, 2733, 127, 6, 52, 292, 17, 4, 6936, 185, 132, 1988, 5304, 1799, 488, 2693, 47, 6, 392, 173, 4, 21686, 4378, 270, 2352, 4, 1500, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2078, 7, 5293, 861, 12746, 5, 4182, 30, 3127, 23651, 56, 4, 841, 5, 990, 692, 8, 4, 1669, 398, 229, 10, 10, 13, 2822, 670, 5304, 14, 9, 31, 7, 27, 111, 108, 15, 2033, 19, 7836, 1429, 875, 551, 14, 22, 9, 1193, 21, 45, 4829, 5, 45, 252, 8, 12508, 6, 565, 921, 3639, 39, 4, 529, 48, 25, 181, 8, 67, 35, 1732, 22, 49, 238, 60, 135, 1162, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 5687, 25, 203, 28, 8, 818, 12, 125, 4, 3077]),\n",
       "         list([1, 111, 748, 4368, 1133, 33782, 24563, 4, 87, 1551, 1262, 7, 31, 318, 9459, 7, 4, 498, 5076, 748, 63, 29, 5161, 220, 686, 10941, 5, 17, 12, 575, 220, 2507, 17, 6, 185, 132, 24563, 16, 53, 928, 11, 51278, 74, 4, 438, 21, 27, 10044, 589, 8, 22, 107, 20123, 19550, 997, 1638, 8, 35, 2076, 9019, 11, 22, 231, 54, 29, 1706, 29, 100, 18995, 2425, 34, 12998, 8738, 48078, 5, 19353, 98, 31, 2122, 33, 6, 58, 14, 3808, 1638, 8, 4, 365, 7, 2789, 3761, 356, 346, 4, 27608, 1060, 63, 29, 93, 11, 5421, 11, 15236, 33, 6, 58, 54, 1270, 431, 748, 7, 32, 2580, 16, 11, 94, 19469, 10, 10, 4, 993, 45222, 7, 4, 1766, 2634, 2164, 24563, 8, 847, 8, 1450, 121, 31, 7, 27, 86, 2663, 10760, 16, 6, 465, 993, 2006, 30995, 573, 17, 61862, 42, 4, 17345, 37, 473, 6, 711, 6, 8869, 7, 328, 212, 70, 30, 258, 11, 220, 32, 7, 108, 21, 133, 12, 9, 55, 465, 849, 3711, 53, 33, 2071, 1969, 37, 70, 1144, 4, 5940, 1409, 74, 476, 37, 62, 91, 1329, 169, 4, 1330, 10104, 146, 655, 2212, 5, 258, 12, 184, 10104, 546, 5, 849, 10333, 7, 4, 22, 1436, 18, 631, 1386, 797, 7, 4, 8712, 71, 348, 425, 4320, 1061, 19, 10288, 5, 12141, 11, 661, 8, 339, 17863, 4, 2455, 11434, 7, 4, 1962, 10, 10, 263, 787, 9, 270, 11, 6, 9466, 4, 61862, 48414, 121, 4, 5437, 26, 4434, 19, 68, 1372, 5, 28, 446, 6, 318, 7149, 8, 67, 51, 36, 70, 81, 8, 4392, 2294, 36, 1197, 8, 68411, 25399, 18, 6, 711, 4, 9909, 26, 10296, 1125, 11, 14, 636, 720, 12, 426, 28, 77, 776, 8, 97, 38, 111, 7489, 6175, 168, 1239, 5189, 137, 25399, 18, 27, 173, 9, 2399, 17, 6, 12397, 428, 14657, 232, 11, 4, 8014, 37, 272, 40, 2708, 247, 30, 656, 6, 13182, 54, 25399, 3292, 98, 6, 2840, 40, 558, 37, 6093, 98, 4, 17345, 1197, 15, 14, 9, 57, 4893, 5, 4659, 6, 275, 711, 7937, 25399, 3292, 98, 6, 31036, 10, 10, 6639, 19, 14, 10241, 267, 162, 711, 37, 5900, 752, 98, 4, 17345, 2378, 90, 19, 6, 73284, 7, 36744, 1810, 77553, 4, 4770, 3183, 930, 8, 508, 90, 4, 1317, 8, 4, 48414, 17, 15454, 3965, 1853, 4, 1494, 8, 4468, 189, 4, 31036, 6287, 5774, 4, 4770, 5, 95, 271, 23, 6, 7742, 6063, 21627, 5437, 33, 1526, 6, 425, 3155, 33697, 4535, 1636, 7, 4, 4669, 11966, 469, 4, 4552, 54, 4, 150, 5664, 17345, 280, 53, 68411, 25399, 18, 339, 29, 1978, 27, 7885, 5, 17303, 68, 1830, 19, 6571, 14605, 4, 1515, 7, 263, 65, 2132, 34, 6, 5680, 7489, 43, 159, 29, 9, 4706, 9, 387, 73, 195, 584, 10, 10, 1069, 4, 58, 810, 54, 14, 6078, 117, 22, 16, 93, 5, 1069, 4, 192, 15, 12, 16, 93, 34, 6, 1766, 28228, 33, 4, 5673, 7, 15, 18760, 9252, 3286, 325, 12, 62, 30, 776, 8, 67, 14, 17, 6, 12214, 44, 148, 687, 24563, 203, 42, 203, 24, 28, 69, 32157, 6676, 11, 330, 54, 29, 93, 61862, 21, 845, 14148, 27, 1099, 7, 819, 4, 22, 1407, 17, 6, 14967, 787, 7, 2460, 19569, 61862, 100, 30, 4, 3737, 3617, 3169, 2321, 42, 1898, 11, 4, 3814, 42, 101, 704, 7, 101, 999, 15, 1625, 94, 2926, 180, 5, 9, 9101, 34, 15205, 45, 6, 1429, 22, 60, 6, 1220, 31, 11, 94, 6408, 96, 21, 94, 749, 9, 57, 975]),\n",
       "         ...,\n",
       "         list([1, 13, 1408, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 5093, 21, 45, 184, 78, 4, 1492, 910, 769, 2290, 2515, 395, 4257, 5, 1454, 11, 119, 16946, 89, 1036, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2280, 284, 1842, 60664, 37, 315, 4, 226, 20, 272, 2942, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 5518]),\n",
       "         list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 3684, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2960, 7, 743, 46, 1028, 9, 3531, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 28739, 5, 9, 1861, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 1276, 92, 124, 51, 45, 1188, 71, 536, 13, 520, 14, 20, 6, 2302, 7, 470]),\n",
       "         list([1, 6, 52, 7465, 430, 22, 9, 220, 2594, 8, 28, 24357, 519, 3227, 6, 769, 15, 47, 6, 3482, 4067, 8, 114, 5, 33, 222, 31, 55, 184, 704, 5586, 18020, 19, 346, 3153, 5, 6, 364, 350, 4, 184, 5586, 9, 133, 1810, 11, 5417, 13226, 21, 4, 7298, 42657, 570, 50, 2005, 2643, 9, 6, 1249, 17, 6, 25194, 27803, 21, 17, 6, 1211, 232, 1138, 2249, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 1078, 19, 4, 78, 173, 7, 27, 20067, 5698, 3406, 718, 21264, 9, 6, 6907, 17, 210, 5, 3281, 5677, 47, 77, 395, 14, 172, 173, 18, 2740, 2931, 4517, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 14274, 53, 40, 35, 390, 7, 11, 4, 3567, 7, 4, 314, 74, 6, 792, 22, 16261, 19, 714, 727, 5205, 382, 4, 91, 6533, 439, 19, 14, 20, 9, 1441, 5805, 1118, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2005, 2643])],\n",
       "        dtype=object),\n",
       "  array([0, 1, 1, ..., 0, 0, 0], dtype=int64)))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use '1' as the character that indicates the start of a sequence\n",
    "imdb.load_data(start_char=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the dataset word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "imdb_word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the word index as a dictionary\n",
    "# Accounting for index_from\n",
    "index_from = 3\n",
    "test = {key: value + index_from for key, value in imdb_word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fawn': 34704,\n",
       " 'tsukino': 52009,\n",
       " 'nunnery': 52010,\n",
       " 'sonja': 16819,\n",
       " 'vani': 63954,\n",
       " 'woods': 1411,\n",
       " 'spiders': 16118,\n",
       " 'hanging': 2348,\n",
       " 'woody': 2292,\n",
       " 'trawling': 52011,\n",
       " \"hold's\": 52012,\n",
       " 'comically': 11310,\n",
       " 'localized': 40833,\n",
       " 'disobeying': 30571,\n",
       " \"'royale\": 52013,\n",
       " \"harpo's\": 40834,\n",
       " 'canet': 52014,\n",
       " 'aileen': 19316,\n",
       " 'acurately': 52015,\n",
       " \"diplomat's\": 52016,\n",
       " 'rickman': 25245,\n",
       " 'arranged': 6749,\n",
       " 'rumbustious': 52017,\n",
       " 'familiarness': 52018,\n",
       " \"spider'\": 52019,\n",
       " 'hahahah': 68807,\n",
       " \"wood'\": 52020,\n",
       " 'transvestism': 40836,\n",
       " \"hangin'\": 34705,\n",
       " 'bringing': 2341,\n",
       " 'seamier': 40837,\n",
       " 'wooded': 34706,\n",
       " 'bravora': 52021,\n",
       " 'grueling': 16820,\n",
       " 'wooden': 1639,\n",
       " 'wednesday': 16821,\n",
       " \"'prix\": 52022,\n",
       " 'altagracia': 34707,\n",
       " 'circuitry': 52023,\n",
       " 'crotch': 11588,\n",
       " 'busybody': 57769,\n",
       " \"tart'n'tangy\": 52024,\n",
       " 'burgade': 14132,\n",
       " 'thrace': 52026,\n",
       " \"tom's\": 11041,\n",
       " 'snuggles': 52028,\n",
       " 'francesco': 29117,\n",
       " 'complainers': 52030,\n",
       " 'templarios': 52128,\n",
       " '272': 40838,\n",
       " '273': 52031,\n",
       " 'zaniacs': 52133,\n",
       " '275': 34709,\n",
       " 'consenting': 27634,\n",
       " 'snuggled': 40839,\n",
       " 'inanimate': 15495,\n",
       " 'uality': 52033,\n",
       " 'bronte': 11929,\n",
       " 'errors': 4013,\n",
       " 'dialogs': 3233,\n",
       " \"yomada's\": 52034,\n",
       " \"madman's\": 34710,\n",
       " 'dialoge': 30588,\n",
       " 'usenet': 52036,\n",
       " 'videodrome': 40840,\n",
       " \"kid'\": 26341,\n",
       " 'pawed': 52037,\n",
       " \"'girlfriend'\": 30572,\n",
       " \"'pleasure\": 52038,\n",
       " \"'reloaded'\": 52039,\n",
       " \"kazakos'\": 40842,\n",
       " 'rocque': 52040,\n",
       " 'mailings': 52041,\n",
       " 'brainwashed': 11930,\n",
       " 'mcanally': 16822,\n",
       " \"tom''\": 52042,\n",
       " 'kurupt': 25246,\n",
       " 'affiliated': 21908,\n",
       " 'babaganoosh': 52043,\n",
       " \"noe's\": 40843,\n",
       " 'quart': 40844,\n",
       " 'kids': 362,\n",
       " 'uplifting': 5037,\n",
       " 'controversy': 7096,\n",
       " 'kida': 21909,\n",
       " 'kidd': 23382,\n",
       " \"error'\": 52044,\n",
       " 'neurologist': 52045,\n",
       " 'spotty': 18513,\n",
       " 'cobblers': 30573,\n",
       " 'projection': 9881,\n",
       " 'fastforwarding': 40845,\n",
       " 'sters': 52046,\n",
       " \"eggar's\": 52047,\n",
       " 'etherything': 52048,\n",
       " 'gateshead': 40846,\n",
       " 'airball': 34711,\n",
       " 'unsinkable': 25247,\n",
       " 'stern': 7183,\n",
       " \"cervi's\": 52049,\n",
       " 'dnd': 40847,\n",
       " 'dna': 11589,\n",
       " 'insecurity': 20601,\n",
       " \"'reboot'\": 52050,\n",
       " 'trelkovsky': 11040,\n",
       " 'jaekel': 52051,\n",
       " 'sidebars': 52052,\n",
       " \"sforza's\": 52053,\n",
       " 'distortions': 17636,\n",
       " 'mutinies': 52054,\n",
       " 'sermons': 30605,\n",
       " '7ft': 40849,\n",
       " 'boobage': 52055,\n",
       " \"o'bannon's\": 52056,\n",
       " 'populations': 23383,\n",
       " 'chulak': 52057,\n",
       " 'mesmerize': 27636,\n",
       " 'quinnell': 52058,\n",
       " 'yahoo': 10310,\n",
       " 'meteorologist': 52060,\n",
       " 'beswick': 42580,\n",
       " 'boorman': 15496,\n",
       " 'voicework': 40850,\n",
       " \"ster'\": 52061,\n",
       " 'blustering': 22925,\n",
       " 'hj': 52062,\n",
       " 'intake': 27637,\n",
       " 'morally': 5624,\n",
       " 'jumbling': 40852,\n",
       " 'bowersock': 52063,\n",
       " \"'porky's'\": 52064,\n",
       " 'gershon': 16824,\n",
       " 'ludicrosity': 40853,\n",
       " 'coprophilia': 52065,\n",
       " 'expressively': 40854,\n",
       " \"india's\": 19503,\n",
       " \"post's\": 34713,\n",
       " 'wana': 52066,\n",
       " 'wang': 5286,\n",
       " 'wand': 30574,\n",
       " 'wane': 25248,\n",
       " 'edgeways': 52324,\n",
       " 'titanium': 34714,\n",
       " 'pinta': 40855,\n",
       " 'want': 181,\n",
       " 'pinto': 30575,\n",
       " 'whoopdedoodles': 52068,\n",
       " 'tchaikovsky': 21911,\n",
       " 'travel': 2106,\n",
       " \"'victory'\": 52069,\n",
       " 'copious': 11931,\n",
       " 'gouge': 22436,\n",
       " \"chapters'\": 52070,\n",
       " 'barbra': 6705,\n",
       " 'uselessness': 30576,\n",
       " \"wan'\": 52071,\n",
       " 'assimilated': 27638,\n",
       " 'petiot': 16119,\n",
       " 'most\\x85and': 52072,\n",
       " 'dinosaurs': 3933,\n",
       " 'wrong': 355,\n",
       " 'seda': 52073,\n",
       " 'stollen': 52074,\n",
       " 'sentencing': 34715,\n",
       " 'ouroboros': 40856,\n",
       " 'assimilates': 40857,\n",
       " 'colorfully': 40858,\n",
       " 'glenne': 27639,\n",
       " 'dongen': 52075,\n",
       " 'subplots': 4763,\n",
       " 'kiloton': 52076,\n",
       " 'chandon': 23384,\n",
       " \"effect'\": 34716,\n",
       " 'snugly': 27640,\n",
       " 'kuei': 40859,\n",
       " 'welcomed': 9095,\n",
       " 'dishonor': 30074,\n",
       " 'concurrence': 52078,\n",
       " 'stoicism': 23385,\n",
       " \"guys'\": 14899,\n",
       " \"beroemd'\": 52080,\n",
       " 'butcher': 6706,\n",
       " \"melfi's\": 40860,\n",
       " 'aargh': 30626,\n",
       " 'playhouse': 20602,\n",
       " 'wickedly': 11311,\n",
       " 'fit': 1183,\n",
       " 'labratory': 52081,\n",
       " 'lifeline': 40862,\n",
       " 'screaming': 1930,\n",
       " 'fix': 4290,\n",
       " 'cineliterate': 52082,\n",
       " 'fic': 52083,\n",
       " 'fia': 52084,\n",
       " 'fig': 34717,\n",
       " 'fmvs': 52085,\n",
       " 'fie': 52086,\n",
       " 'reentered': 52087,\n",
       " 'fin': 30577,\n",
       " 'doctresses': 52088,\n",
       " 'fil': 52089,\n",
       " 'zucker': 12609,\n",
       " 'ached': 31934,\n",
       " 'counsil': 52091,\n",
       " 'paterfamilias': 52092,\n",
       " 'songwriter': 13888,\n",
       " 'shivam': 34718,\n",
       " 'hurting': 9657,\n",
       " 'effects': 302,\n",
       " 'slauther': 52093,\n",
       " \"'flame'\": 52094,\n",
       " 'sommerset': 52095,\n",
       " 'interwhined': 52096,\n",
       " 'whacking': 27641,\n",
       " 'bartok': 52097,\n",
       " 'barton': 8778,\n",
       " 'frewer': 21912,\n",
       " \"fi'\": 52098,\n",
       " 'ingrid': 6195,\n",
       " 'stribor': 30578,\n",
       " 'approporiately': 52099,\n",
       " 'wobblyhand': 52100,\n",
       " 'tantalisingly': 52101,\n",
       " 'ankylosaurus': 52102,\n",
       " 'parasites': 17637,\n",
       " 'childen': 52103,\n",
       " \"jenkins'\": 52104,\n",
       " 'metafiction': 52105,\n",
       " 'golem': 17638,\n",
       " 'indiscretion': 40863,\n",
       " \"reeves'\": 23386,\n",
       " \"inamorata's\": 57784,\n",
       " 'brittannica': 52107,\n",
       " 'adapt': 7919,\n",
       " \"russo's\": 30579,\n",
       " 'guitarists': 48249,\n",
       " 'abbott': 10556,\n",
       " 'abbots': 40864,\n",
       " 'lanisha': 17652,\n",
       " 'magickal': 40866,\n",
       " 'mattter': 52108,\n",
       " \"'willy\": 52109,\n",
       " 'pumpkins': 34719,\n",
       " 'stuntpeople': 52110,\n",
       " 'estimate': 30580,\n",
       " 'ugghhh': 40867,\n",
       " 'gameplay': 11312,\n",
       " \"wern't\": 52111,\n",
       " \"n'sync\": 40868,\n",
       " 'sickeningly': 16120,\n",
       " 'chiara': 40869,\n",
       " 'disturbed': 4014,\n",
       " 'portmanteau': 40870,\n",
       " 'ineffectively': 52112,\n",
       " \"duchonvey's\": 82146,\n",
       " \"nasty'\": 37522,\n",
       " 'purpose': 1288,\n",
       " 'lazers': 52115,\n",
       " 'lightened': 28108,\n",
       " 'kaliganj': 52116,\n",
       " 'popularism': 52117,\n",
       " \"damme's\": 18514,\n",
       " 'stylistics': 30581,\n",
       " 'mindgaming': 52118,\n",
       " 'spoilerish': 46452,\n",
       " \"'corny'\": 52120,\n",
       " 'boerner': 34721,\n",
       " 'olds': 6795,\n",
       " 'bakelite': 52121,\n",
       " 'renovated': 27642,\n",
       " 'forrester': 27643,\n",
       " \"lumiere's\": 52122,\n",
       " 'gaskets': 52027,\n",
       " 'needed': 887,\n",
       " 'smight': 34722,\n",
       " 'master': 1300,\n",
       " \"edie's\": 25908,\n",
       " 'seeber': 40871,\n",
       " 'hiya': 52123,\n",
       " 'fuzziness': 52124,\n",
       " 'genesis': 14900,\n",
       " 'rewards': 12610,\n",
       " 'enthrall': 30582,\n",
       " \"'about\": 40872,\n",
       " \"recollection's\": 52125,\n",
       " 'mutilated': 11042,\n",
       " 'fatherlands': 52126,\n",
       " \"fischer's\": 52127,\n",
       " 'positively': 5402,\n",
       " '270': 34708,\n",
       " 'ahmed': 34723,\n",
       " 'zatoichi': 9839,\n",
       " 'bannister': 13889,\n",
       " 'anniversaries': 52130,\n",
       " \"helm's\": 30583,\n",
       " \"'work'\": 52131,\n",
       " 'exclaimed': 34724,\n",
       " \"'unfunny'\": 52132,\n",
       " '274': 52032,\n",
       " 'feeling': 547,\n",
       " \"wanda's\": 52134,\n",
       " 'dolan': 33269,\n",
       " '278': 52136,\n",
       " 'peacoat': 52137,\n",
       " 'brawny': 40873,\n",
       " 'mishra': 40874,\n",
       " 'worlders': 40875,\n",
       " 'protags': 52138,\n",
       " 'skullcap': 52139,\n",
       " 'dastagir': 57599,\n",
       " 'affairs': 5625,\n",
       " 'wholesome': 7802,\n",
       " 'hymen': 52140,\n",
       " 'paramedics': 25249,\n",
       " 'unpersons': 52141,\n",
       " 'heavyarms': 52142,\n",
       " 'affaire': 52143,\n",
       " 'coulisses': 52144,\n",
       " 'hymer': 40876,\n",
       " 'kremlin': 52145,\n",
       " 'shipments': 30584,\n",
       " 'pixilated': 52146,\n",
       " \"'00s\": 30585,\n",
       " 'diminishing': 18515,\n",
       " 'cinematic': 1360,\n",
       " 'resonates': 14901,\n",
       " 'simplify': 40877,\n",
       " \"nature'\": 40878,\n",
       " 'temptresses': 40879,\n",
       " 'reverence': 16825,\n",
       " 'resonated': 19505,\n",
       " 'dailey': 34725,\n",
       " '2\\x85': 52147,\n",
       " 'treize': 27644,\n",
       " 'majo': 52148,\n",
       " 'kiya': 21913,\n",
       " 'woolnough': 52149,\n",
       " 'thanatos': 39800,\n",
       " 'sandoval': 35734,\n",
       " 'dorama': 40882,\n",
       " \"o'shaughnessy\": 52150,\n",
       " 'tech': 4991,\n",
       " 'fugitives': 32021,\n",
       " 'teck': 30586,\n",
       " \"'e'\": 76128,\n",
       " 'doesnt': 40884,\n",
       " 'purged': 52152,\n",
       " 'saying': 660,\n",
       " \"martians'\": 41098,\n",
       " 'norliss': 23421,\n",
       " 'dickey': 27645,\n",
       " 'dicker': 52155,\n",
       " \"'sependipity\": 52156,\n",
       " 'padded': 8425,\n",
       " 'ordell': 57795,\n",
       " \"sturges'\": 40885,\n",
       " 'independentcritics': 52157,\n",
       " 'tempted': 5748,\n",
       " \"atkinson's\": 34727,\n",
       " 'hounded': 25250,\n",
       " 'apace': 52158,\n",
       " 'clicked': 15497,\n",
       " \"'humor'\": 30587,\n",
       " \"martino's\": 17180,\n",
       " \"'supporting\": 52159,\n",
       " 'warmongering': 52035,\n",
       " \"zemeckis's\": 34728,\n",
       " 'lube': 21914,\n",
       " 'shocky': 52160,\n",
       " 'plate': 7479,\n",
       " 'plata': 40886,\n",
       " 'sturgess': 40887,\n",
       " \"nerds'\": 40888,\n",
       " 'plato': 20603,\n",
       " 'plath': 34729,\n",
       " 'platt': 40889,\n",
       " 'mcnab': 52162,\n",
       " 'clumsiness': 27646,\n",
       " 'altogether': 3902,\n",
       " 'massacring': 42587,\n",
       " 'bicenntinial': 52163,\n",
       " 'skaal': 40890,\n",
       " 'droning': 14363,\n",
       " 'lds': 8779,\n",
       " 'jaguar': 21915,\n",
       " \"cale's\": 34730,\n",
       " 'nicely': 1780,\n",
       " 'mummy': 4591,\n",
       " \"lot's\": 18516,\n",
       " 'patch': 10089,\n",
       " 'kerkhof': 50205,\n",
       " \"leader's\": 52164,\n",
       " \"'movie\": 27647,\n",
       " 'uncomfirmed': 52165,\n",
       " 'heirloom': 40891,\n",
       " 'wrangle': 47363,\n",
       " 'emotion\\x85': 52166,\n",
       " \"'stargate'\": 52167,\n",
       " 'pinoy': 40892,\n",
       " 'conchatta': 40893,\n",
       " 'broeke': 41131,\n",
       " 'advisedly': 40894,\n",
       " \"barker's\": 17639,\n",
       " 'descours': 52169,\n",
       " 'lots': 775,\n",
       " 'lotr': 9262,\n",
       " 'irs': 9882,\n",
       " 'lott': 52170,\n",
       " 'xvi': 40895,\n",
       " 'irk': 34731,\n",
       " 'irl': 52171,\n",
       " 'ira': 6890,\n",
       " 'belzer': 21916,\n",
       " 'irc': 52172,\n",
       " 'ire': 27648,\n",
       " 'requisites': 40896,\n",
       " 'discipline': 7696,\n",
       " 'lyoko': 52964,\n",
       " 'extend': 11313,\n",
       " 'nature': 876,\n",
       " \"'dickie'\": 52173,\n",
       " 'optimist': 40897,\n",
       " 'lapping': 30589,\n",
       " 'superficial': 3903,\n",
       " 'vestment': 52174,\n",
       " 'extent': 2826,\n",
       " 'tendons': 52175,\n",
       " \"heller's\": 52176,\n",
       " 'quagmires': 52177,\n",
       " 'miyako': 52178,\n",
       " 'moocow': 20604,\n",
       " \"coles'\": 52179,\n",
       " 'lookit': 40898,\n",
       " 'ravenously': 52180,\n",
       " 'levitating': 40899,\n",
       " 'perfunctorily': 52181,\n",
       " 'lookin': 30590,\n",
       " \"lot'\": 40901,\n",
       " 'lookie': 52182,\n",
       " 'fearlessly': 34873,\n",
       " 'libyan': 52184,\n",
       " 'fondles': 40902,\n",
       " 'gopher': 35717,\n",
       " 'wearying': 40904,\n",
       " \"nz's\": 52185,\n",
       " 'minuses': 27649,\n",
       " 'puposelessly': 52186,\n",
       " 'shandling': 52187,\n",
       " 'decapitates': 31271,\n",
       " 'humming': 11932,\n",
       " \"'nother\": 40905,\n",
       " 'smackdown': 21917,\n",
       " 'underdone': 30591,\n",
       " 'frf': 40906,\n",
       " 'triviality': 52188,\n",
       " 'fro': 25251,\n",
       " 'bothers': 8780,\n",
       " \"'kensington\": 52189,\n",
       " 'much': 76,\n",
       " 'muco': 34733,\n",
       " 'wiseguy': 22618,\n",
       " \"richie's\": 27651,\n",
       " 'tonino': 40907,\n",
       " 'unleavened': 52190,\n",
       " 'fry': 11590,\n",
       " \"'tv'\": 40908,\n",
       " 'toning': 40909,\n",
       " 'obese': 14364,\n",
       " 'sensationalized': 30592,\n",
       " 'spiv': 40910,\n",
       " 'spit': 6262,\n",
       " 'arkin': 7367,\n",
       " 'charleton': 21918,\n",
       " 'jeon': 16826,\n",
       " 'boardroom': 21919,\n",
       " 'doubts': 4992,\n",
       " 'spin': 3087,\n",
       " 'hepo': 53086,\n",
       " 'wildcat': 27652,\n",
       " 'venoms': 10587,\n",
       " 'misconstrues': 52194,\n",
       " 'mesmerising': 18517,\n",
       " 'misconstrued': 40911,\n",
       " 'rescinds': 52195,\n",
       " 'prostrate': 52196,\n",
       " 'majid': 40912,\n",
       " 'climbed': 16482,\n",
       " 'canoeing': 34734,\n",
       " 'majin': 52198,\n",
       " 'animie': 57807,\n",
       " 'sylke': 40913,\n",
       " 'conditioned': 14902,\n",
       " 'waddell': 40914,\n",
       " '3\\x85': 52199,\n",
       " 'hyperdrive': 41191,\n",
       " 'conditioner': 34735,\n",
       " 'bricklayer': 53156,\n",
       " 'hong': 2579,\n",
       " 'memoriam': 52201,\n",
       " 'inventively': 30595,\n",
       " \"levant's\": 25252,\n",
       " 'portobello': 20641,\n",
       " 'remand': 52203,\n",
       " 'mummified': 19507,\n",
       " 'honk': 27653,\n",
       " 'spews': 19508,\n",
       " 'visitations': 40915,\n",
       " 'mummifies': 52204,\n",
       " 'cavanaugh': 25253,\n",
       " 'zeon': 23388,\n",
       " \"jungle's\": 40916,\n",
       " 'viertel': 34736,\n",
       " 'frenchmen': 27654,\n",
       " 'torpedoes': 52205,\n",
       " 'schlessinger': 52206,\n",
       " 'torpedoed': 34737,\n",
       " 'blister': 69879,\n",
       " 'cinefest': 52207,\n",
       " 'furlough': 34738,\n",
       " 'mainsequence': 52208,\n",
       " 'mentors': 40917,\n",
       " 'academic': 9097,\n",
       " 'stillness': 20605,\n",
       " 'academia': 40918,\n",
       " 'lonelier': 52209,\n",
       " 'nibby': 52210,\n",
       " \"losers'\": 52211,\n",
       " 'cineastes': 40919,\n",
       " 'corporate': 4452,\n",
       " 'massaging': 40920,\n",
       " 'bellow': 30596,\n",
       " 'absurdities': 19509,\n",
       " 'expetations': 53244,\n",
       " 'nyfiken': 40921,\n",
       " 'mehras': 75641,\n",
       " 'lasse': 52212,\n",
       " 'visability': 52213,\n",
       " 'militarily': 33949,\n",
       " \"elder'\": 52214,\n",
       " 'gainsbourg': 19026,\n",
       " 'hah': 20606,\n",
       " 'hai': 13423,\n",
       " 'haj': 34739,\n",
       " 'hak': 25254,\n",
       " 'hal': 4314,\n",
       " 'ham': 4895,\n",
       " 'duffer': 53262,\n",
       " 'haa': 52216,\n",
       " 'had': 69,\n",
       " 'advancement': 11933,\n",
       " 'hag': 16828,\n",
       " \"hand'\": 25255,\n",
       " 'hay': 13424,\n",
       " 'mcnamara': 20607,\n",
       " \"mozart's\": 52217,\n",
       " 'duffel': 30734,\n",
       " 'haq': 30597,\n",
       " 'har': 13890,\n",
       " 'has': 47,\n",
       " 'hat': 2404,\n",
       " 'hav': 40922,\n",
       " 'haw': 30598,\n",
       " 'figtings': 52218,\n",
       " 'elders': 15498,\n",
       " 'underpanted': 52219,\n",
       " 'pninson': 52220,\n",
       " 'unequivocally': 27655,\n",
       " \"barbara's\": 23676,\n",
       " \"bello'\": 52222,\n",
       " 'indicative': 13000,\n",
       " 'yawnfest': 40923,\n",
       " 'hexploitation': 52223,\n",
       " \"loder's\": 52224,\n",
       " 'sleuthing': 27656,\n",
       " \"justin's\": 32625,\n",
       " \"'ball\": 52225,\n",
       " \"'summer\": 52226,\n",
       " \"'demons'\": 34938,\n",
       " \"mormon's\": 52228,\n",
       " \"laughton's\": 34740,\n",
       " 'debell': 52229,\n",
       " 'shipyard': 39727,\n",
       " 'unabashedly': 30600,\n",
       " 'disks': 40404,\n",
       " 'crowd': 2293,\n",
       " 'crowe': 10090,\n",
       " \"vancouver's\": 56437,\n",
       " 'mosques': 34741,\n",
       " 'crown': 6630,\n",
       " 'culpas': 52230,\n",
       " 'crows': 27657,\n",
       " 'surrell': 53347,\n",
       " 'flowless': 52232,\n",
       " 'sheirk': 52233,\n",
       " \"'three\": 40926,\n",
       " \"peterson'\": 52234,\n",
       " 'ooverall': 52235,\n",
       " 'perchance': 40927,\n",
       " 'bottom': 1324,\n",
       " 'chabert': 53366,\n",
       " 'sneha': 52236,\n",
       " 'inhuman': 13891,\n",
       " 'ichii': 52237,\n",
       " 'ursla': 52238,\n",
       " 'completly': 30601,\n",
       " 'moviedom': 40928,\n",
       " 'raddick': 52239,\n",
       " 'brundage': 51998,\n",
       " 'brigades': 40929,\n",
       " 'starring': 1184,\n",
       " \"'goal'\": 52240,\n",
       " 'caskets': 52241,\n",
       " 'willcock': 52242,\n",
       " \"threesome's\": 52243,\n",
       " \"mosque'\": 52244,\n",
       " \"cover's\": 52245,\n",
       " 'spaceships': 17640,\n",
       " 'anomalous': 40930,\n",
       " 'ptsd': 27658,\n",
       " 'shirdan': 52246,\n",
       " 'obscenity': 21965,\n",
       " 'lemmings': 30602,\n",
       " 'duccio': 30603,\n",
       " \"levene's\": 52247,\n",
       " \"'gorby'\": 52248,\n",
       " \"teenager's\": 25258,\n",
       " 'marshall': 5343,\n",
       " 'honeymoon': 9098,\n",
       " 'shoots': 3234,\n",
       " 'despised': 12261,\n",
       " 'okabasho': 52249,\n",
       " 'fabric': 8292,\n",
       " 'cannavale': 18518,\n",
       " 'raped': 3540,\n",
       " \"tutt's\": 52250,\n",
       " 'grasping': 17641,\n",
       " 'despises': 18519,\n",
       " \"thief's\": 40931,\n",
       " 'rapes': 8929,\n",
       " 'raper': 52251,\n",
       " \"eyre'\": 27659,\n",
       " 'walchek': 52252,\n",
       " \"elmo's\": 23389,\n",
       " 'perfumes': 40932,\n",
       " 'spurting': 21921,\n",
       " \"exposition'\\x85\": 52253,\n",
       " 'denoting': 52254,\n",
       " 'thesaurus': 34743,\n",
       " \"shoot'\": 40933,\n",
       " 'bonejack': 49762,\n",
       " 'simpsonian': 52256,\n",
       " 'hebetude': 30604,\n",
       " \"hallow's\": 34744,\n",
       " 'desperation\\x85': 52257,\n",
       " 'incinerator': 34745,\n",
       " 'congratulations': 10311,\n",
       " 'humbled': 52258,\n",
       " \"else's\": 5927,\n",
       " 'trelkovski': 40848,\n",
       " \"rape'\": 52259,\n",
       " \"'chapters'\": 59389,\n",
       " '1600s': 52260,\n",
       " 'martian': 7256,\n",
       " 'nicest': 25259,\n",
       " 'eyred': 52262,\n",
       " 'passenger': 9460,\n",
       " 'disgrace': 6044,\n",
       " 'moderne': 52263,\n",
       " 'barrymore': 5123,\n",
       " 'yankovich': 52264,\n",
       " 'moderns': 40934,\n",
       " 'studliest': 52265,\n",
       " 'bedsheet': 52266,\n",
       " 'decapitation': 14903,\n",
       " 'slurring': 52267,\n",
       " \"'nunsploitation'\": 52268,\n",
       " \"'character'\": 34746,\n",
       " 'cambodia': 9883,\n",
       " 'rebelious': 52269,\n",
       " 'pasadena': 27660,\n",
       " 'crowne': 40935,\n",
       " \"'bedchamber\": 52270,\n",
       " 'conjectural': 52271,\n",
       " 'appologize': 52272,\n",
       " 'halfassing': 52273,\n",
       " 'paycheque': 57819,\n",
       " 'palms': 20609,\n",
       " \"'islands\": 52274,\n",
       " 'hawked': 40936,\n",
       " 'palme': 21922,\n",
       " 'conservatively': 40937,\n",
       " 'larp': 64010,\n",
       " 'palma': 5561,\n",
       " 'smelling': 21923,\n",
       " 'aragorn': 13001,\n",
       " 'hawker': 52275,\n",
       " 'hawkes': 52276,\n",
       " 'explosions': 3978,\n",
       " 'loren': 8062,\n",
       " \"pyle's\": 52277,\n",
       " 'shootout': 6707,\n",
       " \"mike's\": 18520,\n",
       " \"driscoll's\": 52278,\n",
       " 'cogsworth': 40938,\n",
       " \"britian's\": 52279,\n",
       " 'childs': 34747,\n",
       " \"portrait's\": 52280,\n",
       " 'chain': 3629,\n",
       " 'whoever': 2500,\n",
       " 'puttered': 52281,\n",
       " 'childe': 52282,\n",
       " 'maywether': 52283,\n",
       " 'chair': 3039,\n",
       " \"rance's\": 52284,\n",
       " 'machu': 34748,\n",
       " 'ballet': 4520,\n",
       " 'grapples': 34749,\n",
       " 'summerize': 76155,\n",
       " 'freelance': 30606,\n",
       " \"andrea's\": 52286,\n",
       " '\\x91very': 52287,\n",
       " 'coolidge': 45882,\n",
       " 'mache': 18521,\n",
       " 'balled': 52288,\n",
       " 'grappled': 40940,\n",
       " 'macha': 18522,\n",
       " 'underlining': 21924,\n",
       " 'macho': 5626,\n",
       " 'oversight': 19510,\n",
       " 'machi': 25260,\n",
       " 'verbally': 11314,\n",
       " 'tenacious': 21925,\n",
       " 'windshields': 40941,\n",
       " 'paychecks': 18560,\n",
       " 'jerk': 3399,\n",
       " \"good'\": 11934,\n",
       " 'prancer': 34751,\n",
       " 'prances': 21926,\n",
       " 'olympus': 52289,\n",
       " 'lark': 21927,\n",
       " 'embark': 10788,\n",
       " 'gloomy': 7368,\n",
       " 'jehaan': 52290,\n",
       " 'turaqui': 52291,\n",
       " \"child'\": 20610,\n",
       " 'locked': 2897,\n",
       " 'pranced': 52292,\n",
       " 'exact': 2591,\n",
       " 'unattuned': 52293,\n",
       " 'minute': 786,\n",
       " 'skewed': 16121,\n",
       " 'hodgins': 40943,\n",
       " 'skewer': 34752,\n",
       " 'think\\x85': 52294,\n",
       " 'rosenstein': 38768,\n",
       " 'helmit': 52295,\n",
       " 'wrestlemanias': 34753,\n",
       " 'hindered': 16829,\n",
       " \"martha's\": 30607,\n",
       " 'cheree': 52296,\n",
       " \"pluckin'\": 52297,\n",
       " 'ogles': 40944,\n",
       " 'heavyweight': 11935,\n",
       " 'aada': 82193,\n",
       " 'chopping': 11315,\n",
       " 'strongboy': 61537,\n",
       " 'hegemonic': 41345,\n",
       " 'adorns': 40945,\n",
       " 'xxth': 41349,\n",
       " 'nobuhiro': 34754,\n",
       " 'capites': 52301,\n",
       " 'kavogianni': 52302,\n",
       " 'antwerp': 13425,\n",
       " 'celebrated': 6541,\n",
       " 'roarke': 52303,\n",
       " 'baggins': 40946,\n",
       " 'cheeseburgers': 31273,\n",
       " 'matras': 52304,\n",
       " \"nineties'\": 52305,\n",
       " \"'craig'\": 52306,\n",
       " 'celebrates': 13002,\n",
       " 'unintentionally': 3386,\n",
       " 'drafted': 14365,\n",
       " 'climby': 52307,\n",
       " '303': 52308,\n",
       " 'oldies': 18523,\n",
       " 'climbs': 9099,\n",
       " 'honour': 9658,\n",
       " 'plucking': 34755,\n",
       " '305': 30077,\n",
       " 'address': 5517,\n",
       " 'menjou': 40947,\n",
       " \"'freak'\": 42595,\n",
       " 'dwindling': 19511,\n",
       " 'benson': 9461,\n",
       " 'whites': 52310,\n",
       " 'shamelessness': 40948,\n",
       " 'impacted': 21928,\n",
       " 'upatz': 52311,\n",
       " 'cusack': 3843,\n",
       " \"flavia's\": 37570,\n",
       " 'effette': 52312,\n",
       " 'influx': 34756,\n",
       " 'boooooooo': 52313,\n",
       " 'dimitrova': 52314,\n",
       " 'houseman': 13426,\n",
       " 'bigas': 25262,\n",
       " 'boylen': 52315,\n",
       " 'phillipenes': 52316,\n",
       " 'fakery': 40949,\n",
       " \"grandpa's\": 27661,\n",
       " 'darnell': 27662,\n",
       " 'undergone': 19512,\n",
       " 'handbags': 52318,\n",
       " 'perished': 21929,\n",
       " 'pooped': 37781,\n",
       " 'vigour': 27663,\n",
       " 'opposed': 3630,\n",
       " 'etude': 52319,\n",
       " \"caine's\": 11802,\n",
       " 'doozers': 52320,\n",
       " 'photojournals': 34757,\n",
       " 'perishes': 52321,\n",
       " 'constrains': 34758,\n",
       " 'migenes': 40951,\n",
       " 'consoled': 30608,\n",
       " 'alastair': 16830,\n",
       " 'wvs': 52322,\n",
       " 'ooooooh': 52323,\n",
       " 'approving': 34759,\n",
       " 'consoles': 40952,\n",
       " 'disparagement': 52067,\n",
       " 'futureistic': 52325,\n",
       " 'rebounding': 52326,\n",
       " \"'date\": 52327,\n",
       " 'gregoire': 52328,\n",
       " 'rutherford': 21930,\n",
       " 'americanised': 34760,\n",
       " 'novikov': 82199,\n",
       " 'following': 1045,\n",
       " 'munroe': 34761,\n",
       " \"morita'\": 52329,\n",
       " 'christenssen': 52330,\n",
       " 'oatmeal': 23109,\n",
       " 'fossey': 25263,\n",
       " 'livered': 40953,\n",
       " 'listens': 13003,\n",
       " \"'marci\": 76167,\n",
       " \"otis's\": 52333,\n",
       " 'thanking': 23390,\n",
       " 'maude': 16022,\n",
       " 'extensions': 34762,\n",
       " 'ameteurish': 52335,\n",
       " \"commender's\": 52336,\n",
       " 'agricultural': 27664,\n",
       " 'convincingly': 4521,\n",
       " 'fueled': 17642,\n",
       " 'mahattan': 54017,\n",
       " \"paris's\": 40955,\n",
       " 'vulkan': 52339,\n",
       " 'stapes': 52340,\n",
       " 'odysessy': 52341,\n",
       " 'harmon': 12262,\n",
       " 'surfing': 4255,\n",
       " 'halloran': 23497,\n",
       " 'unbelieveably': 49583,\n",
       " \"'offed'\": 52342,\n",
       " 'quadrant': 30610,\n",
       " 'inhabiting': 19513,\n",
       " 'nebbish': 34763,\n",
       " 'forebears': 40956,\n",
       " 'skirmish': 34764,\n",
       " 'ocassionally': 52343,\n",
       " \"'resist\": 52344,\n",
       " 'impactful': 21931,\n",
       " 'spicier': 52345,\n",
       " 'touristy': 40957,\n",
       " \"'football'\": 52346,\n",
       " 'webpage': 40958,\n",
       " 'exurbia': 52348,\n",
       " 'jucier': 52349,\n",
       " 'professors': 14904,\n",
       " 'structuring': 34765,\n",
       " 'jig': 30611,\n",
       " 'overlord': 40959,\n",
       " 'disconnect': 25264,\n",
       " 'sniffle': 82204,\n",
       " 'slimeball': 40960,\n",
       " 'jia': 40961,\n",
       " 'milked': 16831,\n",
       " 'banjoes': 40962,\n",
       " 'jim': 1240,\n",
       " 'workforces': 52351,\n",
       " 'jip': 52352,\n",
       " 'rotweiller': 52353,\n",
       " 'mundaneness': 34766,\n",
       " \"'ninja'\": 52354,\n",
       " \"dead'\": 11043,\n",
       " \"cipriani's\": 40963,\n",
       " 'modestly': 20611,\n",
       " \"professor'\": 52355,\n",
       " 'shacked': 40964,\n",
       " 'bashful': 34767,\n",
       " 'sorter': 23391,\n",
       " 'overpowering': 16123,\n",
       " 'workmanlike': 18524,\n",
       " 'henpecked': 27665,\n",
       " 'sorted': 18525,\n",
       " \"jb's\": 52357,\n",
       " \"'always\": 52358,\n",
       " \"'baptists\": 34768,\n",
       " 'dreamcatchers': 52359,\n",
       " \"'silence'\": 52360,\n",
       " 'hickory': 21932,\n",
       " 'fun\\x97yet': 52361,\n",
       " 'breakumentary': 52362,\n",
       " 'didn': 15499,\n",
       " 'didi': 52363,\n",
       " 'pealing': 52364,\n",
       " 'dispite': 40965,\n",
       " \"italy's\": 25265,\n",
       " 'instability': 21933,\n",
       " 'quarter': 6542,\n",
       " 'quartet': 12611,\n",
       " 'padm': 52365,\n",
       " \"'bleedmedry\": 52366,\n",
       " 'pahalniuk': 52367,\n",
       " 'honduras': 52368,\n",
       " 'bursting': 10789,\n",
       " \"pablo's\": 41468,\n",
       " 'irremediably': 52370,\n",
       " 'presages': 40966,\n",
       " 'bowlegged': 57835,\n",
       " 'dalip': 65186,\n",
       " 'entering': 6263,\n",
       " 'newsradio': 76175,\n",
       " 'presaged': 54153,\n",
       " \"giallo's\": 27666,\n",
       " 'bouyant': 40967,\n",
       " 'amerterish': 52371,\n",
       " 'rajni': 18526,\n",
       " 'leeves': 30613,\n",
       " 'macauley': 34770,\n",
       " 'seriously': 615,\n",
       " 'sugercoma': 52372,\n",
       " 'grimstead': 52373,\n",
       " \"'fairy'\": 52374,\n",
       " 'zenda': 30614,\n",
       " \"'twins'\": 52375,\n",
       " 'realisation': 17643,\n",
       " 'highsmith': 27667,\n",
       " 'raunchy': 7820,\n",
       " 'incentives': 40968,\n",
       " 'flatson': 52377,\n",
       " 'snooker': 35100,\n",
       " 'crazies': 16832,\n",
       " 'crazier': 14905,\n",
       " 'grandma': 7097,\n",
       " 'napunsaktha': 52378,\n",
       " 'workmanship': 30615,\n",
       " 'reisner': 52379,\n",
       " \"sanford's\": 61309,\n",
       " '\\x91doa': 52380,\n",
       " 'modest': 6111,\n",
       " \"everything's\": 19156,\n",
       " 'hamer': 40969,\n",
       " \"couldn't'\": 52382,\n",
       " 'quibble': 13004,\n",
       " 'socking': 52383,\n",
       " 'tingler': 21934,\n",
       " 'gutman': 52384,\n",
       " 'lachlan': 40970,\n",
       " 'tableaus': 52385,\n",
       " 'headbanger': 52386,\n",
       " 'spoken': 2850,\n",
       " 'cerebrally': 34771,\n",
       " \"'road\": 23493,\n",
       " 'tableaux': 21935,\n",
       " \"proust's\": 40971,\n",
       " 'periodical': 40972,\n",
       " \"shoveller's\": 52388,\n",
       " 'tamara': 25266,\n",
       " 'affords': 17644,\n",
       " 'concert': 3252,\n",
       " \"yara's\": 87958,\n",
       " 'someome': 52389,\n",
       " 'lingering': 8427,\n",
       " \"abraham's\": 41514,\n",
       " 'beesley': 34772,\n",
       " 'cherbourg': 34773,\n",
       " 'kagan': 28627,\n",
       " 'snatch': 9100,\n",
       " \"miyazaki's\": 9263,\n",
       " 'absorbs': 25267,\n",
       " \"koltai's\": 40973,\n",
       " 'tingled': 64030,\n",
       " 'crossroads': 19514,\n",
       " 'rehab': 16124,\n",
       " 'falworth': 52392,\n",
       " 'sequals': 52393,\n",
       " ...}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52256"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['simpsonian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'film',\n",
       " 'was',\n",
       " 'just',\n",
       " 'brilliant',\n",
       " 'casting',\n",
       " 'location',\n",
       " 'scenery',\n",
       " 'story',\n",
       " 'direction',\n",
       " \"everyone's\",\n",
       " 'really',\n",
       " 'suited',\n",
       " 'the',\n",
       " 'part',\n",
       " 'they',\n",
       " 'played',\n",
       " 'and',\n",
       " 'you',\n",
       " 'could',\n",
       " 'just',\n",
       " 'imagine',\n",
       " 'being',\n",
       " 'there',\n",
       " 'robert',\n",
       " \"redford's\",\n",
       " 'is',\n",
       " 'an',\n",
       " 'amazing',\n",
       " 'actor',\n",
       " 'and',\n",
       " 'now',\n",
       " 'the',\n",
       " 'same',\n",
       " 'being',\n",
       " 'director',\n",
       " \"norman's\",\n",
       " 'father',\n",
       " 'came',\n",
       " 'from',\n",
       " 'the',\n",
       " 'same',\n",
       " 'scottish',\n",
       " 'island',\n",
       " 'as',\n",
       " 'myself',\n",
       " 'so',\n",
       " 'i',\n",
       " 'loved',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'there',\n",
       " 'was',\n",
       " 'a',\n",
       " 'real',\n",
       " 'connection',\n",
       " 'with',\n",
       " 'this',\n",
       " 'film',\n",
       " 'the',\n",
       " 'witty',\n",
       " 'remarks',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'film',\n",
       " 'were',\n",
       " 'great',\n",
       " 'it',\n",
       " 'was',\n",
       " 'just',\n",
       " 'brilliant',\n",
       " 'so',\n",
       " 'much',\n",
       " 'that',\n",
       " 'i',\n",
       " 'bought',\n",
       " 'the',\n",
       " 'film',\n",
       " 'as',\n",
       " 'soon',\n",
       " 'as',\n",
       " 'it',\n",
       " 'was',\n",
       " 'released',\n",
       " 'for',\n",
       " 'retail',\n",
       " 'and',\n",
       " 'would',\n",
       " 'recommend',\n",
       " 'it',\n",
       " 'to',\n",
       " 'everyone',\n",
       " 'to',\n",
       " 'watch',\n",
       " 'and',\n",
       " 'the',\n",
       " 'fly',\n",
       " 'fishing',\n",
       " 'was',\n",
       " 'amazing',\n",
       " 'really',\n",
       " 'cried',\n",
       " 'at',\n",
       " 'the',\n",
       " 'end',\n",
       " 'it',\n",
       " 'was',\n",
       " 'so',\n",
       " 'sad',\n",
       " 'and',\n",
       " 'you',\n",
       " 'know',\n",
       " 'what',\n",
       " 'they',\n",
       " 'say',\n",
       " 'if',\n",
       " 'you',\n",
       " 'cry',\n",
       " 'at',\n",
       " 'a',\n",
       " 'film',\n",
       " 'it',\n",
       " 'must',\n",
       " 'have',\n",
       " 'been',\n",
       " 'good',\n",
       " 'and',\n",
       " 'this',\n",
       " 'definitely',\n",
       " 'was',\n",
       " 'also',\n",
       " 'congratulations',\n",
       " 'to',\n",
       " 'the',\n",
       " 'two',\n",
       " 'little',\n",
       " \"boy's\",\n",
       " 'that',\n",
       " 'played',\n",
       " 'the',\n",
       " \"part's\",\n",
       " 'of',\n",
       " 'norman',\n",
       " 'and',\n",
       " 'paul',\n",
       " 'they',\n",
       " 'were',\n",
       " 'just',\n",
       " 'brilliant',\n",
       " 'children',\n",
       " 'are',\n",
       " 'often',\n",
       " 'left',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'praising',\n",
       " 'list',\n",
       " 'i',\n",
       " 'think',\n",
       " 'because',\n",
       " 'the',\n",
       " 'stars',\n",
       " 'that',\n",
       " 'play',\n",
       " 'them',\n",
       " 'all',\n",
       " 'grown',\n",
       " 'up',\n",
       " 'are',\n",
       " 'such',\n",
       " 'a',\n",
       " 'big',\n",
       " 'profile',\n",
       " 'for',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'film',\n",
       " 'but',\n",
       " 'these',\n",
       " 'children',\n",
       " 'are',\n",
       " 'amazing',\n",
       " 'and',\n",
       " 'should',\n",
       " 'be',\n",
       " 'praised',\n",
       " 'for',\n",
       " 'what',\n",
       " 'they',\n",
       " 'have',\n",
       " 'done',\n",
       " \"don't\",\n",
       " 'you',\n",
       " 'think',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'story',\n",
       " 'was',\n",
       " 'so',\n",
       " 'lovely',\n",
       " 'because',\n",
       " 'it',\n",
       " 'was',\n",
       " 'true',\n",
       " 'and',\n",
       " 'was',\n",
       " \"someone's\",\n",
       " 'life',\n",
       " 'after',\n",
       " 'all',\n",
       " 'that',\n",
       " 'was',\n",
       " 'shared',\n",
       " 'with',\n",
       " 'us',\n",
       " 'all']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View an input sentence\n",
    "inv_imdb_word_index = {value: key for key, value in test.items()}\n",
    "[inv_imdb_word_index[index] for index in X_train[0] if index > index_from]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding and masking sequence data - Coding tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data with padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 300)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad the inputs to the maximum length using maxlen\n",
    "padded_X_train = pad_sequences(X_train, maxlen=300, padding='post', truncating='pre')\n",
    "padded_X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Masking layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([25000, 300, 1])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Masking expects to see (batch, sequence, features)\n",
    "# Create a dummy feature dimension using expand_dims\n",
    "\n",
    "padded_X_train = tf.expand_dims(padded_X_train, -1)\n",
    "padded_X_train = tf.cast(padded_X_train, 'float32')\n",
    "padded_X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can expand it with `np.expand_dims` then, convert it to tensor with `tf.convert_to_tensor()`\n",
    "\n",
    "```python\n",
    "padded_X_train = np.expand_dims(padded_X_train, -1)\n",
    "tf_X_train = tf.convert_to_tensor(padded_X_train, dtype='float32')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a masking layer\n",
    "masking_layer = Masking(mask_value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass padded_X_train to it\n",
    "masked_X_train = masking_layer(padded_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(25000, 300, 1), dtype=float32, numpy=\n",
       "array([[[1.000e+00],\n",
       "        [1.400e+01],\n",
       "        [2.200e+01],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.940e+02],\n",
       "        [1.153e+03],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.400e+01],\n",
       "        [4.700e+01],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.100e+01],\n",
       "        [6.000e+00],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.446e+03],\n",
       "        [7.079e+03],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.700e+01],\n",
       "        [6.000e+00],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]]], dtype=float32)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(25000, 300, 1), dtype=float32, numpy=\n",
       "array([[[1.000e+00],\n",
       "        [1.400e+01],\n",
       "        [2.200e+01],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.940e+02],\n",
       "        [1.153e+03],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.400e+01],\n",
       "        [4.700e+01],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.100e+01],\n",
       "        [6.000e+00],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.446e+03],\n",
       "        [7.079e+03],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.700e+01],\n",
       "        [6.000e+00],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]]], dtype=float32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object has no attribute '_keras_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-b0f845e22e46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpadded_X_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keras_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute '_keras_mask'"
     ]
    }
   ],
   "source": [
    "padded_X_train._keras_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(25000, 300), dtype=bool, numpy=\n",
       "array([[ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       ...,\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False]])>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_X_train._keras_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Text Data\n",
    "\n",
    "In this section, you will learn how to tokenise text data using `tf.keras.preprocessing.text.Tokenizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The text dataset\n",
    "\n",
    "The text we will work with in this notebook is Three Men in a Boat by Jerome K. Jerome, a comical short story about the perils of going outside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "with open('./dataset/ThreeMenInABoat.txt', 'r', encoding='utf-8') as f:\n",
    "    text_string = f.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform some simple preprocessing, replacing dashes with empty spaces\n",
    "text_string = text_string.replace('', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHAPTER I.   Three invalids.Sufferings of George and Harris.A victim to one hundred and seven fatal maladies.Useful prescriptions.Cure for liver complaint in children.We agree that we are overworked, and need rest.A week on the rolling deep?George suggests the River.Montmorency lodges an objection.Original motion carried by majority of three to one.  There were four of usGeorge, and William Samuel Harris, and myself, and Montmorency.  We were sitting in my room, smoking, and talking about how bad we werebad from a medical point of view I mean, of course.  We were all feeling seedy, and we were getting quite nervous about it. Harris said he felt such extraordinary fits of giddiness come over him at times, that he hardly knew what he was doing; and then George said that _he_ had fits of giddiness too, and hardly knew what _he_ was doing. With me, it was my liver that was out of order.  I knew it was my liver that was out of order, because I had just been reading a patent liver-pill circular, in which were detailed the various symptoms by which a man could tell when his liver was out of order.  I had them all.  It is a most extraordinary thing, but I never read a patent medicine advertisement without being impelled to the conclusion that I am suffering from the particular disease therein dealt with in its most virulent form.  The diagnosis seems in every case to correspond exactly with all the sensations that I have ever felt.  [Picture: Man reading book] I remember going to the British Museum one day to read up the treatment for some slight ailment of which I had a touchhay fever, I fancy it was.  I got down the book, and read all I came to read; and then, in an unthinking moment, I idly turned the leaves, and began to indolently study diseases, generally.  I forget which was the first distemper I plunged intosome fearful, devastating scourge, I knowand, before I had glanced half down the list of premonitory symptoms, it was borne in upon me that I had fairly got it.'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_string[:2001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  I got down the book, and read all I came to read; and then, in an unthinking moment, I idly turned the leaves, and began to indolently study diseases, generally',\n",
       " '  I forget which was the first distemper I plunged intosome fearful, devastating scourge, I knowand, before I had glanced half down the list of premonitory symptoms, it was borne in upon me that I had fairly got it',\n",
       " '  I sat for awhile, frozen with horror; and then, in the listlessness of despair, I again turned over the pages',\n",
       " '  I came to typhoid feverread the symptomsdiscovered that I had typhoid fever, must have had it for months without knowing itwondered what else I had got; turned up St',\n",
       " ' Vituss Dancefound, as I expected, that I had that too,began to get interested in my case, and determined to sift it to the bottom, and so started alphabeticallyread up ague, and learnt that I was sickening for it, and that the acute stage would commence in about another fortnight',\n",
       " '  Brights disease, I was relieved to find, I had only in a modified form, and, so far as that was concerned, I might live for years',\n",
       " '  Cholera I had, with severe complications; and diphtheria I seemed to have been born with',\n",
       " '  I plodded conscientiously through the twenty-six letters, and the only malady I could conclude I had not got was housemaids knee',\n",
       " '  I felt rather hurt about this at first; it seemed somehow to be a sort of slight',\n",
       " '  Why hadnt I got housemaids knee?  Why this invidious reservation?  After a while, however, less grasping feelings prevailed']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the text into sentences\n",
    "sentence_strings = text_string.split('.')\n",
    "sentence_strings[20:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Tokenizer object\n",
    "\n",
    "The `Tokenizer` object allows you to easily tokenise words or characters from a text document. It has several options to allow you to adjust the tokenisation process. Documentation is available for the `Tokenizer` [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define any additional characters that we want to filter out (ignore) from the text\n",
    "additional_filters = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tokenizer has a `filters` keyword argument, that determines which characters will be filtered out from the text. The cell below shows the default characters that are filtered, to which we are adding our additional filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=None,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n' + additional_filters,\n",
    "    lower=True,\n",
    "    split=' ',\n",
    "    char_level=False,\n",
    "    oov_token='<UNK>',\n",
    "    document_count=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all, the `Tokenizer` has the following keyword arguments:\n",
    "\n",
    "`num_words`: int. the maximum number of words to keep, based on word frequency. Only the most common `num_words-1` words will be kept. If set to `None`, all words are kept.\n",
    "    \n",
    "`filters`: str. Each element is a character that will be filtered from the texts. Defaults to all punctuation (inc. tabs and line breaks), except `'`.\n",
    "\n",
    "`lower`: bool. Whether to convert the texts to lowercase. Defaults to `True`.\n",
    "\n",
    "`split`: str. Separator for word splitting. Defaults to `' '`.\n",
    "    \n",
    "`char_level`: bool. if True, every character will be treated as a token. Defaults to `False`.\n",
    "\n",
    "`oov_token`: if given, it will be added to word_index and used to replace out-of-vocabulary words during sequence_to_text calls. Defaults to `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Tokenizer to the text\n",
    "\n",
    "We can now tokenize our text using the `fit_on_texts` method. This method takes a list of strings to tokenize, as we have prepared with `sentence_strings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Tokenizer vocabulary\n",
    "tokenizer.fit_on_texts(sentence_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit_on_texts` method could also take a list of lists of strings, and in this case it would recognise each element of each sublist as an individual token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Tokenizer configuration\n",
    "\n",
    "Now that the Tokenizer has ingested the data, we can see what it has extracted from the text by viewing its configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['num_words', 'filters', 'lower', 'split', 'char_level', 'oov_token', 'document_count', 'word_counts', 'word_docs', 'index_docs', 'index_word', 'word_index'])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the tokenizer config as a python dict\n",
    "tokenizer_config = tokenizer.get_config()\n",
    "tokenizer_config.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"chapter\": 19, \"i\": 1195, \"three\": 79, \"invalids\": 1, \"sufferings\": 2, \"of\": 1487, \"george\": 306, \"and\": 3375, \"harris\": 314, \"a\": 1696, \"victim\": 3, \"to\": 1785, \"one\": 241, \"hundred\": 19, \"seven\": 15, \"fatal\": 1, \"maladies\": 2, \"useful\": 2, \"prescriptions\": 1, \"cure\": 1, \"for\": 525, \"liver\": 8, \"complaint\": 2, \"in\": 976, \"children\": 13, \"we\": 866, \"agree\": 2, \"that\": 944, \"are\": 181, \"overworked\": 1, \"need\": 7, \"rest\": 14, \"week\": 19, \"on\": 501, \"the\": 3603, \"rolling\": 1, \"deep\": 18, \"suggests'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_config['word_counts'][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is the number of times each word appears in the corpus. As you can see, the word counts dictionaries in the config are serialized into plain JSON. The `loads()` method in the Python library `json` can be used to convert this JSON string into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "word_counts = json.loads(tokenizer_config['word_counts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word index is derived from the `word_counts`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"<UNK>\": 1, \"the\": 2, \"and\": 3, \"to\": 4, \"a\": 5, \"of\": 6, \"it\": 7, \"i\": 8, \"in\": 9, \"that\": 10, \"he\": 11, \"we\": 12, \"was\": 13, \"you\": 14, \"had\": 15, \"for\": 16, \"at\": 17, \"on\": 18, \"with\": 19, \"up\": 20, \"they\": 21, \"is\": 22, \"as\": 23, \"not\": 24, \"his\": 25, \"said\": 26, \"but\": 27, \"would\": 28, \"all\": 29, \"s\": 30, \"have\": 31, \"him\": 32, \"there\": 33, \"be\": 34, \"harris\": 35, \"george\": 36, \"out\": 37, \"t\": 38, \"so\": 39, \"then\": 40, \"when\": 41, \"them\": 42, \"one\": 43, \"were\": 44, \"about\": 45, \"us\": 46, \"'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_config['word_index'][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word = json.loads(tokenizer_config['index_word'])\n",
    "word_index = json.loads(tokenizer_config['word_index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the sentences to tokens\n",
    "\n",
    "You can map each sentence to a sequence of integer tokens using the Tokenizer's `texts_to_sequences()` method. As was the case for the IMDb data set, the number corresponding to a word is that word's frequency rank in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHAPTER I',\n",
       " '   Three invalids',\n",
       " 'Sufferings of George and Harris',\n",
       " 'A victim to one hundred and seven fatal maladies',\n",
       " 'Useful prescriptions']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_strings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the data\n",
    "sentence_seq = tokenizer.texts_to_sequences(sentence_strings)\n",
    "type(sentence_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[362, 8],\n",
       " [126, 3362],\n",
       " [2319, 6, 36, 3, 35],\n",
       " [5, 1779, 4, 43, 363, 3, 468, 3363, 2320],\n",
       " [2321, 3364]]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_seq[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362 8\n",
      "126 3362\n",
      "2319 6 36 3 35\n",
      "5 1779 4 43 363 3 468 3363 2320\n",
      "2321 3364\n"
     ]
    }
   ],
   "source": [
    "print(word_index['chapter'], word_index['i'])\n",
    "print(word_index['three'], word_index['invalids'])\n",
    "print(word_index['sufferings'], word_index['of'], word_index['george'], word_index['and'], word_index['harris'])\n",
    "print(word_index['a'], word_index['victim'], word_index['to'], word_index['one'], word_index['hundred'], word_index['and'], word_index['seven'], word_index['fatal'], word_index['maladies'])\n",
    "print(word_index['useful'], word_index['prescriptions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the tokens to sentences\n",
    "\n",
    "You can map the tokens back to sentences using the Tokenizer's `sequences_to_texts` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[362, 8],\n",
       " [126, 3362],\n",
       " [2319, 6, 36, 3, 35],\n",
       " [5, 1779, 4, 43, 363, 3, 468, 3363, 2320],\n",
       " [2321, 3364]]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_seq[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chapter i',\n",
       " 'three invalids',\n",
       " 'sufferings of george and harris',\n",
       " 'a victim to one hundred and seven fatal maladies',\n",
       " 'useful prescriptions']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map the token sequences back to sentences\n",
    "tokenizer.sequences_to_texts(sentence_seq)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chapter i\n",
      "three invalids\n",
      "sufferings of george and harris\n",
      "a victim to one hundred and seven fatal maladies\n",
      "useful prescriptions\n"
     ]
    }
   ],
   "source": [
    "# Verify the mappings in the config\n",
    "print(index_word['362'], index_word['8'])\n",
    "print(index_word['126'], index_word['3362'])\n",
    "print(index_word['2319'], index_word['6'], index_word['36'], index_word['3'], index_word['35'])\n",
    "print(index_word['5'], index_word['1779'], index_word['4'], index_word['43'], index_word['363'], index_word['3'], index_word['468'], index_word['3363'], index_word['2320'])\n",
    "print(index_word['2321'], index_word['3364'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good day world', 'montmorency bit my finger']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Any valida sequence of tokens can be converted to text\n",
    "tokenizer.sequences_to_texts([[92, 104, 241], [152, 169, 53, 2491]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a word is not featured in the Tokenizer's word index, then it will be mapped to the value of the Tokenizer's `oov_token` property. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8, 28, 78, 1, 1]]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize unrecognized words\n",
    "tokenizer.texts_to_sequences(['i would like goobleydoobly hobbledyho'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<UNK>'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the OOV token\n",
    "index_word['1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further reading and resources\n",
    "\n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "* https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(input_dim=1000, output_dim=32, input_length=64, mask_zero=True)\n",
    "test_input = np.random.randint(1000, size=(16, 64))\n",
    "\n",
    "embedded_inputs = embedding_layer(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([16, 64, 32])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (batch_size, sequence, embedding_dim)\n",
    "embedded_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16, 64, 32), dtype=float32, numpy=\n",
       "array([[[ 9.65585560e-03,  2.58246697e-02,  2.83139236e-02, ...,\n",
       "         -1.97027810e-02,  3.73625793e-02,  4.66074981e-02],\n",
       "        [-1.10767260e-02,  3.09304632e-02,  4.11251448e-02, ...,\n",
       "          1.56247951e-02, -4.29970883e-02,  5.83509356e-03],\n",
       "        [ 4.95434292e-02, -2.65830886e-02, -4.69835997e-02, ...,\n",
       "         -5.20426035e-03,  7.71843269e-03, -1.17018335e-02],\n",
       "        ...,\n",
       "        [ 2.30224021e-02, -4.08567786e-02, -4.64139953e-02, ...,\n",
       "         -4.46658731e-02,  3.18944491e-02, -1.09856352e-02],\n",
       "        [ 6.47538900e-03, -3.03988699e-02,  1.80933625e-03, ...,\n",
       "          4.66918387e-02, -4.85729575e-02,  1.82619952e-02],\n",
       "        [-3.77286784e-02, -1.58771984e-02, -4.74475622e-02, ...,\n",
       "         -1.06220357e-02,  3.23696993e-02, -4.96266745e-02]],\n",
       "\n",
       "       [[ 4.84966673e-02,  1.99735165e-04, -9.31495428e-03, ...,\n",
       "         -2.26535555e-02, -3.86798605e-02, -2.29130033e-02],\n",
       "        [-4.50625792e-02, -5.56951761e-03,  1.37720741e-02, ...,\n",
       "         -4.97301333e-02,  2.52547525e-02,  4.94591333e-02],\n",
       "        [-3.38920951e-02, -2.44071726e-02, -4.92677838e-03, ...,\n",
       "         -4.40650843e-02,  3.64709608e-02, -2.03740597e-02],\n",
       "        ...,\n",
       "        [ 3.00076641e-02,  2.16364376e-02,  3.36899795e-02, ...,\n",
       "         -3.38721499e-02, -1.00518391e-03,  6.09902292e-03],\n",
       "        [-1.34600885e-02, -3.85011546e-02,  4.55588102e-03, ...,\n",
       "         -6.02508709e-03, -5.71495295e-03,  2.39129700e-02],\n",
       "        [-2.23586094e-02,  3.39074992e-02,  2.95993201e-02, ...,\n",
       "          1.36086978e-02,  1.98125727e-02,  8.97309929e-03]],\n",
       "\n",
       "       [[ 2.34069563e-02, -1.95923932e-02,  2.04459764e-02, ...,\n",
       "          8.97004455e-03, -3.60324234e-03,  2.93575190e-02],\n",
       "        [ 9.28410143e-03, -2.51861569e-02,  4.25336510e-03, ...,\n",
       "          4.09638174e-02,  4.13139910e-03,  2.51660459e-02],\n",
       "        [ 4.47252132e-02, -1.44106746e-02, -3.25301290e-02, ...,\n",
       "         -1.99888106e-02, -1.49936788e-02,  3.87714170e-02],\n",
       "        ...,\n",
       "        [ 4.99983765e-02,  1.92985050e-02, -1.08853579e-02, ...,\n",
       "          1.50277056e-02, -8.56954977e-03,  4.24336530e-02],\n",
       "        [ 7.62562826e-03, -4.45678830e-02, -1.32351629e-02, ...,\n",
       "         -4.99796122e-04, -4.43366431e-02,  4.48611863e-02],\n",
       "        [-7.82185793e-03, -4.21125814e-03, -2.90266518e-02, ...,\n",
       "          3.03258337e-02, -3.70537117e-03,  1.00080967e-02]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 3.82496826e-02,  2.40025409e-02,  1.61179341e-02, ...,\n",
       "         -4.86022495e-02, -3.24340463e-02,  4.18952964e-02],\n",
       "        [ 4.47252132e-02, -1.44106746e-02, -3.25301290e-02, ...,\n",
       "         -1.99888106e-02, -1.49936788e-02,  3.87714170e-02],\n",
       "        [ 1.06047466e-03, -2.83290744e-02, -1.53122917e-02, ...,\n",
       "          2.32178085e-02, -8.77951458e-03,  4.25253995e-02],\n",
       "        ...,\n",
       "        [ 4.84966673e-02,  1.99735165e-04, -9.31495428e-03, ...,\n",
       "         -2.26535555e-02, -3.86798605e-02, -2.29130033e-02],\n",
       "        [ 3.72472443e-02,  5.87599352e-03,  8.04413110e-04, ...,\n",
       "          4.15475033e-02, -4.66573015e-02, -4.10916656e-03],\n",
       "        [-8.96393135e-03,  3.86121310e-02, -8.28116015e-03, ...,\n",
       "          1.11897700e-02,  2.67444961e-02, -3.78526822e-02]],\n",
       "\n",
       "       [[ 4.06826995e-02,  3.84760983e-02, -1.88190583e-02, ...,\n",
       "         -4.72120754e-02, -5.03792614e-03,  4.37209643e-02],\n",
       "        [-4.75450754e-02, -5.53538650e-03,  2.43393891e-02, ...,\n",
       "          2.11811997e-02, -3.30437422e-02,  3.92180197e-02],\n",
       "        [-4.50625792e-02, -5.56951761e-03,  1.37720741e-02, ...,\n",
       "         -4.97301333e-02,  2.52547525e-02,  4.94591333e-02],\n",
       "        ...,\n",
       "        [ 1.65123679e-02,  1.34287737e-02, -2.14433204e-02, ...,\n",
       "         -3.71073969e-02,  2.00436227e-02,  4.18504588e-02],\n",
       "        [-7.54155964e-03, -2.00370699e-03, -1.52362362e-02, ...,\n",
       "         -2.73358226e-02, -1.48641355e-02,  4.95367907e-02],\n",
       "        [-1.00798011e-02, -1.30214542e-03, -2.72103902e-02, ...,\n",
       "         -1.69873238e-03, -1.52843073e-03, -4.71187346e-02]],\n",
       "\n",
       "       [[-1.49505958e-02, -1.10759363e-02,  1.10448003e-02, ...,\n",
       "         -1.49117485e-02,  1.85119770e-02,  5.35247475e-03],\n",
       "        [-2.40031965e-02,  2.61383764e-02, -4.21015508e-02, ...,\n",
       "          6.75976276e-04, -4.38017733e-02, -3.23214084e-02],\n",
       "        [-3.37954164e-02,  3.65138985e-02, -3.96030173e-02, ...,\n",
       "          4.27058004e-02, -3.11015919e-03, -3.58636975e-02],\n",
       "        ...,\n",
       "        [ 2.81608105e-03, -3.54090929e-02, -4.18279879e-02, ...,\n",
       "          2.23323740e-02,  4.36508097e-02, -3.00712716e-02],\n",
       "        [ 4.39255573e-02,  3.45351584e-02,  2.39293687e-02, ...,\n",
       "         -2.00597774e-02,  4.96455170e-02,  4.26299013e-02],\n",
       "        [ 3.97252180e-02, -8.09490681e-03,  1.56020559e-02, ...,\n",
       "          2.80411877e-02,  4.08045985e-02, -5.97462058e-05]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16, 64), dtype=bool, numpy=\n",
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]])>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_inputs._keras_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and apply an Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding layer using layers.Embedding\n",
    "# Specify input_dim, output_dim, input_length\n",
    "embedding_layer = Embedding(input_dim=501, output_dim=16, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4, 1, 16), dtype=float32, numpy=\n",
       "array([[[[ 0.01727618,  0.03089762,  0.002029  ,  0.02660182,\n",
       "           0.03315575, -0.0158009 , -0.04145148, -0.01046788,\n",
       "          -0.00868522, -0.01087832,  0.03910741,  0.01844318,\n",
       "           0.00452308, -0.00712603, -0.04908859, -0.03017252]],\n",
       "\n",
       "        [[-0.01455295, -0.01239837,  0.04571614, -0.01083236,\n",
       "           0.0072578 , -0.00458877,  0.03554687, -0.01132578,\n",
       "          -0.03172002, -0.04165415,  0.04840617,  0.03732756,\n",
       "          -0.03718423, -0.02182792,  0.02609112,  0.03805694]],\n",
       "\n",
       "        [[-0.00051304,  0.04867173, -0.00368887,  0.04377763,\n",
       "          -0.01178855, -0.01280548, -0.00160016, -0.04611064,\n",
       "           0.00509077, -0.02930354, -0.03712142, -0.02733033,\n",
       "           0.00229036, -0.00932483, -0.02904466,  0.02423595]],\n",
       "\n",
       "        [[ 0.02334765,  0.0198273 , -0.01104606, -0.03421359,\n",
       "          -0.01847423,  0.01022577, -0.03737479, -0.03536389,\n",
       "           0.03680542,  0.0358013 , -0.01858652, -0.0453972 ,\n",
       "           0.00681265, -0.04028156, -0.00017995,  0.04682523]]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect an Embedding layer output for a fixed input\n",
    "sequence_of_indices = tf.constant([[[0], [1], [5], [500]]])\n",
    "sequence_of_embeddings = embedding_layer(sequence_of_indices)\n",
    "sequence_of_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.01727618,  0.03089762,  0.002029  , ..., -0.00712603,\n",
       "         -0.04908859, -0.03017252],\n",
       "        [-0.01455295, -0.01239837,  0.04571614, ..., -0.02182792,\n",
       "          0.02609112,  0.03805694],\n",
       "        [-0.00477733,  0.01999179,  0.00787187, ..., -0.04085811,\n",
       "          0.00575401, -0.04952283],\n",
       "        ...,\n",
       "        [-0.01691771,  0.011001  , -0.02034165, ..., -0.02855944,\n",
       "          0.04151667,  0.01520164],\n",
       "        [ 0.02687527,  0.01937275, -0.01556892, ..., -0.03390159,\n",
       "          0.03692856,  0.02204534],\n",
       "        [ 0.02334765,  0.0198273 , -0.01104606, ..., -0.04028156,\n",
       "         -0.00017995,  0.04682523]], dtype=float32)]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0342101 ,  0.01310667,  0.0132176 ,  0.02718227,  0.04495814,\n",
       "        0.01778303,  0.01667256,  0.04202265, -0.03798028,  0.03308897,\n",
       "        0.01891697, -0.03616475,  0.02652378, -0.03046833, -0.03324846,\n",
       "       -0.03275827], dtype=float32)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the embedding for the 14th index\n",
    "embedding_layer.get_weights()[0][14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a layer that uses the mask_zero kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "masking_embdding_layer = Embedding(input_dim=501, output_dim=16, mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4, 1), dtype=bool, numpy=\n",
       "array([[[False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True]]])>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply this layer to the sequence and see the _keras_mask property\n",
    "masked_sequence_of_embeddings = masking_embdding_layer(sequence_of_indices)\n",
    "masked_sequence_of_embeddings._keras_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Embedding Projector - Coding Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess the IMDb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to load and preprocess the IMDB dataset\n",
    "\n",
    "def get_and_pad_imdb_dataset(num_words=10000, maxlen=None, index_from=2):\n",
    "    from tensorflow.keras.datasets import imdb\n",
    "\n",
    "    # Load the reviews\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(path='imdb.npz',\n",
    "                                                          num_words=num_words,\n",
    "                                                          skip_top=0,\n",
    "                                                          maxlen=maxlen,\n",
    "                                                          start_char=1,\n",
    "                                                          oov_char=2,\n",
    "                                                          index_from=index_from)\n",
    "\n",
    "    x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        maxlen=None,\n",
    "                                                        padding='pre',\n",
    "                                                        truncating='pre',\n",
    "                                                        value=0)\n",
    "    \n",
    "    x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                           maxlen=None,\n",
    "                                                           padding='pre',\n",
    "                                                           truncating='pre',\n",
    "                                                           value=0)\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = get_and_pad_imdb_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to get the dataset word index\n",
    "def get_imdb_word_index(num_words=10000, index_from=2):\n",
    "    imdb_word_index = tf.keras.datasets.imdb.get_word_index(\n",
    "                                        path='imdb_word_index.json')\n",
    "    imdb_word_index = {key: value + index_from for\n",
    "                       key, value in imdb_word_index.items() if value <= num_words-index_from}\n",
    "    return imdb_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word index\n",
    "imdb_word_index = get_imdb_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swap the keys and values of the word index\n",
    "inv_imdb_word_index = {value: key for key, value in imdb_word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'am',\n",
       " 'a',\n",
       " 'great',\n",
       " 'fan',\n",
       " 'of',\n",
       " 'david',\n",
       " 'lynch',\n",
       " 'and',\n",
       " 'have',\n",
       " 'everything',\n",
       " 'that',\n",
       " \"he's\",\n",
       " 'made',\n",
       " 'on',\n",
       " 'dvd',\n",
       " 'except',\n",
       " 'for',\n",
       " 'hotel',\n",
       " 'room',\n",
       " 'the',\n",
       " '2',\n",
       " 'hour',\n",
       " 'twin',\n",
       " 'peaks',\n",
       " 'movie',\n",
       " 'so',\n",
       " 'when',\n",
       " 'i',\n",
       " 'found',\n",
       " 'out',\n",
       " 'about',\n",
       " 'this',\n",
       " 'i',\n",
       " 'immediately',\n",
       " 'grabbed',\n",
       " 'it',\n",
       " 'and',\n",
       " 'and',\n",
       " 'what',\n",
       " 'is',\n",
       " 'this',\n",
       " \"it's\",\n",
       " 'a',\n",
       " 'bunch',\n",
       " 'of',\n",
       " 'drawn',\n",
       " 'black',\n",
       " 'and',\n",
       " 'white',\n",
       " 'cartoons',\n",
       " 'that',\n",
       " 'are',\n",
       " 'loud',\n",
       " 'and',\n",
       " 'foul',\n",
       " 'mouthed',\n",
       " 'and',\n",
       " 'unfunny',\n",
       " 'maybe',\n",
       " 'i',\n",
       " \"don't\",\n",
       " 'know',\n",
       " \"what's\",\n",
       " 'good',\n",
       " 'but',\n",
       " 'maybe',\n",
       " 'this',\n",
       " 'is',\n",
       " 'just',\n",
       " 'a',\n",
       " 'bunch',\n",
       " 'of',\n",
       " 'crap',\n",
       " 'that',\n",
       " 'was',\n",
       " 'on',\n",
       " 'the',\n",
       " 'public',\n",
       " 'under',\n",
       " 'the',\n",
       " 'name',\n",
       " 'of',\n",
       " 'david',\n",
       " 'lynch',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'few',\n",
       " 'bucks',\n",
       " 'too',\n",
       " 'let',\n",
       " 'me',\n",
       " 'make',\n",
       " 'it',\n",
       " 'clear',\n",
       " 'that',\n",
       " 'i',\n",
       " \"didn't\",\n",
       " 'care',\n",
       " 'about',\n",
       " 'the',\n",
       " 'foul',\n",
       " 'language',\n",
       " 'part',\n",
       " 'but',\n",
       " 'had',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'the',\n",
       " 'sound',\n",
       " 'because',\n",
       " 'my',\n",
       " 'neighbors',\n",
       " 'might',\n",
       " 'have',\n",
       " 'all',\n",
       " 'in',\n",
       " 'all',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'highly',\n",
       " 'disappointing',\n",
       " 'release',\n",
       " 'and',\n",
       " 'may',\n",
       " 'well',\n",
       " 'have',\n",
       " 'just',\n",
       " 'been',\n",
       " 'left',\n",
       " 'in',\n",
       " 'the',\n",
       " 'box',\n",
       " 'set',\n",
       " 'as',\n",
       " 'a',\n",
       " 'curiosity',\n",
       " 'i',\n",
       " 'highly',\n",
       " 'recommend',\n",
       " 'you',\n",
       " \"don't\",\n",
       " 'spend',\n",
       " 'your',\n",
       " 'money',\n",
       " 'on',\n",
       " 'this',\n",
       " '2',\n",
       " 'out',\n",
       " 'of',\n",
       " '10']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inv_imdb_word_index[index] for index in X_train[100] if index > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an Embedding layer into a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum index value\n",
    "max_index_value = max(imdb_word_index.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify an embedding dimension\n",
    "embedding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, Dense\n",
    "\n",
    "# Build a model using sequential\n",
    "# 1. Embedding layer\n",
    "# 2. GlobalAveragePooling1D\n",
    "# 3. Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_index_value+1, output_dim=embedding_dim, mask_zero=False),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, None, 16)          160016    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Functional API refresher.\n",
    "\n",
    "inputs = Input((None, ))\n",
    "embedding_sequence = Embedding(input_dim=max_index_value+1, output_dim=embedding_dim, mask_zero=False)(inputs)\n",
    "average_embedding = GlobalAveragePooling1D()(embedding_sequence)\n",
    "positive_probability = Dense(1, activation='sigmoid')(average_embedding)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=positive_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_13 (Embedding)     (None, None, 16)          160016    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_3 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile, train, and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 5s 7ms/step - loss: 0.6903 - accuracy: 0.5447 - val_loss: 0.6843 - val_accuracy: 0.6922\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.6717 - accuracy: 0.6784 - val_loss: 0.6539 - val_accuracy: 0.6969\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.6287 - accuracy: 0.7612 - val_loss: 0.6029 - val_accuracy: 0.7625\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.5765 - accuracy: 0.7901 - val_loss: 0.5518 - val_accuracy: 0.7812\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.5270 - accuracy: 0.8179 - val_loss: 0.5079 - val_accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test), validation_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7AAAAJcCAYAAADATEiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdeZicZ3nn++9Tve+LdrXUkmXJluRNLRuDAWOzOBgDtggJBAcCBGMSQg7MmcMQOMxACCfJnFyTHDLDIYfDJJAMJGEmR/ICYUvgYBYDtrq9Ce+2ltYudbd6X6qe+eOt7q5eVZZV3Srp+7muvqrrfd+uurukEP90P8/9hhgjkiRJkiSd61KLXYAkSZIkSfkwwEqSJEmSioIBVpIkSZJUFAywkiRJkqSiYICVJEmSJBUFA6wkSZIkqSgYYCVJCyKEUBJC6AshtJ7NaxdTCGFjCOGs348uhPC6EMLzOc+fCCFcn8+1Z/BeXwohfOJMf16SpIVUutgFSJLOTSGEvpyn1cAwkM4+/0CM8asv5PVijGmg9mxfeyGIMV56Nl4nhHAH8M4Y4405r33H2XhtSZIWggFWkjSrGONEgMx2+O6IMX5vrutDCKUxxrGFqE06Hf8+StL5ySXEkqQzEkL4bAjhH0MIfx9C6AXeGUK4LoRwfwihO4RwKITwlyGEsuz1pSGEGEJYn33+37Ln/zmE0BtC+GkI4aIXem32/BtCCE+GEHpCCP85hPDjEMJ75qg7nxo/EEJ4OoTQFUL4y5yfLQkh/EUI4UQI4Rng5nk+n0+GEP5h2rHPhxD+PPv9HSGEX2Z/n2ey3dG5XutACOHG7PfVIYS/y9b2GHD1LO/7bPZ1Hwsh3Jo9fgXwX4Drs8uzj+d8tp/O+fnfyf7uJ0IIu0IIq/L5bF7I5zxeTwjheyGEkyGEwyGEf5fzPv8++5mcCiE8EEJYPdty7RDCj8b/nLOf5w+z73MS+GQIYVMI4fvZ3+V49nNryPn5ddnf8Vj2/OdCCJXZmrfkXLcqhDAQQlgy1+8rSVoYBlhJ0ovxFuBrQAPwj8AY8GFgKfAKkoD3gXl+/nbg3wPNwD7gj17otSGE5cDXgY9m3/c54Np5XiefGm8hCYZtJMH8ddnjvwv8CnBV9j3eNs/7fA14UwihJltnKfDr2eMAR4A3AvXA+4H/HEK4cp7XG/cZYC2wIVvnu6edfzL7ezUA/wfwtRDCihjjI8CHgPtijLUxxqXTXziE8CvZ1/81oAU4CExfKj7XZzPdnJ9zNkR+D7gHWAVcAvwg+3Mfzb7/zUAjcAcwNN8HkuPlwC+BZcB/BALw2ex7bCX5zP59toZS4BvA08B6ks/06zHGIZK/T+/Med3bgW/HGE/kWYckqUAMsJKkF+NHMcZ7YoyZGONgjPEXMcafxRjHYozPAl8Ebpjn5/9HjPGBGOMoSVDadgbXvgnoiDHelT33F8DxuV4kzxr/JMbYE2N8niRYjb/X24C/iDEeyIaZP53nfZ4FHgVuyx66CeiOMT6QPX9PjPHZmPhX4F+AWQc1TfM24LMxxq4Y416Srmru+349xngo+2fyNeB54Jo8XhfgN4EvxRg7skHuD4AbQghrcq6Z67OZ4jSf863A/hjj52KMwzHGUzHGn2fP3QF8Isb4VPZ36Igxnsyz/n0xxi/EGNPZv49Pxhj/JcY4EmM8SvJ3Y7yG60jC9cdijP3Z63+cPfcV4PYQQsg+fxfwd3nWIEkqIAOsJOnF2J/7JISwOYTwjeyS0FMk3bwZnb4ch3O+H2D+wU1zXbs6t44YYwQOzPUiedaY13sBe+epF5Ju6zuy399OTjczhPCmEMLPsktou0k6u/N9VuNWzVdDCOE9IYSHsstgu4HNeb4uJL/fxOvFGE8BXSTd2HF5/Zmd5nNeS9L5nM1a4Jk8651u+t/HlSGEr4cQOrM1fHlaDc9nB4ZNkQ2yY8ArQwiXA60k3VpJ0iIzwEqSXozpt5D5f0i6jhtjjPXAfyBZxllIh4CJDmG2a9Yy9+UvqsZDJMFn3Olu8/OPwOuyHczbyC4fDiFUAf8D+BNgRYyxEfhOnnUcnquGEMIG4AskS52XZF/38ZzXPd0tfw4C63Jerw5oAjrzqGu6+T7n/cDFc/zcXOf6szVV5xxbOe2a6b/ffySZnn1Ftob3TKthXQihZI46/pZkGfG7SJYWD89xnSRpARlgJUlnUx3QA/Rnh+DMt//1bLkX2B5CeHN2X+OHSfZAFqLGrwMfCSG0ZAf6fGy+i2OMR4AfAX8DPBFjfCp7qgIoB44B6RDCm4DXvoAaPhFCaAzJfXI/lHOuliTEHSPJ8neQdGDHHQHW5A5TmubvgfeFEK4MIVSQBOz7YoxzdrTnMd/nfDfQGkL4UAihPIRQH0IY37f8JeCzIYSLQ2JbCKGZJLgfJtl3WxJCuJOcsD1PDf1ATwhhLfC/5Zz7KXAC+OOQDMaqCiG8Iuf835Hsxb2dJMxKks4BBlhJ0tn0b0mGCvWSdOD+sdBvmA2Jbwf+nCSQXAy0k3TeznaNXyDZq/oI8AuSLurpfA14HZPDm4gxdgP/BtgJnCQJSvfmWcOnSDrBzwP/TE64ijE+DPwl8PPsNZuBn+X87HeBp4AjIYTcpcDjP/8tkqW+O7M/30qyL/ZMzPk5xxh7SPYEvxU4SjJ4anxv6p8Bu0g+51Mke2crs0vD3w98gmSP88Zpv9tsPkUybKuHJDT/U04NYyT7p7eQdGP3kfw5jJ9/nuTPeSTG+JMX+LtLkgokJP//QJKk80N2SehB4NdijPctdj0qXiGEvwWejTF+erFrkSQlShe7AEmSXqwQws0kS0KHgI+TDOD5+bw/JM0ju5/4NuCKxa5FkjSpYEuIQwh/HUI4GkJ4dI7zIXuz8adDCA+HELYXqhZJ0nnvlcCzJEtLbwZ2OHRHZyqE8CfAQ8Afxxj3LXY9kqRJBVtCHEJ4FdAH/G2M8fJZzt8C/D7JDdFfCnwuxvjSghQjSZIkSSp6BevAxhh/SDKYYi63kYTbGGO8H2gMIawqVD2SJEmSpOK2mHtgW5h6w/ED2WOHpl+YHZV/J0BNTc3Vmzdvnn6JJEmSJOk88OCDDx6PMc56S7zFDLCz3ax91vXMMcYvkozR55prrokPPPBAIeuSJEmSJC2SEMLeuc4t5n1gDwBrc56vIbntgSRJkiRJMyxmgL0b+K3sNOKXAT0xxhnLhyVJkiRJggIuIQ4h/D1wI7A0hHAA+BRQBhBj/CvgmyQTiJ8GBoD3FqoWSZIkSVLxK1iAjTG+4zTnI/B7hXp/SZIkSdL5ZTGXEEuSJEmSlDcDrCRJkiSpKBhgJUmSJElFwQArSZIkSSoKBlhJkiRJUlEwwEqSJEmSioIBVpIkSZJUFAywkiRJkqSiYICVJEmSJBUFA6wkSZIkqSgYYCVJkiRJRcEAK0mSJEkqCgZYSZIkSVJRMMBKkiRJkoqCAVaSJEmSVBQMsJIkSZKkomCAlSRJkiQVBQOsJEmSJKkoGGAlSZIkSUXBACtJkiRJKgoGWEmSJElSUTDASpIkSZKKggFWkiRJklQUDLCSJEmSpKJggJUkSZIkFQUDrCRJkiSpKBhgJUmSJElFwQArSZIkSSoKBlhJkiRJUlEwwEqSJEmSioIBVpIkSZJUFAywkiRJkqSiYICVJEmSJBUFA6wkSZIkqSgYYCVJkiRJRcEAK0mSJEkqCgZYSZIkSVJRMMBKkiRJkoqCAVaSJEmSVBQMsJIkSZKkomCAlSRJkiQVBQOsJEmSJKkoGGAlSZIkSUXBACtJkiRJKgoGWEmSJElSUTDASpIkSZKKggFWkiRJklQUDLCSJEmSpKJggJUkSZIkFQUDrCRJkiSpKBhgJUmSJElFwQArSZIkSSoKBlhJkiRJUlEwwEqSJEmSioIBVpIkSZJUFAywkiRJkqSiYICVJEmSJBUFA6wkSZIkqSgYYCVJkiRJRcEAK0mSJEkqCgZYSZIkSVJRMMBKkiRJkoqCAVaSJEmSVBQMsJIkSZKkomCAlSRJkiQVBQOsJEmSJKkoGGAlSZIkSUXBACtJkiRJKgoGWEmSJElSUTDASpIkSZKKggFWkiRJklQUDLCSJEmSpKJggJUkSZIkFQUDrCRJkiSpKBhgJUmSJElFwQArSZIkSSoKBlhJkiRJUlEwwEqSJEmSioIBVpIkSZJUFAywkiRJkqSiYICVJEmSJBUFA6wkSZIkqSgYYCVJkiRJRcEAK0mSJEkqCgZYSZIkSVJRMMBKkiRJkoqCAVaSJEmSVBQMsJIkSZKkomCAlSRJkiQVBQOsJEmSJKkoGGAlSZIkSUXBACtJkiRJF4AH93bx+e8/zYN7uxa7lDNWutgFSJIkSZLOjhgj/SNpuvpH6B4YpWtghK6BER450MOXf/I8Y5lIZVmKr97xMq5e17TY5b5gBlhJkiRJOgeNpTP0DI7SNTBK98AIXdlAOv5998AIXf3jxyYfR9KZeV93dCzD/c+eMMBKkiRJkqaKMTI4mk4CaH9u2MwNpePd0vFgOsKpobE5X7M0FWisLqepuoym6nLWLalm29pGGmuS5+PHm2qS7/edHOCDX93N6FiGstIUL9uwZAE/gbPHACtJkiRJeUpnIqcGp4XNicfZO6MnB0YYGZu7K1pbUUpjNnA2VpexrrmapuqyyYBaUz4lrDZWl1FbUUoIIe+6Ny6v46t3vIz7nz3ByzYsKcruKxhgJUmSJF2ghkbTSejsn32J7vSluV0DI/QMjhLj7K9Xkgo0VpVNhNE1TdVc0TIeQMc7o7mhtIzGqnLKSxdmtu7V65qKNriOM8BKkiRJKmqZTKR3aGxiYNGM5bjTOqPdAyOcHBhhaHTurmh1eclEt7OpupyWxqqJ8NlYXU5TzXiHdPJYXUUpqVT+XVG9cAZYSZIkSeeM4bH0ZACdozM6vUPaPTBCZo6uaCpAQ9Xk0ttVDZVsWVU/pQs60RnN7h9tqCqjsqxkYX9x5cUAK0mSJOmsizHSOzxGd//oPJ3RnA5p9rqBkfScr1lZlsoG0aTruWVl/ZS9o02zdEbrK8vsip5HChpgQwg3A58DSoAvxRj/dNr5VuArQGP2mj+IMX6zkDVJkiRJemFG05nJANp/muFF2cfugVHG5miLhgD1lWUTS2+X1VZwyfK6iWDaWFNO87Tluk3V5XZFVbgAG0IoAT4P3AQcAH4RQrg7xrgn57JPAl+PMX4hhLAV+CawvlA1SZIkSReyGCP9I+kpt3KZbVDR9M5o3/Dct3MpL01NmY67aXntjIm50zujDVVllNgVXXj7fw7P3wfrr4e11y52NWekkB3Ya4GnY4zPAoQQ/gG4DcgNsBGoz37fABwsYD2SJEnSeWMsnaF7MGc/aP9phhdlH0fTc2wWBeoqS6fcQ3TD0prJ5bg1k3tFG7P7R5uqy6gqK3lBt3PRixAjjA7AcB+MZL/Gvx/uzXneDyO9Oef64NRBOPwwEKG0Et59T1GG2EIG2BZgf87zA8BLp13zaeA7IYTfB2qA1832QiGEO4E7AVpbW896oZIkSdJiiTEyOJqeEUJnu5XLeAg92T9C79DcXdGykjC5HLe6nIuW1rC9eua9RMdDaGN1OY1VZZSWLMztXC4YMcLo4OyBcuJ5/ywBdPx5/9RjI30Q556cPEVpFZTXQEUtlNfBUA9J/xBIjyadWAPsFLP9M8z0f+55B/DlGON/CiFcB/xdCOHyGKf+qcQYvwh8EeCaa66Z+5+MJEmSpEWUzkR6BnMCaP/cS3Nzj42MzR1KaitKpwwqWtdcPbk3dCKETu2M1pTbFT0jMcLY8OyBckq47JsaSEf6Zwmg44Fz7qFUU5RUZMNmLVTUJY/VzdC4NgmgE+dyr6mZev34ufJaKJkW9fb/HL5yK6RHoKQ8WUZchAoZYA8Aa3Oer2HmEuH3ATcDxBh/GkKoBJYCRwtYlyRJknRaQ6PpeW/lMn3v6Mn+EU4NjRLnaLeUpMKU4Lm2uZor1zRMmaqbG0obq8torCqnvNSu6LzGhmcJlGcSNrMdz8zcne0pSspzQmM2YFY2QH3LzEA5JWzWzh5IS8oK+zmtvRbefbd7YOfxC2BTCOEioBP4DeD2adfsA14LfDmEsAWoBI4VsCZJkiSdZx7c28X9z57gZRuWcPW6phnnM5nIqaHRqQG0f2YAnd4hHRqduytaXV4yZUBRS2PVxN7Rxmn7RZuqy2msKaOuotSuKMDYyCxLZGdbPjseNqcF0OkhNTOa3/umyibD4/jS2oo6qF81NVCW18wRQKd1OEvLC/s5FcLaa4s2uI4rWICNMY6FED4EfJvkFjl/HWN8LITwGeCBGOPdwL8F/t8Qwr8hWV78nhjn+jcrSZIkXejGBxeN38rlgedP8hffe5KxdCSVCrxq01JKUmFKh7R7YIQ57uZCKpDs/5wIopVctro+pxs6M5Q2VpdRUXoB3c4lPTrLfsx5AuXpBgqlR/J731TpZFjM7VbWrpgMlLl7PKd0NOtmniutKOznpAURii0vXnPNNfGBBx5Y7DIkSZL0Io3vFz3ZP367luTxZLY7erJ/8lhXdoluz+D83ba6ilLWZPeI5nZIxx+ba3Ju61JdTl1lKanz7XYu6bHTBMr5ltjO0hFND+f3viE1z17NfMNmzvWlFckNY3XBCSE8GGO8ZrZzhVxCLEmSpAvE+DLdk9NCaFf/CCfHH/sn7zva1T9C9+Dc+0UrSlM0Z4cTNdeU09JUTXPuwKKacpqryzl8apD/feejjKUzlJWm+PJvXzvrMuJzWiY9937MWbud08PmtGvGhvJ735CavcNZ3TrP8tma2QNoRW1yaxYDpwrMACtJkqQpYoycGhqbFj6zg4pyno93Scf3j861TLe8JBtGs7ds2bKqnuaJEDoZSsevaa4up6o8/yW6Fy2tnXcP7FmXyUzrYuaGzTOYXjs6kOcbh5lhs7wWGtbMHijnmlA7PlCorNrAqaJjgJUkSTqPxRjpGx6jq39m+JzolOYE1fHluuk50mhZSZgMm9XlXLqybsrz3KA6/ry6wLd0uTr1FFeX3gep64FZBtRkMjDan3+gPN1AodH+/Isb73DmLp+tb8n/dii558qqIeVEYl3YDLCSJElFIsZI/0g6Z6/o1BA6GUpHJgJr98AIo+nZw2hyW5dymrPDiTYur6Ux53luR7QpO8SodiEm6caYXR4734Ta7LnjT8Jju5J7bYYUrLg8eZzSAe0nmReah7Kamfsx61ZNnT47a7dzlum1ZTUGTuksM8BKkiQtghgjg6PpZFrutCW54xN2Z3RM+0cZSc9+a5dUYMre0PVLq9le05gE0vHjNbmDjMqprzxLYTRGGB08fdjMa3pt9jHfwJkqTcIrQMzAYDcs3wzlG+a/HcpsA4XKayB1AU0XloqQAVaSJOksGBpNTwmaM5frzgyqw2Ozh9EQoLGqbCKMrm2u5qo1jZNLc2tyQ2ny/QuaphtjMuinv/vF3Q4l9/o49z1TpyitnNm9rF4Kjevyvx1KbiDtfBC+cmtya5aScvi1/1r097mUNDcDrCRJ0jTDY+kkhPYnS3CnT9HN3UM6ft3gaHrO15u8bUsZq7P3GZ1tr+h4MK2vKqMkN4zGCGPDOYGyezJQ9p1uQu30juh44Jy73ilKKmZ2L6uaoHHt7IFyzum12a+Ss/yfn2uvhXffDc/fB+uvN7xK5zkDrCRJOq+NjGUmQujEJN3+abd3GRhNrske7x+ZO9zVV5ZOLMFdXlfJpSvqk6W52fDZmA2jzZUZmktGqE8NUzrWn9O9PDY1bHb3wdHThM2RPsiM5fcLl5TP7F5WNiSDg2YLlKfbz1lSdpb+JApo7bUGV+kCYYCVJElFYzSdmbhly0R3NLcrOh5Ks8t1u/pH6B2eO/jVVZQmXdCacpZVB65sKmF5ZRnLKmBJ6QhNpcM0lAxTnxqmhiGqGaBkdGBqAB3qg1OzLLHNjOb3S6XKZi6VraiD+lXzDwiasZ8z+31p+Vn6tCXp3GOAlSRJiyKdiXTn3MplvuW6493RU0OTYbSUMWoYSr7CEEvLhllROcalFaMsKRuluW6YxqYRGlLD1IVhasMgVXGQiswAFekBStIDpMaD6Mm+ZA9lPlKls9z6pBZqV8w/IGiu/ZylFQX6hCXp/GOAlSRJL1omE+kZnH9wUU//IAN9PYwMnGJs8BRxuI9qBqlliOpsCK1lkJowxNrUMFeVDdNYMkxdapi6MER11RBVFUkALUv3U5KZJXCOZL9yhZKZS2Yra6FiZU6wrJk9bE6cy+l2llYkU5YkSQvOACtJkqbIZCK9Q2Oc7Buku6eLvp5u+nq7GOrvYajvFCMDpxgdPEVmqJc43EdqpI/Ssf6JEFrDII1hiDUk39eGIWrDEBXTk+UcK11jSBFm7M9snjtQTllOWzMzgJZWGjgl6TxhgJUk6Vy1/+cvbLJqJjPtFihJwBzs72Ggt4eBvm5G+pMAmh7qJTOcDA1KjfZTMtpPebqfiswgVXGAaoa5KAznVWaGwGhFNWOl1WTKkjAZKpZQUlVHWVUdpZV1cyynnX1ibSirMnBKkmZlgJUk6VwTIzz+DTL//b2EzCgxlJC67DZieS1jg6cYG+wlPdRLHO4ljPSRGh2gdKyf8szgjJcKQHX2K1dfrKSfSoZCNcMl1YyWVDNYtYL+siRIpirqKK2qo6y6nsqaBqpq6qmpa6Sipp5QUTel+5kqq6YiBNzJKUkqNAOsJEmLabAbjv4Sjj4GR/bA0T1kjuwhNdxDKntJiGOMPLqL7lg3ETwHqKQvVtFPA32xkoFQRaa0llhRQ6q8llRlPaXVdVRU11NR00BNXSM19Y3U1TXS2NBIU20FyytKCXY6JUlFxAArSdJCGBuB40/C0T1w5LHs4x44dWDikoFUDU/FtTwyeg39VPKekm9TQoZRSvkPDX/M0q3X01yd3PKluaaM5upyLs4+r680jEqSzn8GWEmSzqYYoXvfzKB64inIJLeAyYQyjlW28mTcyM/S1/NYeg1PZNZCfQtt65vZ3tpEfXkJ777npVwdH+PBcBkffeuvcfW6pkX+5SRJWlwGWEmSztTAycmAOrEE+Jcw0jtxyXBtC0cqL+aXjW38pHclP+1bwbNxFanRcq5oaWD7FY28rbWJttYmVjZUTnn5S1b8Fvc/e4KPblhieJUkCQOsJEmnNzoEx5+YFlT3QO+hyWsqGxlZsoVDa97MY+k1/OjUCv75WBNdx5NQ2tJYRdtFjfxGaxNtrY1sXV1PRWnJvG979bomg6skSTkMsJIkjctkoPv5yYA6vgT4xDMQ08k1JRWw7BLS61/F4coNPDK6hh/2LOMHnSUcfCa57Ux5aYorWxr4tesa2d7axPZ1Tayor5z7fSVJUl4MsJKkC1P/8Zw9qtnHo4/DaP/kNU3rYfllsPU2ums38dBoCz86Wc+D+3t5tP0UI2MZAFoay9i+vpE7smF166p6yktTs7+vJEk6YwZYSdL5bWQAjj0+c69q/9HJa6qXwPKtsP1dsHwrI0s3s2eshQcOjtC+r5vdP+viUM8Q0EN5aS9XtjTwnpevZ3trI22tdlclSVooBlhJ0vkhk4aTz+XsUc0+dj0HMemUUloJyzbDppuSwLpiKyy/jEPpOnbv62H3vi7af9bFo50nGUkfB5K9q9esb2Z7a7IceIvdVUmSFo0BVpJUfPqOwpFHp+5VPfYEjA1mLwjQvCEJqFf8+kRQpfkihjPwaOcp2vd10X5/N7v3PZrtrkJFaYor1zTw3lesp621ie2tjSy3uypJ0jnDACtJOneN9Cf7Uqd3VQeOT15TszwJqNf8djaobk26rOXVABzsHkyWAd/fxe599/NY5ylG0klHdk1TFS/Jdlfb7K5KknTOM8BKkhZfegxOPpMdpvTLya5q1/NATK4pq4blW+DSN8CKy7JLgC+DmqUTLzM8lk66qz87zO59Xeze283hU5Pd1avWNE52V9c1srzO7qokScXEACtJWjgxQu/hbCc1p6t67ElIJ7egIaRgyUZYdRVsu31yr2rjekhN7Y4e7B5k98MH2b23m937uthzcLK7ura5imsvyu5dXZd0V8tK7K5KklTMDLCSpMIY7k26qRO3qsmG1cGuyWvqViUBdcONyR7VFVth6aVQNrMzOjSa5rH9JyfCavu+ye5qZVmKK1saee8r17O9tYm2VrurkiSdjwywkqQXJz0KJ56eGVS7901eU16XLP/dettkUF2+FaqbZ33JGCMHe4bYvbcrWQq8r5s9B3sYTSfLidc2V/HSDc1sb21ie2sTm1fV2V2VJOkCYICVJOUnRjjVOXWY0tE9yfTfzGhyTaoUlmyCNS+B7e+e3Kva2AohzPnSQ6NpHu3smeis7t7XxZFTyZLiyrIUV65p5H2v3DAxbGlZXcVC/MaSJOkcY4CVJM002J0dppQTVI/sgeGeyWvq1ySd1I2vmwyqSzdB6fzhMsZIZ/cgu/d10z5Ld7W1uZrrNixh+7om2tbaXZUkSZMMsJJ0IRsbgeNPTk79HQ+qpw5MXlPRkCz/veKt2aB6WfK8qjGvt8jtro7vXz3am3RXq8pKuHJNg91VSZKUFwOsJF0IYkz2pE4PqieegsxYck2qDJZeAuuum7xFzfKt0LBm3uW/U99msru6e28X7fu62HPo1JTu6ssvTrqr21ubuHSl3VVJkpQ/A6wknW8GTk4dpnRkT7IceKR38pqG1mT5b+49VZdugpKyF/RWQ6NpHunsmRi21L6ve0Z39Y7rN0xMBl5aa3dVkiSdOQOsJBWr0SE4/sTMoUq9hyavqWxMAupVv5Gd/Jtd/ltZ/4LfLsbIga7BKYOW9hw8xVgm6a6uW1LNKzYunVgKvHllHaV2VyVJ0llkgJWkc10mA93P5wxTyi4BPvEMxHRyTUkFLLsELrphMqiu2JrcZzXP5b/TDY2mefjA+GTgZNjSsZzu6lVrG7jzVRtos7sqSZIWiAFWks4l/cdz9qhmH48+DqP9k9c0rU8C6tbbJveqNl8MJWf+P+mn666uX1LN9RuX0mZ3VZIkLSIDrCQthpEBOPb4zL2q/Ucnr6lekgTU7e+aDKrLNkNF7Yt++8GR7N7VfV3Z/avdHO9Lun9KU4UAACAASURBVKvV5SVctaaRO181uXd1id1VSZJ0DjDASlIhZdJw8rmcParZx5PPAkl3k9LKJJhuuikbVLNLgGuXn/Hy31y53dXxsPrLQ1O7q6/atJS2dU1sb23k0hV2VyVJ0rnJACtJZ0OM0Hd06jClI4/BsSdgbDB7UYDmDUlAveLXJ4Nq80WQKjlrpQyOpHn4QHdyK5vskuDp3dUP3DDeXW2iuab8rL23JElSIRlgJemFGu5Llv9O36s6cGLymprlSUC95rezQXVr0mUtrz6rpcQY2X8y213Nfv3yUC/pbHf1oqU1vOqSpWxvTe67esmKWrurkiSpaBlgJWku6TE4+UxOUM0uAe7ay8Ty37Lq5LY0l94yeT/VFZdBzdKClDQwMpYzGbib9n1dHO8bAaCmvISr1jbyuzdcPDFsye6qJEk6nxhgJSnG5N6pU+6n+hgcexLSydJbQgqWbIRV22Dbb07uVW1cD6nCdDRjjOw7OTBlMnBud3XD0hpuuGQ5ba2NbG9t4tKVdZSkXvyeWUmSpHOVAVbShWXoFBz95dS9qkf3wGDX5DV1q5KAuuHGyfupLr0UyioLWtrAyBgP7e+hfX8Xu/d207F/and1W2vSXd2+rpG2tU002V2VJEkXGAOspPNTehSOPzV1j+qRPdCzb/Ka8rpk+e/W2yaD6vKtUN1c8PJijOw9MTARVnfv6+LxwzO7q9vXNWb3rtpdlSRJMsBKKm4xwqnOnOW/2c7q8SchM5pckyqFJZtg7Uvg6ndP7lVtbD0rt6nJx3h3NVkOnCwJPtE/tbv6wRsvZntrE9vWNtpdlSRJmoUBVlLxGOyeufz3yB4Y7pm8pn5N0knddNNkUF26CUorFqzM8e7qxGTgvd08cSSnu7qshldvXp5MBl7XyKbldlclSZLyYYCVdO4ZG0k6qNOX/546MHlNRUOy/PeKt2aD6mXJ86rGBS+3f3iMhw50J4OW9nbRvr+bk9nuam1FKdvWNvJ7N15MW2sTba2NNFbbXZUkSToTBlhJi2P/z+G5+5LuaEnZ1KB64inIjCXXpcpg6SWw7rrJW9Qs3woNaxZs+W+uGCPPnxjIBtWku/r44VNkm6tcvKyG125eTpvdVUmSpLPOACudCzIZiGnIpJPgNv59zMxxbCz7fXrq45kcm3jt2WqYr67pNbyAY4NdSWAdv5fquIbWZPnvpW+Yuvy3pGxR/lgg213d3037/pnd1bqKUra1NvKhV2+kbV0TbWvtrkqSJBWSAVZnR4w5ISU3sGRmOXaGQWvK62TyPDZXDZlpgWyuYy+krswLPzb+fsUgVQqhBFIl2cfULMemfT/XseE+JsNrKhmsdNNnoLJ+MX/DKd3VZP9qN0/M0l3dvq6J7a1NbFxea3dVkiRpARlgz6axEej4KhzqSPbjLbtk7sDygjtuZ7ELlxmbFvbyPTZPUJzeSTsXnUnQmvVYafb7slmOpeY+NiXszXZsrlCYb1AsTa7N69gZ1HU27f85fOVWSI9ASTlsu31Rwut4d3U8rLbv66JrIJlcPN5dvek1m9jemtx3taF68TrBkiRJMsCeXc/8K9z7kcK9fjhNOJkIHXkeKy2f57rSqeFlxrHxsDPtWN6hsHRa2Jvt2OlqeAF1hdSi7JfUHNZeC+++G56/D9ZfnzwvsBgjzx3vZ/e+bGDd28WTR3onuqsbl9dy09YV2cnATWxcVkvK7qokSdI5xQB7Nh1+GEgBmSQwbf8tuOr203S7pgeteUKfAUznk7XXFjS49o13V7PLgdv3d9M93l2tTCYDv/6ylbTZXZUkSSoaBtizacONcN+f5yyL/M0F6SxJF7oYI88e758YsjS9u7ppeS2v35qEVburkiRJxcsAezYtwrJI6ULUOzTKQ/t7aN83f3d1+7omtq1tpKHK7qokSdL5wAB7thV4WaR0ocntro4PWnriSC9xWnd1+7pGtrc2cbHdVUmSpPOWAVbSOWW8u7p7vLu6r5uewcnualtrEzdfvpLtrU1cZXdVkiTpgmKAlbQoHtzbxU+fOU5rczVDY5lkOfDebp48mnRXQ0i6q2/IhtW21ka7q5IkSRc4A6ykBdU7NMpf/f/P8IUfPDMxZAmgPttdveWKVbS1NrKttZH6SrurkiRJmmSAlVRwI2MZfvjkMXZ2dPK9PUcYHstMnAvAe1+5nk/estXuqiRJkuZlgJVUEDFGdu/rYmd7J/c+fIjugVGaqst4+0vWsmVVPX94z2OMjmUoK03xxitWG14lSZJ0WgZYSWfV00f7uKujk10dnew/OUhlWYqbtq7kLW2ruX7TMspKUgBcsqKO+589wcs2LOHqdU2LXLUkSZKKgQFW0ot2tHeIex46xK72Th7p7CEV4BUbl/KR117C6y9fSW3FzP+puXpdk8FVkiRJL4gBVtIZ6R8e49uPHWZneyc/fvo4mQiXt9TzyTdu4darVrO8vnKxS5QkSdJ5xgArKW+j6Qw/euo4O9s7+e6eIwyOplnTVMUHb9zIjrbVbFxet9glSpIk6TxmgJU0rxgjHfu72ZUdxnSif4SGqjJ+dXsLb2lr4ep1TYTgACZJkiQVngFW0qyeO97PrvZO7uro5PkTA5SXprhpywp2tLVwwyXLKC9NLXaJkiRJusAYYCVNON43zL0PHWRXx0E69ncTAly3YQkffPVGbr58JfWVZYtdoiRJki5gBljpAjcwMsZ39xxhV3snP3zqOOlMZMuqej5xy2befNVqVjVULXaJkiRJEmCAlS5IY+kMP37mBLvaO/n2Y4cZGEmzuqGSO1+1gR3bWrh0pcOYJEmSdO4xwEoXiBgjj3T2sLO9k3seOsTxvmHqK0u5bdtqbtvWwrXrm0mlHMYkSZKkc5cBVjrP7TsxwK6OTnZ1dPLssX7KS1K8ZvNydrS18OrNy6goLVnsEiVJkqS8GGCl89DJ/hG+8XAyjOnBvV0AvPSiZu68fgNvuHwVDdUOY5IkSVLxMcBK54mh0TTf3XOEuzo6+cETxxjLRC5ZUcvHbt7MrdtW09LoMCZJkiQVNwOsVMTSmchPnznBro5OvvXoYfqGx1hZX8n7XnkRt21rYcuqOkJwX6skSZLODwZYqcjEGHns4Cnu6ujkro6DHO0dpq6ilFuuWMmObS28dMMSShzGJEmSpPOQAVYqEge6Brir4yC72jt56mgfZSWBGy9dzo5tLbx2y3IqyxzGJEmSpPObAVY6h3UPjPDNRw6zq72Tnz9/EoCXrG/iszsu541XrKKppnyRK5QkSZIWjgFWOscMjab5/uNH2dneyfefOMpoOnLxsho++vpLufWq1axtrl7sEiVJkqRFYYCVzgGZTORnz51kV3sn33z0EL1DYyyrq+Dd161nR1sLl62udxiTJEmSLngGWGkRPX74FDvbO7m74yCHeoaoKS/h5stXsaNtNS+/eKnDmCRJkqQcBlhpgR3sHuTuh5JhTI8f7qU0FbjhkmV8/JYt3LRlBVXlDmOSJEmSZmOAlRZAz+Ao33r0EDvbO/nZcyeJEba3NvKZ2y7jjVesYkltxWKXKEmSJJ3zDLBSgQyPpfnBE8fY1d7Jvzx+lJGxDBuW1vCR117CjrbVrFtSs9glSpIkSUXFACudRZlM5IG9Xexs7+SbjxyiZ3CUpbXl3H5tK29pa+HKNQ0OY5IkSZLOkAFWOgueOtLLzvZO7uo4SGf3IFVlJbz+shXsaGvhlRuXUlqSWuwSJUmSpKJngJXO0JFTQ9zdcZCd7Z3sOXSKklTglRuX8tHXX8pNW1dQU+H/eUmSJElnk/+FLb0AvUOjfOvRw+zq6OQnz5wgRrhqTQOfevNW3nTlapbVOYxJkiRJKhQDrHQaI2MZfvjkMXZ2dPK9PUcYHsvQ2lzN779mEzu2rWbDstrFLlGSJEm6IBhgpVnEGNm9LxnG9I2HD9E1MEpTdRlvf8ladrS10La20WFMkiRJ0gIzwEo5nj7ax10dyTCmfScHqCxLcdPWlbylbTXXb1pGmcOYJEmSpEVjgNUF72jvEPc8dIi7Ojp5+EAPqQCv2LiUD792E6+/fCW1DmOSJEmSzgn+l7kuSP3DY3z7scPs6jjIj546RibC5S31fPKNW7j1qtUsr69c7BIlSZIkTWOA1QVjNJ3hR08dZ1dHJ9957AiDo2nWNFXxwRs3sqNtNRuX1y12iZIkSZLmYYDVeS3GSMf+bu7qOMg9Dx3kRP8IDVVl/Or2Ft7S1sLV65ocxiRJkiQVCQOszkvPH+9nV0cnu9o7ef7EAOWlKW7asoIdbS3ccMkyyksdxiRJkiQVGwOszhsn+oa59+FD7GzvpGN/NyHAdRuW8MFXb+Tmy1dSX1m22CVKkiRJehEMsCpqAyNjfHfPEXa1d/LDp46TzkS2rKrnE7ds5tarWljZ4DAmSZIk6XxhgFXRGUtn+MkzJ9jV3sm3HjvMwEia1Q2V3PmqDezY1sKlKx3GJEmSJJ2PChpgQwg3A58DSoAvxRj/dJZr3gZ8GojAQzHG2wtZk4pTjJFHOnvY1X6Qux86yPG+YeorS7lt22p2bGvhJeubSaUcxiRJkiSdzwoWYEMIJcDngZuAA8AvQgh3xxj35FyzCfg48IoYY1cIYXmh6lFx2ndigLs6OtnZ0cmzx/opL0nxms3L2dHWwqs3L6OitGSxS5QkSZK0QArZgb0WeDrG+CxACOEfgNuAPTnXvB/4fIyxCyDGeLSA9ahIdPWPcO8jh9jV3smDe7sAeOlFzdx5/QbecPkqGqodxiRJkiRdiAoZYFuA/TnPDwAvnXbNJQAhhB+TLDP+dIzxW9NfKIRwJ3AnQGtra0GK1eIaGk3zvV8mw5h+8MQxxjKRS1bU8rGbN3PrttW0NFYtdomSJEmSFlkhA+xsGxLjLO+/CbgRWAPcF0K4PMbYPeWHYvwi8EWAa665ZvprqEilM5H7nz3BzvZOvvXoYfqGx1hZX8n7XnkRt21rYcuqOkJwX6skSZKkRCED7AFgbc7zNcDBWa65P8Y4CjwXQniCJND+ooB1aRHFGNlz6BS72ju5+6GDHDk1TF1FKbdcsZId21p46YYllDiMSZIkSdIsChlgfwFsCiFcBHQCvwFMnzC8C3gH8OUQwlKSJcXPFrAmLZIDXQPc1XGQXe2dPHW0j7KSwI2XLmfHthZeu2U5lWUOY5IkSZI0v4IF2BjjWAjhQ8C3Sfa3/nWM8bEQwmeAB2KMd2fP/UoIYQ+QBj4aYzxRqJq0sHoGRvlGdhjTz58/CcBL1jfx2R2X88YrVtFUU77IFUqSJEkqJiHG4tpSes0118QHHnhgscvQHIZG03z/8aPszA5jGklnuHhZDb+6fQ23XrWatc3Vi12iJEmSpHNYCOHBGOM1s50r5BJiXSAymcjPnjvJrvZOvvnoIXqHxlhWV8FvXbeOHW0tXLa63mFMkiRJkl40A6zO2OOHT7GzvZO7Ow5yqGeImvISbr58FTvaVvPyi5c6jEmSJEnSWWWA1QtyqGdwYhjT44d7KU0FbrhkGR+/ZQs3bVlBVbnDmCRJkiQVhgFWp9UzOMq3Hj3ErvaD3P/cCWKE7a2NfOa2y3jjFatYUlux2CVKkiRJugAYYDWr4bE0P3jiGHd1dPK9Xx5lZCzDhqU1fOS1l7CjbTXrltQsdomSJEmSLjAGWE3IZCIP7O1iV0cn33j4ED2DoyytLef2a1t5S1sLV65pcBiTJEmSpEVz2gAbQqgBBmOMmezzFFAZYxwodHFaGE8d6WVneyd3dRyks3uQqrISXn/ZCna0tfDKjUspLUktdomSJEmSlFcH9l+A1wF92efVwHeAlxeqKBXekVND3N1xkF0dnTx28BQlqcArNy7lo6+/lJu2rqCmwua8JEmSpHNLPimlMsY4Hl6JMfaFEKoLWJMKpHdolG89epi7Og7y42eOEyNctaaBT715K2+6cjXL6hzGJEmSJOnclU+A7Q8hbI8x7gYIIVwNDBa2LJ0tI2MZfvjkMXZ1dPLdPUcYHsvQ2lzN779mEzu2rWbDstrFLlGSJEmS8pJPgP0I8N9DCAezz1cBby9cSXqxYozs3tfFrvaD3PvwQboGRmmqLuPtL1nLjrYW2tY2OoxJkiRJUtE5bYCNMf4ihLAZuBQIwOMxxtGCV6YX7JljfdzV3smujoPsOzlAZVmKm7au5C1tq7l+0zLKHMYkSZIkqYjlM4X494CvxhgfzT5vCiG8I8b4fxe8Op3Wsd5h7nkoGcb08IEeUgFesXEpH37tJl5/+UpqHcYkSZIk6TyRT7p5f4zx8+NPYoxdIYT3AwbYRdI/PMZ39hxmZ/tBfvTUMTIRLm+p55Nv3MKtV61meX3lYpcoSZIkSWddPgE2FUIIMcYIEEIoAcoLW5amG0tnuO/p4+xq7+Q7jx1hcDTNmqYqPnjjRna0rWbj8rrFLlGSJEmSCiqfAPtt4OshhL8CIvA7wLcKWpWAZBjTQwd62NXeyT0PHeRE/wiN1WX86vYW3tLWwtXrmhzGJEmSJOmCkU+A/RjwAeB3SYY4fQf4UiGLutA9f7yfXR2d7Grv5PkTA5SXprhpywp2tLVwwyXLKC91GJMkSZKkC08+U4gzwBeyXyqQE33D3PvwIXa2d9Kxv5sQ4LoNS/jgqzdy8+Urqa8sW+wSJUmSJGlR5TOFeBPwJ8BWYGI6UIxxQwHruiAMjqT5zp7D7Grv5IdPHSediWxZVc8nbtnMrVe1sLLBYUySJEmSNC6fJcR/A3wK+Avg1cB7SZYS6wyMpTP85JkT7Grv5NuPHaZ/JM3qhkrufNUGdmxr4dKVDmOSJEmSpNnkE2CrYoz/kp1EvBf4dAjhPpJQqzzEGHm08xQ72zu55+GDHOsdpr6ylFu3rWbHthZesr6ZVMp/E5AkSZKk+eQTYIdCCCngqRDCh4BOYHlhyzo/7D85wK72TnZ1dPLMsX7KS1K8ZvNydrS18OrNy6goLVnsEiVJkiSpaOQTYD8CVAP/C/BHJMuI313IoorZD544yn/72T72nxzgicO9ALz0ombef/0G3nD5KhqqHcYkSZIkSWcinynEv8h+20ey/1VzuPfhg3zoa+1Askn4nS9dx++++mJaGqsWtzBJkiRJOg94Q9GzaO+J/onpVqkAqxorDa+SJEmSdJYYYM+il21YSkVZipIAZaUpXrZhyWKXJEmSJEnnjXz2wCpPV69r4qt3vIz7nz3ByzYs4ep1TYtdkiRJkiSdN04bYEMIy4D3A+tzr48x/nbhyipeV69rMrhKkiRJUgHk04G9C7gP+B6QLmw5kiRJkiTNLp8AWx1j/FjBK5EkSZIkaR75DHG6N4RwS8ErkSRJkiRpHvkE2A+ThNihEEJv9utUoQuTJEmSJCnXaZcQxxjrFqIQSZIkSZLmk9dtdEIItwKvyj79QYzx3sKVJEmSJEnSTKddQhxC+FOSZcR7sl8fzh6TJEmSJGnB5NOBvQXYFmPMAIQQvgK0A39QyMIkSZIkScqVzxAngMac7xsKUYgkSZIkSfPJpwP7J0B7COH7QCDZC/vxglYlSZIkSdI0+Uwh/vsQwg+Al5AE2I/FGA8XujBJkiRJknLNuYQ4hLA5+7gdWAUcAPYDq7PHJEmSJElaMPN1YP9X4E7gP81yLgKvKUhFkiRJkiTNYs4AG2O8M/vtG2KMQ7nnQgiVBa1KkiRJkqRp8plC/JM8j0mSJEmSVDBzdmBDCCuBFqAqhNBGMsAJoB6oXoDaJEmSJEmaMN8e2NcD7wHWAH+ec7wX+EQBa5IkSZIkaYb59sB+BfhKCOGtMcZ/WsCaJEmSJEmaIZ/7wP5TCOGNwGVAZc7xzxSyMEmSJEmScp12iFMI4a+AtwO/T7IP9teBdQWuS5IkSZKkKfKZQvzyGONvAV0xxj8ErgPWFrYsSZIkSZKmyifADmYfB0IIq4FR4KLClSRJkiRJ0kyn3QML3BtCaAT+DNgNROBLBa1KkiRJkqRp8hni9EfZb/8phHAvUBlj7ClsWZIkSZIkTZXPEKffy3ZgiTEOA6kQwgcLXpkkSZIkSTny2QP7/hhj9/iTGGMX8P7ClSRJkiRJ0kz5BNhUCCGMPwkhlADlhStJkiRJkqSZ8hni9G3g69n7wUbgd4BvFbQqSZIkSZKmySfAfgz4APC7QAC+g1OIJUmSJEkLLJ8pxBngC9kvSZIkSZIWxZwBNoTw9Rjj20IIj5AsHZ4ixnhlQSuTJEmSJCnHfB3Yj2Qf37QQhUiSJEmSNJ/5Auy9wHbgszHGdy1QPZIkSZIkzWq+AFseQng38PIQwq9OPxlj/P8KV5YkSZIkSVPNF2B/B/hNoBF487RzETDASpIkSZIWzJwBNsb4I+BHIYQHYoz/dQFrkiRJkiRphvmmEL8mxvivQJdLiCVJkiRJi22+JcQ3AP/KzOXD4BJiSZIkSdICm28J8aeyj+9duHIkSZIkSZpd6nQXhBA+HEKoD4kvhRB2hxB+ZSGKkyRJkiRp3GkDLPDbMcZTwK8Ay4H3An9a0KokSZIkSZomnwAbso+3AH8TY3wo55gkSZIkSQsinwD7YAjhOyQB9tshhDogU9iyJEmSJEmaar4pxOPeB2wDno0xDoQQmkmWEUuSJEmStGDy6cBeBzwRY+wOIbwT+CTQU9iyJEmSJEmaKp8A+wVgIIRwFfDvgL3A3xa0KkmSJEmSpsknwI7FGCNwG/C5GOPngLrCliVJkiRJ0lT57IHtDSF8HHgn8KoQQglQVtiyJEmSJEmaKp8O7NuBYeB9McbDQAvwZwWtSpIkSZKkaU7bgc2G1j/Peb4P98BKkiRJkhbYaTuwIYSXhRB+EULoCyGMhBDSIQSnEEuSJEmSFlQ+S4j/C/AO4CmgCrgD+Hwhi5IkSZIkabp8hjgRY3w6hFASY0wDfxNC+EmB65IkSZIkaYp8AuxACKEc6Agh/J/AIaCmsGVJkiRJkjRVPkuI3wWUAB8C+oG1wFsLWZQkSZIkSdPlM4V4b/bbQeAPC1uOJEmSJEmzmzPAhhAeAeJc52OMVxakIkmSJEmSZjFfB/ZNC1aFJEmSJEmnMV+ALQNWxBh/nHswhHA9cLCgVUmSJEmSNM18Q5z+L6B3luOD2XOSJEmSJC2Y+QLs+hjjw9MPxhgfANYXrCJJkiRJkmYxX4CtnOdc1dkuRJIkSZKk+cwXYH8RQnj/9IMhhPcBDxauJEmSJEmSZppviNNHgJ0hhN9kMrBeA5QDbyl0YdL/bO/eg+0s63uBf38JgSB3Ei414ZBYnZGQEyCkASUqSIcRRWKRNmTkqCClUgHtsefIUUbES8dCpag4HpFLbU8OlCOi4OFSxVRgPFwSIAFDLRmNYxqKIcRAJAGCz/kj2zTEnbDFrL33m3w+M3v2e3ned/32evJM1nc977sWAADAxjYbYFtrjyd5fVUdk2Ry3+b/21r73qBUBgAAABvZ0gxskqS1NjfJ3EGoBQAAADZrS/fAAgAAwLAhwAIAANAJAiwAAACdIMACAADQCQIsAAAAndDTAFtVb6mqH1XV4qo6bwvtTq6qVlXTelkPAAAA3dWzAFtVI5N8KcnxSSYlmV1Vk/ppt1uSc5Pc06taAAAA6L5ezsBOT7K4tfbj1tpzSa5NMrOfdp9KclGStT2sBQAAgI7rZYAdl+RnG60v7du2QVUdluSA1tq3t3SiqjqzquZV1bzly5dv/UoBAAAY9noZYKufbW3DzqoRSf42yYdf6kSttctba9Naa9P22WefrVgiAAAAXdHLALs0yQEbrY9Psmyj9d2STE7yz1W1JMmRSW70QU4AAAD0p5cB9r4kr6mqiVW1Y5JTktz4652ttVWttbGttQmttQlJ7k5yYmttXg9rAgAAoKN6FmBba+uSnJ3ktiSPJLmutfbDqvpkVZ3Yq8cFAABg27RDL0/eWrs5yc2bbPv4Ztoe3ctaAAAA6LZeXkIMAAAAW40ACwAAQCcIsAAAAHSCAAsAAEAnCLAAAAB0ggALAABAJwiwAAAAdIIACwAAQCcIsAAAAHSCAAsAAEAnCLAAAAB0ggALAABAJwiwAAAAdIIACwAAQCcIsAAAAHSCAAsAAEAnCLAAAAB0ggALAABAJwiwAAAAdIIACwAAQCcIsAAAAHSCAAsAAEAnCLAAAAB0ggALAABAJwiwAAAAdIIACwAAQCcIsAAAAHSCAAsAAEAnCLAAAAB0ggALAABAJwiwAAAAdIIACwAAQCcIsAAAAHSCAAsAAEAnCLAAAAB0ggALAABAJwiwAAAAdIIACwAAQCcIsAAAAHSCAAsAAEAnCLAAAAB0ggALAABAJwiwAAAAdIIACwAAQCcIsAAAAHSCAAsAAEAnCLAAAAB0ggALAABAJwiwAAAAdIIACwAAQCcIsAAAAHSCAAsAAEAnCLAAAAB0ggALAABAJwiwAAAAdIIACwAAQCcIsAAAAHSCAAsAAEAnCLAAAAB0ggALAABAJwiwAAAAdIIACwAAQCcIsAAAAHSCAAsAAEAnCLAAAAB0ggALAABAJwiwAAAAdIIACwAAQCcIsAAAAHSCAAsAAEAnCLAAAAB0ggALAABAJwiwAAAAdIIACwAAQCcIsAAAAHSCAAsAAEAnCLAAAAB0ggALAABAJwiwAAAAdIIACwAAQCcIsAAAAHSCAAsAAEAnCLAAAAB0ggALAABAJwiwAAAAdIIACwAAQCcIsAAAAHSCAAsAAEAnCLAAAAB0ggALAABAJwiwAAAAdIIACwAAQCcIsAAAAHSCAAsAAEAnCLAAAAB0ggALAABAJwiwAAAAdIIACwAAQCcIsAAAAHSCAAsAAEAn9DTAVtVbqupHVbW4qs7rZ/9/rapFVbWwqm6vqgN7WQ8AAADd1bMAW1Ujk3wpyfFJJiWZXVWTNmn2QJJprbUpSb6e5KJe1QMAAEC39XIGdnqSxa21H7fWnktybZKZGzdorc1trT3Tt3p3kvE9rAcAAIAO62WAHZfkYvDXJwAAFjRJREFUZxutL+3btjnvS3JLfzuq6syqmldV85YvX74VSwQAAKArehlgq59trd+GVacmmZbk4v72t9Yub61Na61N22effbZiiQAAAHTFDj0899IkB2y0Pj7Jsk0bVdUfJvlYkje11p7tYT0AAAB0WC9nYO9L8pqqmlhVOyY5JcmNGzeoqsOSfCXJia21n/ewFgAAADquZwG2tbYuydlJbkvySJLrWms/rKpPVtWJfc0uTrJrkv9TVQ9W1Y2bOR0AAADbuV5eQpzW2s1Jbt5k28c3Wv7DXj4+AAAA245eXkIMAAAAW40ACwAAQCcIsAAAAHSCAAsAAEAnCLAAAAB0ggALAABAJwiwAAAAdIIACwAAQCcIsAAAAHSCAAsAAEAnCLAAAAB0ggALAABAJwiwAAAAdIIACwAAQCcIsAAAAHSCAAsAAEAnCLAAAAB0ggALAABAJwiwAAAAdIIACwAAQCcIsAAAAHSCAAsAAEAnCLAAAAB0ggALAABAJwiwAAAAdIIACwAAQCcIsAAAAHSCAAsAAEAnCLAAAAB0ggALAABAJwiwAAAAdIIACwAAQCcIsAAAAHSCAAsAAEAnCLAAAAB0ggALAABAJwiwAAAAdIIACwAAQCcIsAAAAHSCAAsAAEAnCLAAAAB0ggALAABAJwiwAAAAdIIACwAAQCcIsAAAAHSCAAsAAEAnCLAAAAB0ggALAABAJwiwAAAAdIIACwAAQCcIsAAAAHSCAAsAAEAnCLAAAAB0ggALAABAJwiwAAAAdIIACwAAQCcIsAAAAHSCAAsAAEAnCLAAAAB0ggALAABAJwiwAAAAdIIACwAAQCcIsAAAAHSCAAsAAEAnCLAAAAB0wg5DXQAAAMDW9Pzzz2fp0qVZu3btUJfCFowePTrjx4/PqFGjBnyMAAsAAGxTli5dmt122y0TJkxIVQ11OfSjtZYVK1Zk6dKlmThx4oCPcwkxAACwTVm7dm3GjBkjvA5jVZUxY8b81rPkAiwAALDNEV6Hv5fTRwIsAAAAnSDAAgAAbEUrVqzIoYcemkMPPTT7779/xo0bt2H9ueeeG9A5TjvttPzoRz/aYpsvfelLmTNnztYouTN8iBMAALDdm//Tlbn7xyty5KvG5PAD9/qdzjVmzJg8+OCDSZJPfOIT2XXXXfOXf/mXL2rTWktrLSNG9D+nePXVV7/k43zgAx/4nersIgEWAADYZl140w+zaNlTW2zz9Nrn8y///nR+1ZIRlbx2/92y2+jNf7XLpFfungvefvBvXcvixYvzjne8IzNmzMg999yTb3/727nwwgtz//33Z82aNZk1a1Y+/vGPJ0lmzJiRyy67LJMnT87YsWPz/ve/P7fcckte8YpX5Fvf+lb23XffnH/++Rk7dmw+9KEPZcaMGZkxY0a+973vZdWqVbn66qvz+te/Pr/85S/z7ne/O4sXL86kSZPy6KOP5oorrsihhx76otouuOCC3HzzzVmzZk1mzJiRL3/5y6mq/Ou//mve//73Z8WKFRk5cmS+8Y1vZMKECfmrv/qrXHPNNRkxYkROOOGEfOYzn/mtn4+XwyXEAADAdu2ptevyq7Z++Vdt/XqvLFq0KO973/vywAMPZNy4cfnsZz+befPmZcGCBfnOd76TRYsW/cYxq1atypve9KYsWLAgr3vd63LVVVf1e+7WWu69995cfPHF+eQnP5kk+eIXv5j9998/CxYsyHnnnZcHHnig32M/+MEP5r777stDDz2UVatW5dZbb02SzJ49O3/xF3+RBQsW5Ac/+EH23Xff3HTTTbnlllty7733ZsGCBfnwhz+8lZ6dl2YGFgAA2GYNZKZ0/k9X5l1X3J3n1/0qo3YYkc+fctjvfBnx5vz+7/9+/uAP/mDD+jXXXJMrr7wy69aty7Jly7Jo0aJMmjTpRcfsvPPOOf7445Mkhx9+eO68885+z33SSSdtaLNkyZIkyV133ZWPfOQjSZJDDjkkBx/c//Nx++235+KLL87atWvzxBNP5PDDD8+RRx6ZJ554Im9/+9uTJKNHj06SfPe7383pp5+enXfeOUmy9957v5yn4mURYAEAgO3a4QfulTlnHLnV7oHdkl122WXD8qOPPprPf/7zuffee7Pnnnvm1FNP7fd7UXfccccNyyNHjsy6df3PEO+0006/0aa19pI1PfPMMzn77LNz//33Z9y4cTn//PM31NHfV9201obsa4pcQgwAAGz3Dj9wr3zgmFf3NLxu6qmnnspuu+2W3XffPY899lhuu+22rf4YM2bMyHXXXZckeeihh/q9RHnNmjUZMWJExo4dm6effjrXX399kmSvvfbK2LFjc9NNNyVJ1q5dm2eeeSbHHXdcrrzyyqxZsyZJ8uSTT271ujfHDCwAAMAQmDp1aiZNmpTJkyfnVa96VY466qit/hjnnHNO3v3ud2fKlCmZOnVqJk+enD322ONFbcaMGZP3vOc9mTx5cg488MAcccQRG/bNmTMnf/Znf5aPfexj2XHHHXP99dfnhBNOyIIFCzJt2rSMGjUqb3/72/OpT31qq9fenxrIlPJwMm3atDZv3ryhLgMAABimHnnkkRx00EFDXcawsG7duqxbty6jR4/Oo48+muOOOy6PPvpodthheMxl9tdXVTW/tTatv/bDo2oAAAC2utWrV+fYY4/NunXr0lrLV77ylWETXl+O7lYOAADAFu25556ZP3/+UJex1fgQJwAAADpBgAUAAKATBFgAAAA6QYAFAACgEwRYAACArejoo4/Obbfd9qJtl156af78z/98i8ftuuuuSZJly5bl5JNP3uy5X+prRS+99NI888wzG9bf+ta35he/+MVASh/2BFgAAICf3Zvc+bn1v39Hs2fPzrXXXvuibddee21mz549oONf+cpX5utf//rLfvxNA+zNN9+cPffc82WfbzjxNToAAMC265bzkn9/aMttnn0qefzhpP0qqRHJfpOTnXbffPv9/3Ny/Gc3u/vkk0/O+eefn2effTY77bRTlixZkmXLlmXGjBlZvXp1Zs6cmZUrV+b555/Ppz/96cycOfNFxy9ZsiQnnHBCHn744axZsyannXZaFi1alIMOOihr1qzZ0O6ss87KfffdlzVr1uTkk0/OhRdemC984QtZtmxZjjnmmIwdOzZz587NhAkTMm/evIwdOzaXXHJJrrrqqiTJGWeckQ996ENZsmRJjj/++MyYMSM/+MEPMm7cuHzrW9/Kzjvv/KK6brrppnz605/Oc889lzFjxmTOnDnZb7/9snr16pxzzjmZN29eqioXXHBB3vnOd+bWW2/NRz/60bzwwgsZO3Zsbr/99i33wwAIsAAAwPZt7ar14TVZ/3vtqi0H2JcwZsyYTJ8+PbfeemtmzpyZa6+9NrNmzUpVZfTo0bnhhhuy++6754knnsiRRx6ZE088MVXV77m+/OUv5xWveEUWLlyYhQsXZurUqRv2feYzn8nee++dF154Iccee2wWLlyYc889N5dccknmzp2bsWPHvuhc8+fPz9VXX5177rknrbUcccQRedOb3pS99torjz76aK655pp89atfzZ/8yZ/k+uuvz6mnnvqi42fMmJG77747VZUrrrgiF110UT73uc/lU5/6VPbYY4889ND6NwpWrlyZ5cuX50//9E9zxx13ZOLEiXnyySdf9vO5MQEWAADYdm1hpnSDn92bfO3E5IXnkpE7Ju+8Ijlg+u/0sL++jPjXAfbXs56ttXz0ox/NHXfckREjRuTf/u3f8vjjj2f//ffv9zx33HFHzj333CTJlClTMmXKlA37rrvuulx++eVZt25dHnvssSxatOhF+zd111135Y/+6I+yyy67JElOOumk3HnnnTnxxBMzceLEHHrooUmSww8/PEuWLPmN45cuXZpZs2blsccey3PPPZeJEycmSb773e++6JLpvfbaKzfddFPe+MY3bmiz9957D/Sp2yL3wAIAANu3A6Yn77kxefPH1v/+HcNrkrzjHe/I7bffnvvvvz9r1qzZMHM6Z86cLF++PPPnz8+DDz6Y/fbbL2vXrt3iufqbnf3JT36Sv/mbv8ntt9+ehQsX5m1ve9tLnqe1ttl9O+2004blkSNHZt26db/R5pxzzsnZZ5+dhx56KF/5ylc2PF5r7Tdq7G/b1iDAAgAAHDA9ecOHt0p4TdZ/ovDRRx+d008//UUf3rRq1arsu+++GTVqVObOnZuf/vSnWzzPG9/4xsyZMydJ8vDDD2fhwoVJkqeeeiq77LJL9thjjzz++OO55ZZbNhyz22675emnn+73XN/85jfzzDPP5Je//GVuuOGGvOENbxjw37Rq1aqMGzcuSfK1r31tw/bjjjsul1122Yb1lStX5nWve12+//3v5yc/+UmSbLVLiAVYAACAHpg9e3YWLFiQU045ZcO2d73rXZk3b16mTZuWOXPm5LWvfe0Wz3HWWWdl9erVmTJlSi666KJMn74+YB9yyCE57LDDcvDBB+f000/PUUcdteGYM888M8cff3yOOeaYF51r6tSpee9735vp06fniCOOyBlnnJHDDjtswH/PJz7xifzxH/9x3vCGN7zo/trzzz8/K1euzOTJk3PIIYdk7ty52WeffXL55ZfnpJNOyiGHHJJZs2YN+HG2pLY0jTwcTZs2rb3U9x4BAADbr0ceeSQHHXTQUJfBAPTXV1U1v7U2rb/2ZmABAADoBAEWAACAThBgAQCAbU7XbpXcHr2cPhJgAQCAbcro0aOzYsUKIXYYa61lxYoVGT169G913A49qgcAAGBIjB8/PkuXLs3y5cuHuhS2YPTo0Rk/fvxvdYwACwAAbFNGjRqViRMnDnUZ9EBPLyGuqrdU1Y+qanFVndfP/p2q6h/79t9TVRN6WQ8AAADd1bMAW1Ujk3wpyfFJJiWZXVWTNmn2viQrW2uvTvK3Sf66V/UAAADQbb2cgZ2eZHFr7cetteeSXJtk5iZtZib5Wt/y15McW1XVw5oAAADoqF7eAzsuyc82Wl+a5IjNtWmtrauqVUnGJHli40ZVdWaSM/tWV1fVj3pS8dYzNpv8DQw5fTI86ZfhR58MP/pkeNIvw48+GZ70y/DThT45cHM7ehlg+5tJ3fRzrAfSJq21y5NcvjWKGgxVNa+1Nm2o6+A/6JPhSb8MP/pk+NEnw5N+GX70yfCkX4afrvdJLy8hXprkgI3WxydZtrk2VbVDkj2SPNnDmgAAAOioXgbY+5K8pqomVtWOSU5JcuMmbW5M8p6+5ZOTfK/5tmEAAAD60bNLiPvuaT07yW1JRia5qrX2w6r6ZJJ5rbUbk1yZ5B+qanHWz7ye0qt6BllnLnfejuiT4Um/DD/6ZPjRJ8OTfhl+9MnwpF+Gn073SZnwBAAAoAt6eQkxAAAAbDUCLAAAAJ0gwL5MVXVVVf28qh7ezP6qqi9U1eKqWlhVUwe7xu3NAPrk6KpaVVUP9v18fLBr3N5U1QFVNbeqHqmqH1bVB/tpY6wMsgH2i/EyiKpqdFXdW1UL+vrkwn7a7FRV/9g3Vu6pqgmDX+n2ZYD98t6qWr7RWDljKGrd3lTVyKp6oKq+3c8+Y2UIvESfGCdDoKqWVNVDfc/5vH72d/I1WC+/B3Zb93dJLkvy95vZf3yS1/T9HJHky32/6Z2/y5b7JEnubK2dMDjlkGRdkg+31u6vqt2SzK+q77TWFm3UxlgZfAPpl8R4GUzPJnlza211VY1KcldV3dJau3ujNu9LsrK19uqqOiXJXyeZNRTFbkcG0i9J8o+ttbOHoL7t2QeTPJJk9372GStDY0t9khgnQ+WY1toTm9nXyddgZmBfptbaHdnyd9bOTPL3bb27k+xZVb83ONVtnwbQJwyy1tpjrbX7+5afzvr/2MZt0sxYGWQD7BcGUd+//9V9q6P6fjb9lMWZSb7Wt/z1JMdWVQ1SidulAfYLg6yqxid5W5IrNtPEWBlkA+gThqdOvgYTYHtnXJKfbbS+NF4gDgev67sU7JaqOnioi9me9F3CdViSezbZZawMoS30S2K8DKq+y+8eTPLzJN9prW12rLTW1iVZlWTM4Fa5/RlAvyTJO/suv/t6VR0wyCVujy5N8t+T/Goz+42VwfdSfZIYJ0OhJfmnqppfVWf2s7+Tr8EE2N7p750+79oOrfuTHNhaOyTJF5N8c4jr2W5U1a5Jrk/yodbaU5vu7ucQY2UQvES/GC+DrLX2Qmvt0CTjk0yvqsmbNDFWhsAA+uWmJBNaa1OSfDf/MfNHD1TVCUl+3lqbv6Vm/WwzVnpkgH1inAyNo1prU7P+UuEPVNUbN9nfybEiwPbO0iQbv7s0PsmyIaqFJK21p359KVhr7eYko6pq7BCXtc3ru2/s+iRzWmvf6KeJsTIEXqpfjJeh01r7RZJ/TvKWTXZtGCtVtUOSPeK2iUGzuX5pra1orT3bt/rVJIcPcmnbm6OSnFhVS5Jcm+TNVfW/NmljrAyul+wT42RotNaW9f3+eZIbkkzfpEknX4MJsL1zY5J3932615FJVrXWHhvqorZnVbX/r++BqarpWf/vf8XQVrVt63u+r0zySGvtks00M1YG2UD6xXgZXFW1T1Xt2be8c5I/TPIvmzS7Mcl7+pZPTvK91tqwf6e8ywbSL5vcL3Zi1t9TTo+01v5Ha218a21CklOyfhycukkzY2UQDaRPjJPBV1W79H1QY6pqlyTHJdn0mzo6+RrMpxC/TFV1TZKjk4ytqqVJLsj6D3dIa+1/Jrk5yVuTLE7yTJLThqbS7ccA+uTkJGdV1boka5Kc4j+0njsqyX9J8lDfPWRJ8tEk/ykxVobQQPrFeBlcv5fka1U1MuvfLLiutfbtqvpkknmttRuz/k2Hf6iqxVk/m3TK0JW73RhIv5xbVSdm/ad7P5nkvUNW7XbMWBl+jJMht1+SG/rei94hyf9urd1aVe9Puv0arLweAQAAoAtcQgwAAEAnCLAAAAB0ggALAABAJwiwAAAAdIIACwAAQCcIsAAwCKrqhap6cKOf87biuSdU1abf7wcA2xzfAwsAg2NNa+3QoS4CALrMDCwADKGqWlJVf11V9/b9vLpv+4FVdXtVLez7/Z/6tu9XVTdU1YK+n9f3nWpkVX21qn5YVf9UVTsP2R8FAD0iwALA4Nh5k0uIZ22076nW2vQklyW5tG/bZUn+vrU2JcmcJF/o2/6FJN9vrR2SZGqSH/Ztf02SL7XWDk7yiyTv7PHfAwCDrlprQ10DAGzzqmp1a23XfrYvSfLm1tqPq2pUkn9vrY2pqieS/F5r7fm+7Y+11sZW1fIk41trz250jglJvtNae03f+keSjGqtfbr3fxkADB4zsAAw9NpmljfXpj/PbrT8QnzOBQDbIAEWAIberI1+/7++5R8kOaVv+V1J7upbvj3JWUlSVSOravfBKhIAhpp3ZwFgcOxcVQ9utH5ra+3XX6WzU1Xdk/VvLM/u23Zukquq6r8lWZ7ktL7tH0xyeVW9L+tnWs9K8ljPqweAYcA9sAAwhPrugZ3WWntiqGsBgOHOJcQAAAB0ghlYAAAAOsEMLAAAAJ0gwAIAANAJAiwAAACdIMACAADQCQIsAAAAnfD/AcsAow0SCTkdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1152x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, marker='.', label='Training acc')\n",
    "plt.plot(epochs, val_acc, marker='.', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification acc')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow embedding projector\n",
    "\n",
    "The Tensorflow embedding projector can be found [here](https://projector.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the embedding layer's weights from the trained model\n",
    "weights = model.layers[1].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the word Embeddings to tsv files\n",
    "# Two files: \n",
    "#     one contains the embedding labels (meta.tsv),\n",
    "#     one contains the embeddings (vecs.tsv)\n",
    "\n",
    "k = 0\n",
    "\n",
    "for word, token in word_index.items():\n",
    "    if k != 0:\n",
    "        with open('meta.tsv', 'w', encoding='utf-8') as out_m:\n",
    "            out_m.write('\\n')\n",
    "        with open('vecs.tsv', 'w', encoding='utf-8') as out_v:\n",
    "            out_v.write('\\n')\n",
    "    \n",
    "    with open('vecs.tsv', 'w', encoding='utf-8') as out_v:\n",
    "        out_v.write('\\t'.join([str(x) for x in weights[token]]))\n",
    "    with open('meta.tsv', 'w', encoding='utf-8') as out_m:\n",
    "        out_m.write(word)\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins import projector\n",
    "\n",
    "config = projector.ProjectorConfig()\n",
    "embedding_config = config.embeddings.add()\n",
    "\n",
    "embedding_config.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "embedding_config.metadata_path = 'meta.tsv'\n",
    "projector.visualize_embeddings('.', config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 19832), started 0:06:35 ago. (Use '!kill 19832' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5d71a31530f7cbfb\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5d71a31530f7cbfb\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
