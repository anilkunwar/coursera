{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On-policy Prediction with Approximation\n",
    "\n",
    "> This is the summary of lecture \"Prediction and Control with Function Approximation\" from Coursera.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Chanseok Kang\n",
    "- categories: [Python, Coursera, Reinforcement_Learning]\n",
    "- image: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving to Parameterized Functions\n",
    "\n",
    "### Parameterizing the Value Function\n",
    "\n",
    "$$ \\hat{v}(s, w) \\approx v_{\\pi}(s) $$\n",
    "\n",
    "For example, suppose we have the state space in 2D dimension, $X, Y$. Then we can express our value function like this,\n",
    "\n",
    "$$ \\hat{v}(s, w) \\doteq w_1 X + w_2 Y $$\n",
    "\n",
    "### Linear Value Function Approximation\n",
    "\n",
    "$$ \\begin{aligned} \\hat{v}(s, w) &\\doteq \\sum w_i x_i(s) \\\\ \n",
    "&= <w, x(s) > \\end{aligned} $$\n",
    "\n",
    "### Nonlinear Function Approximation\n",
    "\n",
    "![nfa](image/nfa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization and Discrimination\n",
    "\n",
    "### Categorizing methods based on generalization and discrimination\n",
    "\n",
    "![gene-desc](image/gene_desc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framing Value Estimation as Supervised Learning\n",
    "\n",
    "- The function approximator should be compatible with online updates (online means that the full dataset is available from the start and remains fixed throughout learning.)\n",
    "\n",
    "- Thf function approximator should be compatible with Bootstrapping (usually, the target in RL problem depends on feature weight $w$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Value Error Objective\n",
    "\n",
    "### The Mean Squared Value Error Objective\n",
    "\n",
    "$$\\overline{VE} =  \\sum_{s} \\mu(s)[v_{\\pi}(s) - \\hat{v}(s, w)]^2 $$\n",
    "\n",
    "Here, $\\mu(s)$ is the fraction of time we spend in $S$ when following policy $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Gradient Descent\n",
    "\n",
    "### Gradient - Derivatives in Multiple Dimensions\n",
    "\n",
    "$$ w \\doteq \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\dots \\\\ w_d \\end{bmatrix} \\nabla f \\doteq \\begin{bmatrix} \\frac{\\partial f}{\\partial w_1} \\\\ \\frac{\\partial f}{\\partial w_2} \\\\ \\dots \\\\ \\frac{\\partial f}{\\partial w_d} \\end{bmatrix} $$\n",
    "\n",
    "The sign indicates the direction to change $w$ in order to increase $f$. And the magnitude means how quickly $f$ changes.\n",
    "\n",
    "The gradient gives the direction of steepest ascent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Monte for Policy Evaluation\n",
    "\n",
    "### Gradient of the Mean Squared Value Error Objective\n",
    "\n",
    "$\\begin{aligned}  &\\nabla \\sum\\limits_{s \\in \\mathcal{S}} \\mu(s) [ v_{\\pi}(s) - \\hat{v}(s, w)]^2 \\\\ \n",
    "&= \\sum\\limits_{s \\in \\mathcal{S}} \\mu(s) \\nabla [v_{\\pi}(s) - \\hat{v}(s, w)]^2 \\\\\n",
    "&= - \\sum\\limits_{s \\in \\mathcal{S}} \\mu(s) 2 [v_{\\pi}(s) - \\hat{v}(s, w)] \\nabla \\hat{v}(s, w) \\end{aligned}$\n",
    "\n",
    "from previous definition,\n",
    "\n",
    "$ \\hat{v}(s, w) \\doteq < w, x(s) > $\n",
    "\n",
    "So, the gradient of value function is\n",
    "\n",
    "$ \\nabla \\hat{v}(s, w) = x(s) $\n",
    "\n",
    "As a result,\n",
    "\n",
    "$ \\Delta w \\propto \\sum\\limits_{s \\in \\mathcal{S}} \\mu(s) [v_{\\pi}(s) - \\hat{v}(s, w)] \\nabla \\hat{v}(s, w) $\n",
    "\n",
    "### Gradient Monte Carlo\n",
    "\n",
    "$$ w_{t+1} \\doteq w_{t} + \\alpha [ G_t - \\hat{v}(S_t, w_t) ] \\nabla \\hat{v}(S_t, w_t) $$\n",
    "\n",
    "Recall that\n",
    "\n",
    "$$ v_{\\pi}(s) \\doteq \\mathbb{E}_{\\pi}[G_t \\vert S_t = s] $$\n",
    "\n",
    "$$ \\begin{aligned} &\\mathbb{E}_{\\pi}\\big[2 [v_{\\pi}(S_t) - \\hat{v}(S_t, w)] \\nabla \\hat{v}(S_t, w) \\big] \\\\\n",
    "&= \\mathbb{E}_{\\pi} \\big[ 2[ G_t - \\hat{v}(S_t, w)] \\nabla \\hat{v}(S_t, w)\\big] \\end{aligned} $$\n",
    "\n",
    "### Gradient Monte Carlo Algorithm for Estimating $\\hat{v} \\approx v_{\\pi}$\n",
    "\n",
    "$\\begin{aligned}\n",
    "&\\text{Input: the policy } \\pi \\text{ to be evaluated } \\\\\n",
    "&\\text{Input: a differentiable function: } \\hat{v} : \\mathcal{S} \\times \\mathbb{R}^d \\rightarrow \\mathbb{R} \\\\\n",
    "&\\text{Algorithm parameter: step size } \\alpha > 0 \\\\\n",
    "&\\text{Initialize value-function weights } w \\in \\mathbb{R}^d \\text{ arbitrarily (e.g., } w = 0\\text{)} \\\\\n",
    "\\newline\n",
    "&\\text{Loop forever (for each episode):} \\\\\n",
    "&\\quad \\text{Generate an episode } S_0, A_0, R_1, S_1, A_1, \\dots, R_T, S_T \\text{ using } \\pi \\\\\n",
    "&\\quad \\text{Loop for each step of episode, } t=0, 1, \\dots, T-1: \\\\\n",
    "&\\qquad w \\leftarrow w + \\alpha[G_t - \\hat{v}(S_t, w)] \\nabla \\hat{v}(S_t, w)\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-Gradient TD for Policy Evaluation\n",
    "\n",
    "### The TD Update for Function Approximation\n",
    "\n",
    "$w \\leftarrow w + \\alpha [ U_t - \\hat{v}(S_t, w)] \\nabla \\hat{v}(S_t, w) $\n",
    "\n",
    "If $U_t$ is unbiased, $w$ will coverage to a local optimum. But we can replace $U_t$ with bootstrap target (or one-step TD target).\n",
    "\n",
    "$U_t \\doteq R_{t+1} + \\gamma \\hat{v}(S_{t+1}, w)$\n",
    "\n",
    "If we choose bootstrap target, this must be biased, since the TD target uses our current value estimate, which will likely not equal to the true value function. In this case, $w$ may not converge to a local optimum. But TD target usually has low variance than the sample of the return.\n",
    "\n",
    "### Semi-gradient method of TD\n",
    "\n",
    "$ \\nabla \\frac{1}{2} [U_t - \\hat{v}(S_t, w)]^2 \\\\\n",
    "= (U_t - \\hat{v}(S_t, w))(\\nabla U_t - \\nabla \\hat{v}(S_t, w)) \\neq -(U_t - \\hat{v}(S_t, w)) \\nabla \\hat{v}(S_t, w)$\n",
    "\n",
    "Right term is the TD update, and the inequality will be satisfied when $\\nabla U_t = 0$\n",
    "\n",
    "But for TD, $\\nabla U_t \\neq 0$\n",
    "\n",
    "$\\begin{aligned} \\nabla U_t &= \\nabla(R_{t+1} + \\gamma \\hat{v}(S_{t+1}, w)) \\\\\n",
    "&= \\gamma \\nabla \\hat{v}(S_{t+1}, w) \\\\\n",
    "&\\neq 0 \\end{aligned}$\n",
    "\n",
    "So we cannot apply Gradient Descent on TD Learning, directly. This is called **semi-gradient** method.\n",
    "\n",
    "### Semi-gradient TD(0) for estimating $\\hat{v} \\approx v_{\\pi}$\n",
    "\n",
    "$\\begin{aligned}\n",
    "&\\text{Input: the policy } \\pi \\text{ to be evaluated } \\\\\n",
    "&\\text{Input: a differentiable function } \\hat{v} : \\mathcal{S}^{+} \\times \\mathbb{R}^d \\to \\mathbb{R} \\text{ such that } \\hat{v}(\\text{terminal}, \\cdot) = 0 \\\\\n",
    "&\\text{Algorithm parameter: step size } \\alpha > 0 \\\\\n",
    "&\\text{Initialize value-function weights } w \\in \\mathbb{R}^d \\text{ arbitrarily (e.g., } w = 0) \\\\\n",
    "\\newline\n",
    "&\\text{Loop for each episode:} \\\\\n",
    "&\\quad \\text{Initialize } S \\\\\n",
    "&\\quad \\text{Loop for each step of episode: } \\\\\n",
    "&\\qquad \\text{Choose } A \\sim \\pi(\\cdot \\vert S) \\\\\n",
    "&\\qquad \\text{Take action } A, \\text{ observe } R, S' \\\\\n",
    "&\\qquad w \\leftarrow w + \\alpha [ R + \\gamma \\hat{v}(S', w) - \\hat{v}(S, w)] \\nabla \\hat{v}(S, w) \\\\\n",
    "&\\qquad S \\leftarrow S'\\\\\n",
    "&\\quad \\text{until } S \\text{ is terminal}\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building knowledge for AI agents with Reinforcement Learning (Doina Precup)\n",
    "\n",
    "- Focusing on two types of knowledge\n",
    "    - Procedural knowledge: policies, but also skills, goal-driven behavior\n",
    "    - Predictive, empirical knowledge: Value function, but also models\n",
    "    \n",
    "- Knowledge of RL agents should be...\n",
    "    - Expressive: able to represent many things, including abstractions like objects, space, people, and extended actions\n",
    "    - Learnable: from data without labels or supervision (for scalability)\n",
    "    - Composable: suitable for supporting planning / reasoning by assembling existing pieces.\n",
    "    \n",
    "- Two kinds of abstraction\n",
    "    - Temporal abstraction: reasoning and generalization over many different time scales\n",
    "    - State abstraction / function approximation: generalization over many different states\n",
    "    \n",
    "- Temporal abstraction and procedural knowledge - Options (Sutton, Precup & Singh, 1999)\n",
    "    - An option $w$ consists of 3 components\n",
    "        - An initiation set $I_w \\subseteq \\mathcal{S}$ (a.k.a precondition)\n",
    "        - A policy $\\pi_w: \\mathcal{S} \\times \\mathcal{A} \\to [0, 1], \\pi_{w}(a \\vert s)$ is the probability of taking $a$ in $s$ when following option $w$\n",
    "        - A termination condition $\\beta_{w} : \\mathcal{S} \\to [0, 1], \\beta_{w}(s)$ is the probability of termination the option $w$ upon entering $s$\n",
    "    - E.g., robot navigation: if there is no obstacle in front $(I_w)$, go forward $(\\pi_w)$ until you get too close to another object $(\\beta_w)$\n",
    "    - Inspired from macro-actions / behaviors in robotics / hybrid planning and control\n",
    "    \n",
    "- Option models\n",
    "    - Option model has two parts:\n",
    "        1. Expected Reward $r_w(s)$: the expected return during $w$'s execution from state $s$\n",
    "        2. Transition model $P_w(s' \\vert s)$: specifies where the agent will end up after the option / program execution and when termination will happen\n",
    "    - Models are predictions about the future, conditioned on the option being executed\n",
    "    \n",
    "- MDP + options = Semi-markov Decision Process\n",
    "    - Introducing options in an MDP induces a related semi-MDP\n",
    "    - Hence all planning and learning algorithm from classical MDPs transfer directly to options\n",
    "    - But planning and learning with options can be much faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
