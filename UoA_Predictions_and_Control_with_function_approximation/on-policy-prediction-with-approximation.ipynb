{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On-policy Prediction with Approximation\n",
    "\n",
    "> This is the summary of lecture \"Prediction and Control with Function Approximation\" from Coursera.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Chanseok Kang\n",
    "- categories: [Python, Coursera, Reinforcement_Learning]\n",
    "- image: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving to Parameterized Functions\n",
    "\n",
    "### Parameterizing the Value Function\n",
    "\n",
    "$$ \\hat{v}(s, w) \\approx v_{\\pi}(s) $$\n",
    "\n",
    "For example, suppose we have the state space in 2D dimension, $X, Y$. Then we can express our value function like this,\n",
    "\n",
    "$$ \\hat{v}(s, w) \\doteq w_1 X + w_2 Y $$\n",
    "\n",
    "### Linear Value Function Approximation\n",
    "\n",
    "$$ \\begin{aligned} \\hat{v}(s, w) &\\doteq \\sum w_i x_i(s) \\\\ \n",
    "&= <w, x(s) > \\end{aligned} $$\n",
    "\n",
    "### Nonlinear Function Approximation\n",
    "\n",
    "![nfa](image/nfa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization and Discrimination\n",
    "\n",
    "### Categorizing methods based on generalization and discrimination\n",
    "\n",
    "![gene-desc](image/gene_desc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framing Value Estimation as Supervised Learning\n",
    "\n",
    "- The function approximator should be compatible with online updates (online means that the full dataset is available from the start and remains fixed throughout learning.)\n",
    "\n",
    "- Thf function approximator should be compatible with Bootstrapping (usually, the target in RL problem depends on feature weight $w$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Value Error Objective\n",
    "\n",
    "### The Mean Squared Value Error Objective\n",
    "\n",
    "$$\\overline{VE} =  \\sum_{s} \\mu(s)[v_{\\pi}(s) - \\hat{v}(s, w)]^2 $$\n",
    "\n",
    "Here, $\\mu(s)$ is the fraction of time we spend in $S$ when following policy $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Gradient Descent\n",
    "\n",
    "### Gradient - Derivatives in Multiple Dimensions\n",
    "\n",
    "$$ w \\doteq \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\dots \\\\ w_d \\end{bmatrix} \\nabla f \\doteq \\begin{bmatrix} \\frac{\\partial f}{\\partial w_1} \\\\ \\frac{\\partial f}{\\partial w_2} \\\\ \\dots \\\\ \\frac{\\partial f}{\\partial w_d} \\end{bmatrix} $$\n",
    "\n",
    "The sign indicates the direction to change $w$ in order to increase $f$. And the magnitude means how quickly $f$ changes.\n",
    "\n",
    "The gradient gives the direction of steepest ascent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Monte for Policy Evaluation\n",
    "\n",
    "### Gradient of the Mean Squared Value Error Objective\n",
    "\n",
    "$\\begin{aligned}  &\\nabla \\sum\\limits_{s \\in \\mathcal{S}} \\mu(s) [ v_{\\pi}(s) - \\hat{v}(s, w)]^2 \\\\ \n",
    "&= \\sum\\limits_{s \\in \\mathcal{S}} \\mu(s) \\nabla [v_{\\pi}(s) - \\hat{v}(s, w)]^2 \\\\\n",
    "&= - \\sum\\limits_{s \\in \\mathcal{S}} \\mu(s) 2 [v_{\\pi}(s) - \\hat{v}(s, w)] \\nabla \\hat{v}(s, w) \\end{aligned}$\n",
    "\n",
    "from previous definition,\n",
    "\n",
    "$ \\hat{v}(s, w) \\doteq < w, x(s) > $\n",
    "\n",
    "So, the gradient of value function is\n",
    "\n",
    "$ \\nabla \\hat{v}(s, w) = x(s) $\n",
    "\n",
    "As a result,\n",
    "\n",
    "$ \\Delta w \\propto \\sum\\limits_{s \\in \\mathcal{S}} \\mu(s) [v_{\\pi}(s) - \\hat{v}(s, w)] \\nabla \\hat{v}(s, w) $\n",
    "\n",
    "### Gradient Monte Carlo\n",
    "\n",
    "$$ w_{t+1} \\doteq w_{t} + \\alpha [ G_t - \\hat{v}(S_t, w_t) ] \\nabla \\hat{v}(S_t, w_t) $$\n",
    "\n",
    "Recall that\n",
    "\n",
    "$$ v_{\\pi}(s) \\doteq \\mathbb{E}_{\\pi}[G_t \\vert S_t = s] $$\n",
    "\n",
    "$$ \\begin{aligned} &\\mathbb{E}_{\\pi}\\big[2 [v_{\\pi}(S_t) - \\hat{v}(S_t, w)] \\nabla \\hat{v}(S_t, w) \\big] \\\\\n",
    "&= \\mathbb{E}_{\\pi} \\big[ 2[ G_t - \\hat{v}(S_t, w)] \\nabla \\hat{v}(S_t, w)\\big] \\end{aligned} $$\n",
    "\n",
    "### Gradient Monte Carlo Algorithm for Estimating $\\hat{v} \\approx v_{\\pi}$\n",
    "\n",
    "$\\begin{aligned}\n",
    "&\\text{Input: the policy } \\pi \\text{ to be evaluated } \\\\\n",
    "&\\text{Input: a differentiable function: } \\hat{v} : \\mathcal{S} \\times \\mathbb{R}^d \\rightarrow \\mathbb{R} \\\\\n",
    "&\\text{Algorithm parameter: step size } \\alpha > 0 \\\\\n",
    "&\\text{Initialize value-function weights } w \\in \\mathbb{R}^d \\text{ arbitrarily (e.g., } w = 0\\text{)} \\\\\n",
    "\\newline\n",
    "&\\text{Loop forever (for each episode):} \\\\\n",
    "&\\quad \\text{Generate an episode } S_0, A_0, R_1, S_1, A_1, \\dots, R_T, S_T \\text{ using } \\pi \\\\\n",
    "&\\quad \\text{Loop for each step of episode, } t=0, 1, \\dots, T-1: \\\\\n",
    "&\\qquad w \\leftarrow w + \\alpha[G_t - \\hat{v}(S_t, w)] \\nabla \\hat{v}(S_t, w)\n",
    "\\end{aligned}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
