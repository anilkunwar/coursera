{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the right algorithm\n",
    "\n",
    "> This is the summary of lecture \"A complete reinforcement learning system (capstone)\" from Coursera.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Chanseok Kang\n",
    "- categories: [Python, Coursera, Reinforcement_Learning]\n",
    "- image: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected SARSA\n",
    "\n",
    "### The bellman equation for action-values\n",
    "\n",
    "$$ q_{\\pi}(s, a) = \\sum\\limits_{s', r} p(s', r \\vert s, a) \\big( r + \\gamma \\sum\\limits_a \\pi(a' \\vert s') q_{\\pi}(s', a')\\big) $$\n",
    "\n",
    "$$ \\text{SARSA: } \\quad Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\big( R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\big) \\\\\n",
    "S_{t+1} \\sim p(s', r \\vert s, a) \\\\\n",
    "A_{t+1} \\sim \\pi(a' \\vert s') $$\n",
    "\n",
    "### The expected SARSA algorithm\n",
    "\n",
    "$$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\big(R_{t+1} + \\gamma \\sum\\limits_{a'} \\pi(a' \\vert S_{t+1}) Q(S_{t+1}, a') - Q(S_t, A_t)\\big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "### The Q-learning algorithm\n",
    "\n",
    "- Q-learning (off-policy TD control) for estimating $\\pi \\approx \\pi_*$\n",
    "\n",
    "$\\begin{aligned} \n",
    "&\\text{Algorithm paramters: step size } \\alpha \\in (0, 1], \\text{ small } \\epsilon > 0 \\\\\n",
    "&\\text{Initialize } Q(s, a), \\text{ for all } s \\in \\mathcal{S}^+, a \\in \\mathcal{A}(s), \\text{ arbitrarily except that } Q(terminal, \\cdot) = 0 \\\\\n",
    "\\newline\n",
    "&\\text{Loop for each episode:} \\\\\n",
    "&\\quad \\text{Initialize } S \\\\\n",
    "&\\quad \\text{Loop for each step of episode:} \\\\\n",
    "&\\qquad \\text{Choose } A \\text{ from } S \\text{ using policy derived from } Q \\text{ (e.g., } \\epsilon\\text{-greedy)} \\\\\n",
    "&\\qquad \\text{Take action } A, \\text{ observe } R, S' \\\\\n",
    "&\\qquad Q(S, A) \\leftarrow Q(S, A) + \\alpha[R + \\gamma \\max_a Q(S', a) - Q(S, A)] \\\\\n",
    "&\\qquad S \\leftarrow S' \\\\\n",
    "&\\quad \\text{utill } S \\text{ is terminal} \\\\\n",
    "\\end{aligned}$\n",
    "\n",
    "### Revisiting Bellman equations\n",
    "\n",
    "$$ \\text{SARSA: } Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha\\big(R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)\\big) \\\\\n",
    "q_{\\pi}(s, a) = \\sum\\limits_{s', r}p(s', r \\vert s, a) \\big( r + \\gamma \\sum\\limits_{a'}\\pi(a' \\vert s') q_{\\pi}(s', a') \\big) $$\n",
    "\n",
    "$$ \\text{Q-learning: } Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\big( R_{t+1} + \\gamma \\max \\limits_{a'} Q(S_{t+1}, a') - Q(S_t, A_t) \\big) \\\\\n",
    "q_{*}(s, a) = \\sum\\limits_{s', r} p(s', r \\vert s, a) \\big( r+ \\gamma \\max\\limits_{a'} q_{\\pi}(s', a') \\big)$$\n",
    "\n",
    "### Connections to Dynamic Programming\n",
    "\n",
    "SARSA $ \\sim $ Policy Iteration\n",
    "\n",
    "Q-learning $ \\sim $ Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Reward- A New Way of Formulating Control Problems\n",
    "\n",
    "### The average reward objective\n",
    "\n",
    "$$ r(\\pi) \\doteq \\lim\\limits_{h \\to \\infty} \\frac{1}{h} \\sum_{t=1}^h \\mathbb{E}[R_t \\vert S_0, A_{0:t-1} \\sim \\pi ] $$\n",
    "\n",
    "$$ r(\\pi) = \\sum_s \\mu_{\\pi}(s) \\sum_a \\pi(a \\vert s) \\sum_{s', r} p(s', r \\vert s, a) r $$\n",
    "\n",
    "### Value Functions for Average reward\n",
    "\n",
    "$$ G_t = R_{t+1} - r(\\pi) + R_{t+2} - r(\\pi) + R_{t+3} - r(\\pi) + \\dots $$\n",
    "\n",
    "$$ q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}[G_t \\vert S_t = s, A_t = a ] $$\n",
    "\n",
    "$$ q_{\\pi}(s, a) = \\sum\\limits_{s', r} p(s', r \\vert s, a) \\big(r - r(\\pi) + \\sum\\limits_{a'}\\pi(a' \\vert s') q_{\\pi}(s', a') \\big) $$\n",
    "\n",
    "### Differential semi-gradient SARSA for estimating $\\hat{q} \\approx q_{*}$\n",
    "\n",
    "$\\begin{aligned}\n",
    "&\\text{Input: a differentiable action-value function parameterization } \\hat{q} : \\mathcal{S} \\times \\mathcal{A} \\times \\mathbb{R}^d \\to \\mathbb{R} \\\\\n",
    "&\\text{Algorithm parameters: step sizes } \\alpha, \\beta > 0 \\\\\n",
    "&\\text{Initialize value-function weights } w \\in \\mathbb{R}^d \\text{ arbitrarily (e.g., } w=0 \\text{)} \\\\\n",
    "&\\text{Initialize average reward estimate } \\bar{R} \\in \\mathbb{R} \\text{ arbitrarily (e.g., } \\bar{R} = 0 \\text{)} \\\\\n",
    "\\newline\n",
    "&\\text{Initialize state } S, \\text{ and action } A \\\\\n",
    "&\\text{Loop for each step: } \\\\\n",
    "&\\quad \\text{Take action } A, \\text{ observe } R, S' \\\\\n",
    "&\\quad \\text{Choose } A', \\text{ as a function } \\hat{q}(S', \\cdot, w) \\text{ (e.g., } \\epsilon\\text{-greedy)} \\\\\n",
    "&\\quad \\delta \\leftarrow R - \\bar{R} + \\hat{q}(S', A', w) - \\hat{q}(S, A, w) \\\\\n",
    "&\\quad \\bar{R} \\leftarrow \\bar{R} + \\beta \\delta \\\\\n",
    "&\\quad w \\leftarrow w + \\alpha \\delta \\nabla \\hat{q}(S, A, w) \\\\\n",
    "&\\quad S \\leftarrow S' \\\\\n",
    "&\\quad A \\leftarrow A' \\\\\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic Algorithm\n",
    "\n",
    "### Approximating the Action Value in the policy update\n",
    "\n",
    "Using one step boot-strapping,\n",
    "\n",
    "$ \\begin{aligned} \\theta_{t+1} &\\doteq + \\alpha \\nabla \\ln \\pi(A_t \\vert S_t, \\theta_t) q_{\\pi}(S_t \\vert A_t) \\\\\n",
    " &= \\theta_t + \\alpha \\nabla \\ln \\pi(A_t \\vert S_t, \\theta_t)[ R_{t+1} - \\bar{R} + \\hat{v}(S_{t+1}, w)] \\end{aligned} $\n",
    " \n",
    "![ac](image/actor_critic.png)\n",
    "\n",
    "### Subtracting the current state's value estimate\n",
    "\n",
    "$ \\theta_{t+1} \\doteq \\theta_t + \\alpha \\nabla \\ln \\pi(A_t \\vert S_t, \\theta_t) [ R_{t+1} - \\bar{R} + \\hat{v}(S_{t+1}, w) - \\hat{v}(S_t, w)] $\n",
    "\n",
    "$R_{t+1} - \\bar{R} + \\hat{v}(S_{t+1}, w) - \\hat{v}(S_t, w)$ is TD error $\\delta$, and it does not affect the expected update.\n",
    "\n",
    "### Adding a baseline\n",
    "\n",
    "$\\mathbb{E}_{\\pi}[ \\nabla \\ln \\pi(A_t \\vert S_t, \\theta_t) [R_{t+1} - \\bar{R} + \\hat{v}(S_{t+1}, w) - \\hat{v}(S_t, w)] \\vert S_t = s ] \\\\\n",
    "= \\mathbb{E}_{\\pi}[ \\nabla \\ln \\pi(A_t \\vert S_t, \\theta_t) [ R_{t+1} - \\bar{R} + \\hat{v}(S_{t+1}, w)] \\vert S_t = s] - \\mathbb{E}_{\\pi} [ \\nabla \\ln \\pi (A_t \\vert S_t, \\theta_t) \\hat{v}(S_t, w) \\vert S_t = s] $\n",
    "\n",
    "Baseline term ($\\mathbb{E}_{\\pi}[ \\nabla \\ln \\pi(A_t \\vert S_t, \\theta_t) \\hat{v}(S_t, w) \\vert S_t = s$) is 0. But we can reduce the update variance through it, which results in faster learning.\n",
    "\n",
    "### How the actor and the critic interact\n",
    "\n",
    "![](image/actor_critic2.png)\n",
    "\n",
    "$ \\theta_{t+1} \\doteq \\theta_t + \\alpha \\nabla \\ln \\pi (A_t \\vert S_t, \\theta_t) \\delta_t $\n",
    "\n",
    "### Actor-Critic (continuing), for estimating $\\pi_{\\theta} \\approx \\pi_{*}$\n",
    "\n",
    "$\\begin{aligned}\n",
    "&\\text{Input: a differentiable policy parameterization } \\pi(a \\vert s, \\theta) \\\\\n",
    "&\\text{Input: a differentiable state-value function parameterization } \\hat{v}(S, w) \\\\\n",
    "&\\text{Initialize } \\bar{R} \\in \\mathbb{R} \\text{ to } 0 \\\\\n",
    "&\\text{Initialize state-value weights } w \\in \\mathbb{R}^d \\text{ and policy parameter } \\theta \\in \\mathbb{R}^d \\text{ (e.g., to } 0 ) \\\\\n",
    "&\\text{Algorithm parameters: } \\alpha^w > 0, \\alpha^{\\theta} > 0, \\alpha^{\\bar{R}} > 0 \\\\\n",
    "&\\text{Initialize } S \\in \\mathcal{S} \\\\\n",
    "&\\text{Loop forever (for each time step):} \\\\\n",
    "&\\quad A \\sim \\pi(\\cdot \\vert S, \\theta) \\\\\n",
    "&\\quad \\text{Take action } A, \\text{ observe } S', R \\\\\n",
    "&\\quad \\delta \\leftarrow R - \\bar{R} + \\hat{v}(s', w) - \\hat{v}(S, w) \\\\ \n",
    "&\\quad \\bar{R} \\leftarrow \\bar{R} + \\alpha^{\\bar{R}} \\delta \\\\\n",
    "&\\quad w \\leftarrow w + \\alpha^w \\delta \\nabla \\hat{v}(S, w) \\\\\n",
    "&\\quad \\theta \\leftarrow \\theta + \\alpha^{\\theta} \\delta \\nabla \\ln \\pi(A \\vert S, \\theta) \\\\\n",
    "&\\quad S \\leftarrow S'\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
