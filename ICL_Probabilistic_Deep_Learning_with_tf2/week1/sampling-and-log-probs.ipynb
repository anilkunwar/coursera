{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling and log probs\n",
    "\n",
    "> In this post, we will learn the sampling and log probability of distribution. This is the summary of lecture \"Probabilistic Deep Learning with Tensorflow 2\" from Coursera.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Chanseok Kang\n",
    "- categories: [Python, Coursera, Deep_Learning, Tensorflow, Probability]\n",
    "- image: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow : v2.3.1\n",
      "Tensorflow probability : v0.11.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "tfd = tfp.distributions\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "\n",
    "print(\"Tensorflow : v\" + tf.__version__)\n",
    "print(\"Tensorflow probability : v\" + tfp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfp.distributions.Exponential(\"Exponential\", batch_shape=[2, 3], event_shape=[], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "exp = tfd.Exponential(rate=[[1., 1.5, 0.8], [0.3, 0.4, 1.8]])\n",
    "print(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfp.distributions.Independent(\"IndependentExponential\", batch_shape=[2], event_shape=[3], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "ind_exp = tfd.Independent(exp)\n",
    "print(ind_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2, 3), dtype=float32, numpy=\n",
       "array([[[0.9336921 , 1.1014012 , 0.95849776],\n",
       "        [1.859738  , 1.5860045 , 0.6979616 ]],\n",
       "\n",
       "       [[0.45026   , 0.08602523, 1.4929458 ],\n",
       "        [2.1974816 , 1.9354982 , 2.1604896 ]],\n",
       "\n",
       "       [[0.14067604, 0.13271774, 0.5351997 ],\n",
       "        [2.0170953 , 0.44593987, 0.6286139 ]],\n",
       "\n",
       "       [[0.6865209 , 1.2604585 , 0.3267352 ],\n",
       "        [7.852364  , 1.1664017 , 1.066288  ]]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample, batch, event_shape\n",
    "ind_exp.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfp.distributions.Exponential(\"Exponential\", batch_shape=[2, 1, 2, 3], event_shape=[], dtype=float32)\n",
      "tfp.distributions.Independent(\"IndependentExponential\", batch_shape=[2, 1], event_shape=[2, 3], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "rates = [\n",
    "    [[[1., 1.5, 0.8], [0.3, 0.4, 1.8]]],\n",
    "    [[[0.2, 0.4, 1.4], [0.4, 1.1, 0.9]]]\n",
    "]\n",
    "\n",
    "exp = tfd.Exponential(rate=rates)\n",
    "ind_exp = tfd.Independent(exp, reinterpreted_batch_ndims=2)\n",
    "\n",
    "print(exp)\n",
    "print(ind_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2, 2, 1, 2, 3), dtype=float32, numpy=\n",
       "array([[[[[[4.2317632e-01, 3.0489284e-01, 9.0440281e-02],\n",
       "           [2.3724518e+00, 2.0689788e+00, 1.1605508e+00]]],\n",
       "\n",
       "\n",
       "         [[[4.0992074e+00, 2.3008916e+00, 1.2091119e+00],\n",
       "           [2.8646481e+00, 1.0102176e-01, 2.6999652e-01]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[1.0713497e+00, 5.1539797e-02, 1.4517514e-01],\n",
       "           [2.7249320e+00, 8.7734246e-01, 2.6865232e-01]]],\n",
       "\n",
       "\n",
       "         [[[1.9503578e+01, 1.1081877e+00, 1.9015626e+00],\n",
       "           [1.7259812e+00, 4.4414407e-01, 4.0184917e+00]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "       [[[[[6.7009032e-01, 1.1803232e-02, 1.8138820e+00],\n",
       "           [8.3526602e+00, 1.4914610e+00, 6.1227548e-01]]],\n",
       "\n",
       "\n",
       "         [[[7.1173310e+00, 3.6172342e+00, 2.3583232e-01],\n",
       "           [7.0146952e+00, 6.4747000e-01, 1.5199455e+00]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[2.0505677e-01, 2.2933252e-01, 1.4931535e+00],\n",
       "           [3.0053165e+00, 5.2224517e+00, 4.1780460e-01]]],\n",
       "\n",
       "\n",
       "         [[[2.4266434e+00, 6.4278895e-01, 1.8892743e-01],\n",
       "           [3.5566807e-01, 1.1548362e+00, 1.2084136e+00]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "       [[[[[4.6471825e-01, 1.6992559e+00, 2.2435300e-01],\n",
       "           [2.8340337e+00, 6.1149251e-01, 9.8619956e-01]]],\n",
       "\n",
       "\n",
       "         [[[4.4793863e-02, 2.7272236e+00, 4.8408687e-01],\n",
       "           [2.7744882e+00, 3.2963245e+00, 5.2700025e-01]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[4.9334896e-01, 1.2744902e-01, 2.1591997e+00],\n",
       "           [4.6567683e+00, 4.7076397e+00, 5.2703358e-02]]],\n",
       "\n",
       "\n",
       "         [[[1.3996329e+00, 1.1809536e+00, 7.4351007e-01],\n",
       "           [4.3251753e+00, 4.3387700e-02, 1.5996653e+00]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "       [[[[[1.3861032e+00, 4.9527463e-01, 1.3929354e+00],\n",
       "           [9.9333918e-01, 1.9837898e+00, 3.4596512e-01]]],\n",
       "\n",
       "\n",
       "         [[[8.2568836e+00, 1.0451572e-01, 1.4219654e+00],\n",
       "           [2.2366199e+00, 1.6131952e-01, 1.4206251e+00]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[1.2562810e+00, 1.0124590e+00, 4.3909411e+00],\n",
       "           [1.0498550e+01, 3.9848027e-01, 5.9761599e-02]]],\n",
       "\n",
       "\n",
       "         [[[1.9843968e+00, 6.0017878e-01, 2.9966807e-01],\n",
       "           [6.4608369e+00, 5.3175431e-01, 1.3628409e+00]]]]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_exp.sample([4, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[-4.2501554],\n",
       "       [-5.3155975]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_exp.log_prob(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[-4.770155],\n",
       "       [-5.885597]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Broadcasting\n",
    "ind_exp.log_prob([[0.3, 0.5, 0.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 2, 1), dtype=float32, numpy=\n",
       "array([[[-4.6889906],\n",
       "        [-5.353305 ]],\n",
       "\n",
       "       [[-4.0902996],\n",
       "        [-5.5923157]],\n",
       "\n",
       "       [[-2.566263 ],\n",
       "        [-4.05225  ]],\n",
       "\n",
       "       [[-2.7958922],\n",
       "        [-4.4053006]],\n",
       "\n",
       "       [[-2.4176977],\n",
       "        [-4.136319 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_exp.log_prob(tf.random.uniform((5, 1, 1, 2, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tfp.distributions.MultivariateNormalDiag 'MultivariateNormalDiag' batch_shape=[3] event_shape=[2] dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make Multivariate Distribution\n",
    "normal_distributions = tfd.MultivariateNormalDiag(loc=[[0.5, 1], [0.1, 0], [0, 0.2]],\n",
    "                                                  scale_diag=[[2, 3], [1, 3], [4, 4]])\n",
    "normal_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 3, 2), dtype=float32, numpy=\n",
       "array([[[ 0.47599712,  2.3994164 ],\n",
       "        [ 1.1864761 ,  1.6828698 ],\n",
       "        [-4.9358287 ,  1.0669106 ]],\n",
       "\n",
       "       [[ 2.0767107 , -2.5989242 ],\n",
       "        [-0.13957322,  0.47952217],\n",
       "        [ 2.6879144 , -0.72602165]],\n",
       "\n",
       "       [[-0.21863753, -0.04923904],\n",
       "        [-0.567453  , -2.7127376 ],\n",
       "        [-0.53446406, -1.1026759 ]],\n",
       "\n",
       "       [[ 0.7530515 ,  2.6175892 ],\n",
       "        [-0.75680864, -3.3777373 ],\n",
       "        [ 2.1403995 ,  2.210537  ]],\n",
       "\n",
       "       [[-3.1377547 ,  0.89707947],\n",
       "        [-1.7728709 ,  6.2212987 ],\n",
       "        [-1.9061239 ,  0.20411325]]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_distributions.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tfp.distributions.MultivariateNormalDiag 'MultivariateNormalDiag' batch_shape=[2, 2] event_shape=[3] dtype=float32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multivariate Normal batched Distribution\n",
    "\n",
    "loc = [[[0.3, 1.5, 1.], [0.2, 0.4, 2.8]],\n",
    "       [[2., 2.3, 8], [1.4, 1, 1.3]]]\n",
    "scale_diag = [0.4, 1., 0.7]\n",
    "normal_distributions = tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale_diag)\n",
    "normal_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tfp.distributions.Independent 'IndependentMultivariateNormalDiag' batch_shape=[2] event_shape=[2, 3] dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use independent to move part of the batch shape\n",
    "ind_normal_distributions = tfd.Independent(normal_distributions,\n",
    "                                           reinterpreted_batch_ndims=1)\n",
    "ind_normal_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([5, 2, 2, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = ind_normal_distributions.sample(5)\n",
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([-10.663086, -73.64146 ], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [B, E] shaped input\n",
    "inp = tf.random.uniform((2, 2, 3))\n",
    "ind_normal_distributions.log_prob(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([-10.561053, -61.601433], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [E] shaped input (broadcasting over batch size)\n",
    "inp = tf.random.uniform((2, 3))\n",
    "ind_normal_distributions.log_prob(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9, 2), dtype=float32, numpy=\n",
       "array([[ -9.327406, -80.234634],\n",
       "       [ -9.255189, -71.55394 ],\n",
       "       [-10.053996, -70.705414],\n",
       "       [-12.143329, -66.72578 ],\n",
       "       [-10.251413, -66.373   ],\n",
       "       [-10.405237, -72.52043 ],\n",
       "       [-10.011557, -66.78318 ],\n",
       "       [-10.101498, -74.51347 ],\n",
       "       [ -8.64311 , -72.493744]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [S, B, E] shaped input (broadcasting over samples)\n",
    "inp = tf.random.uniform((9, 2, 2, 3))\n",
    "ind_normal_distributions.log_prob(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 2), dtype=float32, numpy=\n",
       "array([[ -9.530364, -67.75916 ],\n",
       "       [-10.439697, -83.35648 ],\n",
       "       [ -9.748083, -73.85253 ],\n",
       "       [ -9.404794, -62.255707],\n",
       "       [-10.059189, -81.1289  ]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [S, b, e] shaped input, where [b, e] is broadcastable over [B, E]\n",
    "inp = tf.random.uniform((5, 1, 2, 1))\n",
    "ind_normal_distributions.log_prob(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a function get_data which:\n",
    "#   1) Fetches the 20 newsgroup dataset\n",
    "#   2) Performs a word count on the articles and binarizes the result\n",
    "#   3) Returns the data as a numpy matrix with the labels\n",
    "\n",
    "def get_data(categories):\n",
    "    \n",
    "    newsgroups_train_data = fetch_20newsgroups(data_home='20_Newsgroup_Data/',\n",
    "                                               subset='train', categories=categories)\n",
    "    newsgroups_test_data = fetch_20newsgroups(data_home='20_Newsgroup_Data/',\n",
    "                                              subset='test', categories=categories)\n",
    "\n",
    "    n_documents = len(newsgroups_train_data['data'])\n",
    "    count_vectorizer = CountVectorizer(input='content', binary=True,max_df=0.25, min_df=1.01/n_documents)\n",
    "    \n",
    "    train_binary_bag_of_words = count_vectorizer.fit_transform(newsgroups_train_data['data'])\n",
    "    test_binary_bag_of_words = count_vectorizer.transform(newsgroups_test_data['data']) \n",
    "\n",
    "    return (train_binary_bag_of_words.todense(), newsgroups_train_data['target']),  (test_binary_bag_of_words.todense(), newsgroups_test_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to conduct Laplace smoothing. This adds a base level of probability for a given feature\n",
    "# to occur in every class.\n",
    "\n",
    "def laplace_smoothing(labels, binary_data, n_classes):\n",
    "    # Compute the parameter estimates (adjusted fraction of documents in class that contain word)\n",
    "    n_words = binary_data.shape[1]\n",
    "    alpha = 1 # parameters for Laplace smoothing\n",
    "    theta = np.zeros([n_classes, n_words]) # stores parameter values - prob. word given class\n",
    "    for c_k in range(n_classes): # 0, 1, ..., 19\n",
    "        class_mask = (labels == c_k)\n",
    "        N = class_mask.sum() # number of articles in class\n",
    "        theta[c_k, :] = (binary_data[class_mask, :].sum(axis=0) + alpha)/(N + alpha*2)\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a subset of the 20 newsgroup dataset\n",
    "\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = get_data(categories=categories)\n",
    "smoothed_counts = laplace_smoothing(labels=train_labels, binary_data=train_data, n_classes=len(categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To now make our NB classifier we need to build three functions:\n",
    "* Compute the class priors\n",
    "* Build our class conditional distributions\n",
    "* Put it all together and classify our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function which computes the prior probability of every class based on frequency of occurence in \n",
    "# the dataset\n",
    "\n",
    "def class_priors(n_classes, labels):\n",
    "    counts = np.zeros(n_classes)\n",
    "    for c_k in range(n_classes):\n",
    "        counts[c_k] = np.sum(np.where(labels==c_k, 1, 0))\n",
    "    priors = counts / np.sum(counts)\n",
    "    print('The class priors are {}'.format(priors))\n",
    "    return priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The class priors are [0.2359882  0.28711898 0.29154376 0.18534907]\n"
     ]
    }
   ],
   "source": [
    "priors = class_priors(n_classes=len(categories), labels=train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tfp.distributions.Independent 'IndependentBernoulli' batch_shape=[4] event_shape=[17495] dtype=int32>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we will do a function that given the feature occurence counts returns a Bernoulli distribution of \n",
    "# batch_shape=number of classes and event_shape=number of features.\n",
    "def make_distribution(probs):\n",
    "    batch_of_bernoullis = tfd.Bernoulli(probs=probs)\n",
    "    dist = tfd.Independent(batch_of_bernoullis,\n",
    "                           reinterpreted_batch_ndims=1)\n",
    "    \n",
    "    return dist\n",
    "\n",
    "tf_dist = make_distribution(smoothed_counts)\n",
    "tf_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final function predict_sample which given the distribution, a test sample, and the class priors:\n",
    "#   1) Computes the class conditional probabilities given the sample\n",
    "#   2) Forms the joint likelihood\n",
    "#   3) Normalises the joint likelihood and returns the log prob\n",
    "def predict_sample(dist, sample, priors):\n",
    "    cond_probs = dist.log_prob(sample)\n",
    "    joint_likelihood = tf.add(np.log(priors), cond_probs)\n",
    "    norm_factor = tf.math.reduce_logsumexp(joint_likelihood, axis=-1, keepdims=True)\n",
    "    log_prob = joint_likelihood - norm_factor\n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=\n",
       "array([-6.1736343e+01, -1.5258789e-05, -1.1620026e+01, -6.3327866e+01],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting one example from our test data\n",
    "log_probs = predict_sample(tf_dist, test_data[0], priors)\n",
    "log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1  0.7848499112849504\n"
     ]
    }
   ],
   "source": [
    "# Loop over our test data and classify.\n",
    "probabilities = []\n",
    "for sample, label in zip(test_data, test_labels):\n",
    "    probabilities.append(tf.exp(predict_sample(tf_dist, sample, priors)))\n",
    "\n",
    "probabilities = np.asarray(probabilities)\n",
    "predicted_classes = np.argmax(probabilities, axis =-1)\n",
    "print('f1 ', f1_score(test_labels, predicted_classes, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 from sklean  0.7848499112849504\n"
     ]
    }
   ],
   "source": [
    "# Make a Bernoulli Naive Bayes classifier using sklearn with the same level of alpha smoothing. \n",
    "clf = BernoulliNB(alpha=1)\n",
    "clf.fit(train_data, train_labels)\n",
    "pred = clf.predict(test_data)\n",
    "print('f1 from sklean ', f1_score(test_labels, pred, average='macro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
