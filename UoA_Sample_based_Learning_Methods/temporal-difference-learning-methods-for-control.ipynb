{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference Learning methods for Control\n",
    "\n",
    "> This is the summary of lecture \"Sample-based Learning Methods\" from Coursera.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Chanseok Kang\n",
    "- categories: [Python, Coursera, Reinforcement_Learning]\n",
    "- image: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA - GPI with TD\n",
    "\n",
    "### Generalized Policy Iteration\n",
    "\n",
    "![gpi](image/gpi.png)\n",
    "\n",
    "### SARSA\n",
    "\n",
    "The acronym describes the data used in the updates.\n",
    "\n",
    "$$ S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1} $$\n",
    "\n",
    "$$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\big(R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)\\big) $$\n",
    "\n",
    "Recall the TD update,\n",
    "\n",
    "$$ V((S_t) \\leftarrow V(S_t) + \\alpha \\big( R_{t+1} + \\gamma (V(S_{t+1}) - V(S_t)\\big) $$\n",
    "\n",
    "As a result, **SARSA** is the GPI algorithm that uses TD for policy evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Q-learning?\n",
    "\n",
    "### Q-learning (off-policy TD Control) for estimating $\\pi \\approx \\pi_{*}$\n",
    "\n",
    "$\\begin{aligned}\n",
    "&\\text{Algorithm parameters: step size } \\alpha \\in (0, 1], \\text{ small } \\epsilon > 0 \\\\\n",
    "&\\text{Initialize } Q(s, a), \\text{ for all } s \\in \\mathcal{S}^{+}, a \\in \\mathcal{A}(s), \\text{ arbitrarily except that } Q(terminal, \\cdot) = 0 \\\\\n",
    "\\newline\n",
    "&\\text{Loop for each episode: } \\\\\n",
    "&\\quad \\text{Initialize } S\\\\\n",
    "&\\quad \\text{Loop for each step of episode:} \\\\\n",
    "&\\qquad \\text{Choose } A \\text{ from } S \\text{ using policy derived from } Q \\text{ (e.g., } \\epsilon-\\text{greedy)} \\\\\n",
    "&\\qquad \\text{Take action } A, \\text{ observe } R, S' \\\\\n",
    "&\\qquad Q(S, A) \\leftarrow Q(S, A) + \\alpha [R + \\gamma \\max_{a} Q(S', a) - Q(S, A)] \\\\\n",
    "&\\qquad S \\leftarrow S' \\\\\n",
    "&\\quad \\text{until } S \\text{ is terminal} \\\\\n",
    "\\end{aligned}$\n",
    "\n",
    "### Revisiting Bellman equations\n",
    "\n",
    "- SARSA: SARSA is sample-based version of policy iteration, and it uses standard bellman equation for action value.\n",
    "\n",
    "$q_{\\pi}(s, a) = \\sum_{s', r} p(s', r \\vert s, a) \\big(r + \\gamma \\sum_{a'}\\pi(a' \\vert s') q_{\\pi}(s', a')\\big) \\\\\n",
    "Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\big(R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)\\big) $\n",
    "\n",
    "- Q-learning: Q-learning is sample-based version of value iteration, and it uses bellman optimality equation.\n",
    "\n",
    "$q_{*}(s, a) = \\sum_{s', r} p(s', r \\vert s, a) \\big(r + \\gamma \\max_{a'} q_{*}(s', a') \\big) \\\\\n",
    "Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\big( R_{t+1} + \\gamma \\max_{a'} Q(S_{t+1}, a') - Q(S_t, A_t)\\big) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is Q-learning off-policy?\n",
    "\n",
    "SARSA selects its action from current policy ($\\pi$). In this case, the target policy and the behavior policy is the same.(**on-policy**) But in Q-learning, action is selected from optimal policy($\\pi_{*}$), that is, the target policy is not the same as the behavior policy. This is called **off-policy**. But when taking the expection of state-action value in Q-learning, it is same as current policy, so it doesn't require any importance sampling techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
