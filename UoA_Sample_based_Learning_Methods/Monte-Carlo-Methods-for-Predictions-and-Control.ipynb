{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte-Carlo Methods for Predictions and Control\n",
    "\n",
    "> This is the summary of lecture \"Sample-based Learning Methods\" from Coursera.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Chanseok Kang\n",
    "- categories: [Python, Coursera, Reinforcement_Learning]\n",
    "- image: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Monte Carlo?\n",
    "\n",
    "To use a pure Dynamic Programming approach, the agent needs to know the environment's transition probabilities\n",
    "\n",
    "- In some problems, we don't know the environment transition probabilities\n",
    "- The computation can be error-prone and tedious\n",
    "\n",
    "**Monte-Carlo**(MC for abbreviation) methods estimate values by averaging over a large number of random samples.\n",
    "\n",
    "### MC prediction, for estimating $V \\approx v_{\\pi} $\n",
    "\n",
    "$ \\begin{aligned} \n",
    "    &\\text{Input: } \\text{a policy } \\pi \\text{ to be evaluated} \\\\ \n",
    "    &\\text{Initialize: } \\\\\n",
    "    &\\quad V(s) \\in \\mathcal{R} \\text{, arbitrarily, for all } s \\in \\mathbb{S} \\\\\n",
    "    &\\quad Returns(s) \\leftarrow \\text{ an empty list, for all } s \\in \\mathbb{S} \\\\\n",
    "    \\newline\n",
    "    &\\text{Loop forever (for each episode):} \\\\\n",
    "    &\\quad \\text{Generate an episode following } \\pi: S_0, A_0, R_1, S_1, A_1, R_2, \\dots, S_{T-1}, A_{T-1}, R_T \\\\\n",
    "    &\\quad G \\leftarrow 0 \\\\\n",
    "    &\\quad \\text{Loop for each step of episode, } t = T-1, T-2, \\dots, 0: \\\\\n",
    "    &\\qquad G \\leftarrow \\gamma G + R_{t+1} \\\\\n",
    "    &\\qquad \\text{Append } G \\text{ to } Returns(S_t) \\\\\n",
    "    &\\qquad V(S_t) \\leftarrow \\text{ average} (Returns(S_t)) \\\\\n",
    "\\end{aligned} $\n",
    "\n",
    "### Increamental Update\n",
    "\n",
    "$ NewEstimate \\leftarrow OldEstimate + StepSize [ Target - OldEstimate] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Monte Carlo for Prediction\n",
    "\n",
    "### Example - Blackjack\n",
    "\n",
    "Collect cards so that their sum is as large as possible without exceeding 21.\n",
    "\n",
    "#### Problem Formulation\n",
    "- Undiscounted MDP where each game of blackjack corresponds to an episode\n",
    "- Reward: -1 for a loss, 0 for a draw, and 1 for a win\n",
    "- Action: Hit or Stick\n",
    "- States (200 in total):\n",
    "    - Whether the player has a usable ace (Yes or No)\n",
    "    - The sum of the player's cards (12-21)\n",
    "    - The card the dealer shows (Ace-10)\n",
    "- Cards are dealt from a deck with replacement\n",
    "- Policy: Stops requesting cards when the player's sum is 20 or 21\n",
    "\n",
    "### Implications of Monte Carlo learning\n",
    "\n",
    "- We do not need to keep a **large model** of the environment.\n",
    "\n",
    "- We are estimating the value of an individual state **independently** of the values of other states\n",
    "\n",
    "- The **computation** needed to update the value of each state does not depend on the size of the MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Monte Carlo for Action Values\n",
    "\n",
    "### Monte Carlo methods in RL\n",
    "\n",
    "$ v_{\\pi}(s) \\doteq \\mathbb{E}_{\\pi}[G_t \\vert S_t = s] \\\\\n",
    "q_{\\pi}(s, a) \\doteq \\mathbb{E}_{\\pi}[G_t \\vert S_t = s, A_t = a] $\n",
    "\n",
    "For exploration in MC, we usually use **exploring starts** for defining initial state and action. Or we can use **Epsilon-Greedy strategy** for stochastic policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Monte Carlo methods for Generalized Policy Iteration\n",
    "\n",
    "### Monte Carlo Generalized Policy Iteration\n",
    "\n",
    "![gpi](image/gpi.png)\n",
    "\n",
    "When gather policies ($\\pi_0 \\rightarrow \\pi_1 \\rightarrow \\pi_2 \\dots$)\n",
    "\n",
    "- Improvement:\n",
    "\n",
    "$\\pi_{k+1}(s) \\doteq \\arg \\max_{a} q_{\\pi_k}(s, a)$\n",
    "\n",
    "- Evaluation:\n",
    "Use Monte Carlo Prediction\n",
    "\n",
    "### Monte Carlo methods with ES (Exploring Starts), for estimating $\\pi \\approx \\pi_{*}$\n",
    "\n",
    "$\\begin{aligned} & \\text{Initialize:} \\\\ \n",
    " & \\quad \\pi(s) \\in \\mathcal{A}(s) \\text{ (arbitrarily), for all } s \\in \\mathcal{S} \\\\\n",
    " & \\quad Q(s, a) \\in \\mathbb{R} \\text{ (arbitrarily), for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}(s) \\\\\n",
    " & \\quad Returns(s, a) \\leftarrow \\text{ empty list, for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}(s) \\\\\n",
    " \\newline\n",
    " & \\text{Loop forever (for each episode):} \\\\\n",
    " & \\quad \\text{Choose } S_0 \\in \\mathcal{S}, A_0 \\in \\mathcal{A}(S_0) \\text{ randomly such that all pairs have probability } > 0 \\\\\n",
    " & \\quad \\text{Generate an episode from } S_0, A_0, \\text{following } \\pi: S_0, A_0, R_1, \\dots, S_{T-1}, A_{T-1}, R_T \\\\\n",
    " & \\quad G \\leftarrow 0 \\\\\n",
    " & \\quad \\text{Loop for each step of episode, } t=T-1, T-2, \\dots, 0: \\\\\n",
    " & \\qquad G \\leftarrow \\gamma G + R_{t+1} \\\\\n",
    " & \\qquad \\text{Append } G \\text{ to } Returns(S_t, A_t) \\\\\n",
    " & \\qquad Q(S_t, A_t) \\leftarrow \\text{ average}(Returns(S_t, A_t)) \\\\\n",
    " & \\qquad \\pi(S_t) \\leftarrow \\arg \\max_a Q(S_t, a) \n",
    " \\end{aligned} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
