{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte-Carlo Methods for Predictions and Control\n",
    "\n",
    "> This is the summary of lecture \"Sample-based Learning Methods\" from Coursera.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Chanseok Kang\n",
    "- categories: [Python, Coursera, Reinforcement_Learning]\n",
    "- image: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Monte Carlo?\n",
    "\n",
    "To use a pure Dynamic Programming approach, the agent needs to know the environment's transition probabilities\n",
    "\n",
    "- In some problems, we don't know the environment transition probabilities\n",
    "- The computation can be error-prone and tedious\n",
    "\n",
    "**Monte-Carlo**(MC for abbreviation) methods estimate values by averaging over a large number of random samples.\n",
    "\n",
    "### MC prediction, for estimating $V \\approx v_{\\pi} $\n",
    "\n",
    "$ \\begin{aligned} \n",
    "    &\\text{Input: } \\text{a policy } \\pi \\text{ to be evaluated} \\\\ \n",
    "    &\\text{Initialize: } \\\\\n",
    "    &\\quad V(s) \\in \\mathcal{R} \\text{, arbitrarily, for all } s \\in \\mathbb{S} \\\\\n",
    "    &\\quad Returns(s) \\leftarrow \\text{ an empty list, for all } s \\in \\mathbb{S} \\\\\n",
    "    \\newline\n",
    "    &\\text{Loop forever (for each episode):} \\\\\n",
    "    &\\quad \\text{Generate an episode following } \\pi: S_0, A_0, R_1, S_1, A_1, R_2, \\dots, S_{T-1}, A_{T-1}, R_T \\\\\n",
    "    &\\quad G \\leftarrow 0 \\\\\n",
    "    &\\quad \\text{Loop for each step of episode, } t = T-1, T-2, \\dots, 0: \\\\\n",
    "    &\\qquad G \\leftarrow \\gamma G + R_{t+1} \\\\\n",
    "    &\\qquad \\text{Append } G \\text{ to } Returns(S_t) \\\\\n",
    "    &\\qquad V(S_t) \\leftarrow \\text{ average} (Returns(S_t)) \\\\\n",
    "\\end{aligned} $\n",
    "\n",
    "### Increamental Update\n",
    "\n",
    "$ NewEstimate \\leftarrow OldEstimate + StepSize [ Target - OldEstimate] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Monte Carlo for Prediction\n",
    "\n",
    "### Example - Blackjack\n",
    "\n",
    "Collect cards so that their sum is as large as possible without exceeding 21.\n",
    "\n",
    "#### Problem Formulation\n",
    "- Undiscounted MDP where each game of blackjack corresponds to an episode\n",
    "- Reward: -1 for a loss, 0 for a draw, and 1 for a win\n",
    "- Action: Hit or Stick\n",
    "- States (200 in total):\n",
    "    - Whether the player has a usable ace (Yes or No)\n",
    "    - The sum of the player's cards (12-21)\n",
    "    - The card the dealer shows (Ace-10)\n",
    "- Cards are dealt from a deck with replacement\n",
    "- Policy: Stops requesting cards when the player's sum is 20 or 21\n",
    "\n",
    "### Implications of Monte Carlo learning\n",
    "\n",
    "- We do not need to keep a **large model** of the environment.\n",
    "\n",
    "- We are estimating the value of an individual state **independently** of the values of other states\n",
    "\n",
    "- The **computation** needed to update the value of each state does not depend on the size of the MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Monte Carlo for Action Values\n",
    "\n",
    "### Monte Carlo methods in RL\n",
    "\n",
    "$ v_{\\pi}(s) \\doteq \\mathbb{E}_{\\pi}[G_t \\vert S_t = s] \\\\\n",
    "q_{\\pi}(s, a) \\doteq \\mathbb{E}_{\\pi}[G_t \\vert S_t = s, A_t = a] $\n",
    "\n",
    "For exploration in MC, we usually use **exploring starts** for defining initial state and action. Or we can use **Epsilon-Greedy strategy** for stochastic policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Monte Carlo methods for Generalized Policy Iteration\n",
    "\n",
    "### Monte Carlo Generalized Policy Iteration\n",
    "\n",
    "![gpi](image/gpi.png)\n",
    "\n",
    "When gather policies ($\\pi_0 \\rightarrow \\pi_1 \\rightarrow \\pi_2 \\dots$)\n",
    "\n",
    "- Improvement:\n",
    "\n",
    "$\\pi_{k+1}(s) \\doteq \\arg \\max_{a} q_{\\pi_k}(s, a)$\n",
    "\n",
    "- Evaluation:\n",
    "Use Monte Carlo Prediction\n",
    "\n",
    "### Monte Carlo methods with ES (Exploring Starts), for estimating $\\pi \\approx \\pi_{*}$\n",
    "\n",
    "$\\begin{aligned} & \\text{Initialize:} \\\\ \n",
    " & \\quad \\pi(s) \\in \\mathcal{A}(s) \\text{ (arbitrarily), for all } s \\in \\mathcal{S} \\\\\n",
    " & \\quad Q(s, a) \\in \\mathbb{R} \\text{ (arbitrarily), for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}(s) \\\\\n",
    " & \\quad Returns(s, a) \\leftarrow \\text{ empty list, for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}(s) \\\\\n",
    " \\newline\n",
    " & \\text{Loop forever (for each episode):} \\\\\n",
    " & \\quad \\text{Choose } S_0 \\in \\mathcal{S}, A_0 \\in \\mathcal{A}(S_0) \\text{ randomly such that all pairs have probability } > 0 \\\\\n",
    " & \\quad \\text{Generate an episode from } S_0, A_0, \\text{following } \\pi: S_0, A_0, R_1, \\dots, S_{T-1}, A_{T-1}, R_T \\\\\n",
    " & \\quad G \\leftarrow 0 \\\\\n",
    " & \\quad \\text{Loop for each step of episode, } t=T-1, T-2, \\dots, 0: \\\\\n",
    " & \\qquad G \\leftarrow \\gamma G + R_{t+1} \\\\\n",
    " & \\qquad \\text{Append } G \\text{ to } Returns(S_t, A_t) \\\\\n",
    " & \\qquad Q(S_t, A_t) \\leftarrow \\text{ average}(Returns(S_t, A_t)) \\\\\n",
    " & \\qquad \\pi(S_t) \\leftarrow \\arg \\max_a Q(S_t, a) \n",
    " \\end{aligned} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon-soft policies\n",
    "\n",
    "We cannot apply Exploring Start in every case. Instead, we can use epsilon-soft policy instead of using exploring start in epsilon-greedy policy.\n",
    "\n",
    "### Epsilon-Greedy policies\n",
    "\n",
    "![eg](image/e-greedy.png)\n",
    "\n",
    "$\\epsilon -$greedy policy is deterministic policy.\n",
    "\n",
    "### Epsilon-soft policies\n",
    "\n",
    "![es](image/e-soft.png)\n",
    "\n",
    "As you can see, $\\epsilon -$soft policies are always stochastic. All actions have a probability of at least $\\epsilon$ over the number of actions.\n",
    "\n",
    "### MC Control (for $\\epsilon-$soft policies), estimates $\\pi \\approx \\pi_{*}$\n",
    "\n",
    "$\\begin{aligned}\n",
    "&\\text{Algorithm parameter: small } \\epsilon > 0 \\\\\n",
    "&\\text{Initialize: } \\\\\n",
    "&\\quad \\pi \\leftarrow \\text{ an arbitrary } \\epsilon-\\text{soft policy} \\\\\n",
    "&\\quad Q(s, a) \\in \\mathcal{R} \\text{ (arbitrary), for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}(s) \\\\\n",
    "&\\quad Returns(s, a) \\leftarrow \\text{ empty list, for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}(s) \\\\\n",
    "\\newline\n",
    "&\\text{Repeat forever (for each episode):} \\\\\n",
    "&\\quad \\text{Generate an episode following } \\pi: S_0, A_0, R_1, \\dots, S_{T-1}, A_{T-1}, R_T \\\\\n",
    "&\\quad G \\leftarrow 0 \\\\\n",
    "&\\quad \\text{Loop for each step of episode, } t = T-1, T-2, \\dots, 0: \\\\\n",
    "&\\qquad G \\leftarrow \\gamma G + R_{t+1} \\\\\n",
    "&\\qquad \\text{Append } G \\text{ to } Returns(S_t, A_t) \\\\\n",
    "&\\qquad Q(S_t, A_t) \\leftarrow \\text{average}(Returns(S_t, A_t)) \\\\\n",
    "&\\qquad A^{*} \\leftarrow \\arg \\max_a Q(S_t, a) \\qquad \\qquad \\qquad \\text{(with ties broken arbitrarily)} \\\\\n",
    "&\\qquad \\text{For all } a \\in \\mathcal{A}(S_t): \\\\\n",
    "&\\qquad \\qquad \\pi(a \\vert S_t ) \\leftarrow \\begin{cases} 1 - \\epsilon + \\frac{\\epsilon}{\\vert \\mathcal{A}(S_t) \\vert} \\quad &\\text{ if } a = A^{*} \\\\\n",
    "\\frac{\\epsilon}{\\vert A(S_t) \\vert} &\\text{ if } a \\neq A^{*} \\end{cases}\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why does off-policy learning matter?\n",
    "\n",
    "### On-Policy and Off-Policy\n",
    "\n",
    "- On-policy: improve and evaluate the policy being used to select actions\n",
    "\n",
    "- Off-policy: improve and evaluate a different policy from the one used to select actions. It allows learning an optimal policy from suboptimal behavior.\n",
    "\n",
    "### Target policy\n",
    "\n",
    "Usually denoted by $\\pi(a \\vert s)$. The value function that the agent learning is based on the target policy (e.g. optimal policy)\n",
    "\n",
    "### Behavior policy\n",
    "\n",
    "usually denoted by $b(a \\vert s)$. The behavior policy is in charge of selecting actions for the agent. (e.g. uniform random policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance Sampling\n",
    "\n",
    "### Derivation\n",
    "\n",
    "We have some random variable $x$ that's being sampled from a probability distribution $b$\n",
    "\n",
    "$$ x \\sim b $$\n",
    "\n",
    "We want to estimate the expected value of $x$, but with respect to the target distribution $\\pi$\n",
    "\n",
    "$$ \\mathbb{E}_{\\pi}[X] $$\n",
    "\n",
    "$\\pi$ is not the same as $b$. So we cannot simply use the sample average to compute the expection under $pi$.\n",
    "\n",
    "$$ \\begin{aligned} \\mathbb{E}_{\\pi}[X] &\\doteq \\sum_{x \\in X} x \\pi(x) \\\\\n",
    " &= \\sum_{x \\in X} x \\pi(x) \\frac{b(x)}{b(x)} \\\\ \n",
    " &= \\sum_{x \\in X} x \\frac{\\pi(x)}{b(x)} b(x) \\\\\\end{aligned}$$\n",
    " \n",
    "This ratio ($\\frac{\\pi(x)}{b(x)}$) is called **importance sampling ratio** and denoted by $\\rho(x)$.\n",
    "\n",
    "$$ \\begin{aligned} \\mathbb{E}_{\\pi}[X] &= \\sum_{x \\in X}x \\rho(x) b(x) \\\\\n",
    " &= \\mathbb{E}_b[X \\rho(X)] \\\\ \\end{aligned}$$\n",
    " \n",
    "Recall that,\n",
    "\n",
    "$$ \\mathbb{E}[X] \\approx \\frac{1}{n} \\sum_{i=1}^{n}x_i $$\n",
    "\n",
    "Using this,\n",
    "\n",
    "$$ \\mathbb{E}[X \\rho(X)] \\approx \\frac{1}{n} \\sum_{i=1}^{n} x_i \\rho(x_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-Policy Monte Carlo Prediction\n",
    "\n",
    "### Off-Policy Monte Carlo\n",
    "\n",
    "$\\rho = \\frac{\\mathbb{P}(\\text{trajectory under } \\pi)}{\\mathbb{P}(\\text{trajectory under }b)}$\n",
    "\n",
    "$ \\begin{aligned} V_{\\pi}(s) &= \\mathbb{E}_{\\pi}[G_t \\vert S_t = s] \\\\\n",
    " &= \\mathbb{E}_{b}[\\rho G_t \\vert S_t = s] \\end{aligned} $\n",
    "\n",
    "### Off-Policy Trajectory\n",
    "\n",
    "Because of Markov Property,\n",
    "\n",
    "$$ P(A_t, S_{t+1}, A_{t+1}, \\dots, S_T \\vert S_t, A_{t:T}) = \\\\\n",
    "b(A_t \\vert S_t) p(S_{t+1} \\vert S_t, A_t) b(A_{t+1} \\vert S_{t+1}) p(S_{t+2} \\vert S_{t+1}, A_{t+1}), \\dots, p(S_T \\vert S_{T-1}, A_{T-1}) \\\\\n",
    "\\downarrow \\\\\n",
    "\\prod_{k=t}^{T-1} b(A_k \\vert S_k) p(S_{k+1} \\vert S_k, A_k) $$\n",
    "\n",
    "After that,\n",
    "\n",
    "$ \\begin{aligned} \\rho_{t:T-1} &\\doteq \\frac{P(\\text{trajectory under } \\pi)}{P(\\text{trajectory under } b)} \\\\\n",
    " &\\doteq \\prod_{k=t}^{T-1} \\frac{\\pi(A_k \\vert S_k) p(S_{k+1} \\vert S_k, A_k)}{b(A_k \\vert S_k) p(S_{k+1} \\vert S_k, A_k)} \\\\\n",
    " &\\doteq \\prod_{k=t}^{T-1} \\frac{\\pi(A_k \\vert S_k)}{b(A_k \\vert S_k)} \\end{aligned}$\n",
    " \n",
    "### Off-Policy Value\n",
    "\n",
    "$$ \\mathbb{E}_b[\\rho_{t:T-1} G_t \\vert S_t = s] = v_{\\pi}(s) $$\n",
    "\n",
    "### Every-visit MC prediction, for estimating $V \\approx v_{\\pi}$\n",
    "\n",
    "$\\begin{aligned}\n",
    "&\\text{Input: a policy } \\pi \\text{ to be evaluated} \\\\\n",
    "&\\text{Initialize:} \\\\\n",
    "&\\quad V(s) \\in \\mathcal{R} \\text{, arbitrarily, for all } s \\in S \\\\\n",
    "&\\quad Returns(s) \\leftarrow \\text{ an empty list, for all } s \\in S \\\\\n",
    "\\newline\n",
    "&\\text{Loop forever (for each episode):} \\\\\n",
    "&\\quad \\text{Generate an episode following } \\pi: S_0, A_0, R_1, S_1, \\dots, S_{T-1}, A_{T-1}, R_T \\\\\n",
    "&\\quad G \\leftarrow 0 \\\\\n",
    "&\\quad \\text{Loop for each step of episode, } t = T-1, T-2, \\dots, 0 \\\\\n",
    "&\\qquad G \\leftarrow \\gamma G + R_{t+1} \\\\\n",
    "&\\qquad \\text{Append } G \\text{ to } Returns(S_t) \\\\\n",
    "&\\qquad V(S_t) \\leftarrow \\text{average}(Returns(S_t)) \\end{aligned}$\n",
    "\n",
    "### Off-Policy every-visit MC prediction, for estimating $V \\approx v_{\\pi}$\n",
    "\n",
    "$\\begin{aligned}\n",
    "&\\text{Input: a policy } \\pi \\text{ to be evaluated} \\\\\n",
    "&\\text{Initialize:} \\\\\n",
    "&\\quad V(s) \\in \\mathcal{R} \\text{, arbitrarily, for all } s \\in S \\\\\n",
    "&\\quad Returns(s) \\leftarrow \\text{ an empty list, for all } s \\in S \\\\\n",
    "\\newline\n",
    "&\\text{Loop forever (for each episode):} \\\\\n",
    "&\\quad \\text{Generate an episode following } b: S_0, A_0, R_1, S_1, \\dots, S_{T-1}, A_{T-1}, R_T \\\\\n",
    "&\\quad G \\leftarrow 0, \\quad W \\leftarrow 1 \\\\\n",
    "&\\quad \\text{Loop for each step of episode, } t = T-1, T-2, \\dots, 0 \\\\\n",
    "&\\qquad G \\leftarrow \\gamma W G + R_{t+1} \\\\\n",
    "&\\qquad \\text{Append } G \\text{ to } Returns(S_t) \\\\\n",
    "&\\qquad V(S_t) \\leftarrow \\text{average}(Returns(S_t)) \\\\\n",
    "&\\qquad W \\leftarrow W \\frac{\\pi(A_t \\vert S_t)}{b(A_t \\vert S_t)} \\end{aligned}$\n",
    "\n",
    "### Computing $\\rho_{t:T-1}$ incrementally\n",
    "\n",
    "$\\begin{aligned} \n",
    "\\rho_{t:T-1} &\\doteq \\prod_{k=t}^{T-1} \\frac{\\pi(A_k \\vert S_k)}{b(A_k \\vert S_k)} \\\\\n",
    "&= \\rho_t \\rho_{t+1} \\rho_{t+2} \\dots \\rho_{T-2} \\rho_{T-1} \n",
    "\\end{aligned}$\n",
    "\n",
    "$W_1 \\leftarrow \\rho_{T-1} \\\\\n",
    " W_2 \\leftarrow \\rho_{T-1} \\rho_{T-2} \\\\\n",
    " W_3 \\leftarrow \\rho_{T-1} \\rho_{T-2} \\rho_{T-3} \\\\\n",
    " \\vdots \\\\\n",
    " W_{t+1} \\leftarrow W_t \\rho_t $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counterfactual / Batch Policy Optimization\n",
    "- Learn a Good Policy That will perform well in the future\n",
    "\n",
    "$$ \\mathop{\\mathrm{argmax}}_{\\pi} \\int_{s \\in S_0} \\hat{V}^{\\pi}(s, \\mathcal{D})ds \\\\\n",
    "\\begin{aligned}\n",
    "&\\mathcal{D}: \\text{ Dataset of }n \\text{ trajectories } \\tau, \\quad \\tau \\sim \\pi_b \\\\\n",
    "&\\pi: \\text{ Policy mapping } s \\rightarrow a \\\\\n",
    "&S_0: \\text{ Set of initial states} \\\\\n",
    "&\\hat{V}^{\\pi}(S, \\mathcal{D}): \\text{ Estimate } V(s) \\text{ w/dataset } \\mathcal{D}\n",
    "\\end{aligned}$$\n",
    "\n",
    "### Importance Sampling for Policy Evaluation\n",
    "\n",
    "$\\begin{aligned}\n",
    "V^{\\pi}(s) &= \\sum_{\\tau} p(\\tau \\vert \\pi, s) R(\\tau) \\\\\n",
    "&= \\sum_{\\tau} p(\\tau \\vert \\pi_b, s) \\frac{\\tau \\vert \\pi, s)}{\\tau \\vert \\pi_b, s)} R_{\\tau} \\\\ \n",
    "&\\approx \\sum_{i=1, \\tau_i \\sim \\pi_b}^N \\frac{p(\\tau_i \\vert \\pi, s)} {\\tau_i \\vert \\pi_b, s)} R_{\\tau_i} \\\\\n",
    "&= \\sum_{i=1, \\tau_i \\sim \\pi_b}^N R_{\\tau_i} \\prod_{t=1}^{H_i} \\frac{p(s_{i, t+1} \\vert s_{i,t}, a_{i,t}) p(a_{i, t} \\vert \\pi, s_{i, t})}{p(s_{i, t+1} \\vert s_{i,t}, a_{i,t}) p(a_{i, t} \\vert \\pi_b, s_{i, t})} \\\\\n",
    "&= \\sum_{i=1, \\tau_i \\sim \\pi_b}^N R_{\\tau_i} \\prod_{t=1}^{H_i} \\frac{p(a_{i, t} \\vert \\pi, s_{i, t})}{ p(a_{i, t} \\vert \\pi_b, s_{i, t})} \\\\\n",
    "\\end{aligned}$\n",
    "\n",
    "### Doubly Robust\n",
    "\n",
    "![dr](image/doubly_robust.png)\n",
    "\n",
    "+ Smaller variance than importance sampling\n",
    "+ Unbiased if either model realizable or behavior policy known"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
