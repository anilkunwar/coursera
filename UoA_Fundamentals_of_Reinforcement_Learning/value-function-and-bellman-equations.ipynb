{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value functions and Bellman Equations\n",
    "\n",
    "> In this post, we will learn the Value functions and Bellman equations. This is the summary of lecture \"Fundamentals of Reinforcement Learning\" from Coursera.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Chanseok Kang\n",
    "- categories: [Python, Coursera, Reinforcement_Learning]\n",
    "- image: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying Policies\n",
    "\n",
    "Policy maps the current state onto a set of probabilities for taking each action.\n",
    "Policies can only depend on the current state.\n",
    "\n",
    "### Deterministic Policy\n",
    "\n",
    "$$ \\pi(s) = a $$\n",
    "\n",
    "A policy that maps each state to a single action.\n",
    "\n",
    "![deterministic](image/deterministic.png)\n",
    "\n",
    "### Stochastic policy \n",
    "\n",
    "$$ \\pi(a \\vert s) $$\n",
    "\n",
    "Follows some basic rules\n",
    "\n",
    "* $ \\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\vert s) = 1 $\n",
    "* $ \\pi(a \\vert s) \\ge 0 $\n",
    "\n",
    "![stochastic](image/stochastic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value functions\n",
    "\n",
    "### State-value functions\n",
    "\n",
    "**state-value function** is the future reward an agent can expect to receive starting from a particular state. That is, the expected return from given state.\n",
    "\n",
    "$$ v_{\\pi}(s) \\doteq \\mathbb{E}_{\\pi} [G_t \\vert S_t = s] $$\n",
    "\n",
    "Note that expected return is,\n",
    "\n",
    "$$ G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} $$\n",
    "\n",
    "### Action-value functions\n",
    "\n",
    "An action value describes what happens when the agent first selects a particular action. More formally, the action value of a state is the expected return if the agent selects action $a$ and then follows policy $\\pi$.\n",
    "\n",
    "$$ q_{\\pi}(s, a) \\doteq \\mathbb{E}_{\\pi} [G_t \\vert S_t = s, A_t = a] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Equation Derivation\n",
    "\n",
    "### State-value Bellman equation\n",
    "\n",
    "$$ \\begin{aligned} v_{\\pi}(s) &\\doteq \\mathbb{E}_{\\pi}[G_t \\vert S_t = s] \\\\\n",
    "&= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} \\vert S_t = s] \\\\\n",
    "&= \\sum_{a} \\pi(a \\vert s) \\sum_{s'} \\sum_{r} p(s', r \\vert s, a) \\big[r + \\gamma \\mathbb{E}_{\\pi}[G_{t+1} \\vert S_{t+1} = s']\\big] \\\\\n",
    "&= \\sum_{a} \\pi(a \\vert s) \\sum_{s'} \\sum_{r} p(s', r \\vert s, a) \\Big[ r + \\gamma \\sum_{a} \\pi(a' \\vert s') \\sum_{s''} \\sum_{r'} p(s'', r' \\vert s', a') \\big[ r' + \\gamma \\mathbb{E}_{\\pi}[G_{t+2} \\vert S_{t+2} = s''] \\big] \\Big] \\\\\n",
    "&= \\sum_{a} \\pi(a \\vert s) \\sum_{s'} \\sum_{r} p(s', r \\vert s, a) [r + \\gamma v_{\\pi}(s')]\\end{aligned} $$\n",
    "\n",
    "### Action-value Bellman equation\n",
    "\n",
    "$$ \\begin{aligned} q_{\\pi}(s, a) &\\doteq \\mathbb{E}_{\\pi}[G_t \\vert S_t = s, A_t = a] \\\\\n",
    "&= \\sum_{s'} \\sum_{r} p(s', r \\vert s, a) \\big[ r + \\gamma \\mathbb{E}_{\\pi} [G_{t+1} \\vert S_{t+1} = s'] \\big] \\\\\n",
    "&= \\sum_{s'} \\sum_{r} p(s', r \\vert s, a) \\big[ r + \\gamma \\sum_{a'} \\pi (a' \\vert s') \\mathbb{E}_{\\pi} [G_{t+1} \\vert S_{t+1} = s', A_{t+1} = a'] \\big] \\\\\n",
    "&= \\sum_{a'} \\sum_{r} p(s', r \\vert s, a) \\big[ r+ \\gamma \\sum_{a'} \\pi (a' \\vert s') q_{\\pi}(s', a') \\big] \\end{aligned} $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
