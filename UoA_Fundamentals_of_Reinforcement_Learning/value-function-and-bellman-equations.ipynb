{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value functions and Bellman Equations\n",
    "\n",
    "> In this post, we will learn the Value functions and Bellman equations. This is the summary of lecture \"Fundamentals of Reinforcement Learning\" from Coursera.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Chanseok Kang\n",
    "- categories: [Python, Coursera, Reinforcement_Learning]\n",
    "- image: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying Policies\n",
    "\n",
    "Policy maps the current state onto a set of probabilities for taking each action.\n",
    "Policies can only depend on the current state.\n",
    "\n",
    "### Deterministic Policy\n",
    "\n",
    "$$ \\pi(s) = a $$\n",
    "\n",
    "A policy that maps each state to a single action.\n",
    "\n",
    "![deterministic](image/deterministic.png)\n",
    "\n",
    "### Stochastic policy \n",
    "\n",
    "$$ \\pi(a \\vert s) $$\n",
    "\n",
    "Follows some basic rules\n",
    "\n",
    "* $ \\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\vert s) = 1 $\n",
    "* $ \\pi(a \\vert s) \\ge 0 $\n",
    "\n",
    "![stochastic](image/stochastic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value functions\n",
    "\n",
    "### State-value functions\n",
    "\n",
    "**state-value function** is the future reward an agent can expect to receive starting from a particular state. That is, the expected return from given state.\n",
    "\n",
    "$$ v_{\\pi}(s) \\doteq \\mathbb{E}_{\\pi} [G_t \\vert S_t = s] $$\n",
    "\n",
    "Note that expected return is,\n",
    "\n",
    "$$ G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} $$\n",
    "\n",
    "### Action-value functions\n",
    "\n",
    "An action value describes what happens when the agent first selects a particular action. More formally, the action value of a state is the expected return if the agent selects action $a$ and then follows policy $\\pi$.\n",
    "\n",
    "$$ q_{\\pi}(s, a) \\doteq \\mathbb{E}_{\\pi} [G_t \\vert S_t = s, A_t = a] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Equation Derivation\n",
    "\n",
    "### State-value Bellman equation\n",
    "\n",
    "$$ \\begin{aligned} v_{\\pi}(s) &\\doteq \\mathbb{E}_{\\pi}[G_t \\vert S_t = s] \\\\\n",
    "&= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} \\vert S_t = s] \\\\\n",
    "&= \\sum_{a} \\pi(a \\vert s) \\sum_{s'} \\sum_{r} p(s', r \\vert s, a) \\big[r + \\gamma \\mathbb{E}_{\\pi}[G_{t+1} \\vert S_{t+1} = s']\\big] \\\\\n",
    "&= \\sum_{a} \\pi(a \\vert s) \\sum_{s'} \\sum_{r} p(s', r \\vert s, a) \\Big[ r + \\gamma \\sum_{a} \\pi(a' \\vert s') \\sum_{s''} \\sum_{r'} p(s'', r' \\vert s', a') \\big[ r' + \\gamma \\mathbb{E}_{\\pi}[G_{t+2} \\vert S_{t+2} = s''] \\big] \\Big] \\\\\n",
    "&= \\sum_{a} \\pi(a \\vert s) \\sum_{s'} \\sum_{r} p(s', r \\vert s, a) [r + \\gamma v_{\\pi}(s')]\\end{aligned} $$\n",
    "\n",
    "### Action-value Bellman equation\n",
    "\n",
    "$$ \\begin{aligned} q_{\\pi}(s, a) &\\doteq \\mathbb{E}_{\\pi}[G_t \\vert S_t = s, A_t = a] \\\\\n",
    "&= \\sum_{s'} \\sum_{r} p(s', r \\vert s, a) \\big[ r + \\gamma \\mathbb{E}_{\\pi} [G_{t+1} \\vert S_{t+1} = s'] \\big] \\\\\n",
    "&= \\sum_{s'} \\sum_{r} p(s', r \\vert s, a) \\big[ r + \\gamma \\sum_{a'} \\pi (a' \\vert s') \\mathbb{E}_{\\pi} [G_{t+1} \\vert S_{t+1} = s', A_{t+1} = a'] \\big] \\\\\n",
    "&= \\sum_{a'} \\sum_{r} p(s', r \\vert s, a) \\big[ r+ \\gamma \\sum_{a'} \\pi (a' \\vert s') q_{\\pi}(s', a') \\big] \\end{aligned} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Policies\n",
    "\n",
    "![optimal_policy](image/optimal_policy.png)\n",
    "\n",
    "In this case, we can say that $\\pi_1$ is good as or better than $\\pi_2$ in every state. And **optimal policy** $\\pi_{*}$ is as good as or better than all the other policies. Optimal policy is always existed in whole case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Value Function\n",
    "\n",
    "Recall that\n",
    "\n",
    "$$ \\pi_1 \\gt \\pi_2 \\text{ if and only if } v_{\\pi_1}(s) \\gt v_{\\pi_2}(s) \\text{ for all } s \\in \\mathcal{S} $$\n",
    "\n",
    "The value function for optimal policy can be defined like this:\n",
    "\n",
    "$$ v_{\\pi_{*}} (s) \\doteq \\mathbb{E}_{\\pi_{*}} [ G_t \\vert S_t = s] = \\max_{\\pi} v_{\\pi}(s) \\text{ for all } s \\in \\mathcal{S} $$\n",
    "\n",
    "The state-value function for optimal policy is denoted by $V_{*}$.\n",
    "\n",
    "Optimal policies also share the same optimal action-value function,\n",
    "\n",
    "$$ q_{\\pi_{*}} (s, a) = \\max_{\\pi} q_{\\pi}(s, a) \\text{ for all } s \\in \\mathcal{S} \\text{ and } a \\in \\mathcal{A} $$\n",
    "\n",
    "The action-value function for optimal policy is denoted by $q_{*}$.\n",
    "\n",
    "Recall the bellman equation,\n",
    "\n",
    "$$ v_{\\pi}(s) = \\sum_a \\pi(a \\vert s) \\sum_{s'} \\sum_{r} p(s', r \\vert s, a) [ r + \\gamma v_{\\pi}(s')] $$\n",
    "\n",
    "If we apply optimal policy in here,\n",
    "\n",
    "$$ v_{\\pi_{*}}(s) = \\sum_a \\pi_{*}(a \\vert s) \\sum_{s'} \\sum_{r} p(s', r \\vert s, a) [ r + \\gamma v_{\\pi_{*}}(s')] $$\n",
    "\n",
    "Or we can re-define the form that doesn't reference the policy itself,\n",
    "\n",
    "$$ v_{\\pi_{*}}(s) = \\max_a \\sum_{s'} \\sum_{r} p(s', r \\vert s, a) [r + \\gamma v_{*}(s')] $$\n",
    "\n",
    "This makes sense since the optimal determisitic policy will select the action that can get maximum sum of rewards. This form is called **Bellman optimality equation for $v_{*}$**.\n",
    "\n",
    "Same approach can be applied in action-value function.\n",
    "\n",
    "Recall that,\n",
    "\n",
    "$$ q_{\\pi}(s, a) = \\sum_{s'} \\sum_{r} p(s', r \\vert s, a) [ r + \\gamma \\sum_{a'} \\pi (a' \\vert s') q_{\\pi}(s', a') ] $$\n",
    "\n",
    "If we apply optimal policy in here,\n",
    "\n",
    "$$ q_{*}(s, a) = \\sum_{s'} \\sum_{r} p(s', r \\vert s, a) [ r + \\gamma \\sum_{a'} \\pi_{*} (a' \\vert s') q_{*}(s', a') ] $$\n",
    "\n",
    "Remove the reference of optimal policy,\n",
    "\n",
    "$$ q_{*}(s, a) = \\sum_{s'} \\sum_{r} p(s', r \\vert s, a) [ r + \\gamma \\max_{a'} q_{*}(s', a')] $$\n",
    "\n",
    "This is called **Bellman optimality equation for $q_{*}$**.\n",
    "\n",
    "But $\\max$ operation is not linear, so we cannot derive the solution from linear algebra for solving this linear system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Optimal Value Functions to Get Optimal Policies\n",
    "\n",
    "### Detemining an Optimal policy\n",
    "\n",
    "![state_transition](image/state_transition.png)\n",
    "\n",
    "$$ v_{*}(s) = \\max_a \\sum_{s'} \\sum_{r} p(s', r \\vert s, a) [ r + \\gamma v_{*}(s')] $$\n",
    "\n",
    "$$ \\pi_{*}(s) = \\arg \\max_{a} \\sum_{s'} \\sum_{r} p(s', r \\vert s, a) [ r + \\gamma v_{*}(s')] $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
