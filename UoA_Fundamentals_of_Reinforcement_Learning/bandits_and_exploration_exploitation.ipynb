{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bandits and Exploration/Exploitation\n",
    "\n",
    "> In this post, we will learn simple reinforcement learning problem, the bandits, This is the summary of lecture \"Fundamentals of Reinforcement Learning\" from Coursera.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Chanseok Kang\n",
    "- categories: [Python, Coursera, Reinforcement_Learning]\n",
    "- image: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Decision Making with Evaluative Feedback\n",
    "\n",
    "### Action-Values (or Action-Value function)\n",
    "\n",
    "The value is the expected reward\n",
    "\n",
    "$$ \\begin{aligned} q_{*}(a) &\\doteq \\mathbb{E}[R_t \\vert A_t = a] \\quad \\forall a \\in \\{1, \\dots, k\\} \\\\ &= \\sum_{r} p(r \\vert a ) r \\end{aligned}$$\n",
    "\n",
    "Here, $\\doteq$ means **\"is defined as\"**. \n",
    "\n",
    "The goal of Action-value is to maximize the expected reward. In formula, we can express it like this,\n",
    "\n",
    "$$ \\arg\\max_a q_{*}(a) $$\n",
    "\n",
    "Then, how can we calculate the $q_{*}(a)$?\n",
    "\n",
    "In K-armed bandit problem, we can explain it in medicine prescription case. Consider we have three types of medicines.\n",
    "\n",
    "![medicine](image/bandit_q_a.png)\n",
    "\n",
    "And each medicine has its own distribution.\n",
    "\n",
    "![medicine](image/bandit_distribution.png)\n",
    "\n",
    "Red one is Binomial, green one is normal, and blue one is uniform distribution. Based on this information, we can calculate each $q_{*}(a)$.\n",
    "\n",
    "$$ \\begin{aligned} \\text{Red :} q_{*}(a) &= 0.5 \\times -11 + 0.5 \\times 9 = -0.1 \\\\ \n",
    "   \\text{Green :} q_{*}(a) &= 1 \\\\\n",
    "   \\text{Blue :} q_{*}(a) &= 3 \\\\ \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn Action Values\n",
    "\n",
    "### Value of an Action\n",
    "\n",
    "- The value of an action is the **expected reward** when that action is taken\n",
    "\n",
    "$$ q_{*}(a) \\doteq \\mathbb{E}[R_t \\vert A_t = a] $$\n",
    "\n",
    "- Usually, $q_{*}(a)$ is not known for agent, so we need to estimate it. One way to estimate is the **Sample-average Method**\n",
    "\n",
    "### Sample Average Method\n",
    "\n",
    "$$ \\begin{aligned} Q_t(a) &\\doteq \\frac{\\text{sum of rewards when } a \\text{ taken prior to } t}{\\text{number of times } a \\text{ taken prior to } t} \\\\ &= \\frac{\\sum_{i=1}^{t-1}R_i}{t-1} \\end{aligned} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Action Values Incrementally\n",
    "\n",
    "### Incremental update rule\n",
    "\n",
    "$$ \\begin{aligned} Q_{n+1} &= \\frac{1}{n} \\sum_{i=1}^n R_i \\\\ \n",
    " &= \\frac{1}{n} (R_n + \\sum_{i=1}^{n-1}R_i) \\\\ \n",
    " &= \\frac{1}{n} \\big(R_n + (n-1) \\frac{1}{n-1} \\sum_{i=1}^{n-1}R_i \\big) \\end{aligned} $$\n",
    " \n",
    "Recall that,\n",
    "\n",
    "$$ Q_n = \\frac{1}{n-1} \\sum_{i=1}^{n-1}R_i $$\n",
    "\n",
    "So we can substitute it with $Q_n$.\n",
    "\n",
    "$$ \\begin{aligned} Q_{n+1} &= \\frac{1}{n} (R_n + (n-1)Q_n) \\\\\n",
    "  &= \\frac{1}{n}(R_n + nQ_n - Q_n) \\\\\n",
    "  &= Q_n + \\frac{1}{n}(R_n - Q_n) \\end{aligned} $$\n",
    "  \n",
    "In upper formula, $R_n - Q_n$ means the error between the target(new reward) and previous estimate. And we are using $\\frac{1}{n}$ as an step size, but we can choose another step size. If we assume the step size to $\\alpha_n$, we can express the formula with general form.\n",
    "\n",
    "$$ Q_{n+1} = Q_n + \\alpha_n ( R_n - Q_n) $$\n",
    "\n",
    "But when the $n$ is increased, that is, the older reward than just before will be decaying depending on step size $n$.\n",
    "\n",
    "### Decaying past rewards\n",
    "\n",
    "Using the formular that we derived from previous, we can explain the past reward decaying.\n",
    "\n",
    "$$ \\begin{aligned} Q_{n+1} &= Q_n + \\alpha_n (R_n - Q_n) \\\\\n",
    " &= \\alpha R_n + Q_n - \\alpha Q_n \\\\\n",
    " &= \\alpha R_n + (1 - \\alpha)Q_n \\\\\n",
    " &= \\alpha R_n + (1 - \\alpha) [\\alpha R_{n-1} + (1 - \\alpha) Q_{n-1}] \\\\\n",
    " &= \\alpha R_n + (1 - \\alpha) \\alpha R_{n-1} + (1 - \\alpha)^2 Q_{n-1} \\end{aligned} $$\n",
    " \n",
    "We can unroll this until the initial action value appears.\n",
    "\n",
    "$$ \\begin{aligned} Q_{n+1} &= Q_n + \\alpha_n (R_n - Q_n) \\\\\n",
    " &= \\alpha R_n + (1-\\alpha)\\alpha R_{n-1} + (1 - \\alpha)^2 \\alpha R_{n-2} + \\dots + (1-\\alpha)^{n-1} \\alpha R_1 + (1- \\alpha)^{n} Q_1 \\\\\n",
    " &= (1-\\alpha)^n Q_1 + \\sum_{i=1}^{n} \\alpha (1-\\alpha)^{n-i} R_i \\end{aligned} $$\n",
    " \n",
    "From the formula, we can see that the current estimated value($Q_{n+1}$) is related on the initial action value and all the observed reward. But when $n$ is increased, $Q_1$ term will be decreased exponentially. After that, the influence of initial action value goes to zero with more and more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
