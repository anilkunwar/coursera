{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bandits and Exploration/Exploitation\n",
    "\n",
    "> In this post, we will learn simple reinforcement learning problem, the bandits, This is the summary of lecture \"Fundamentals of Reinforcement Learning\" from Coursera.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Chanseok Kang\n",
    "- categories: [Python, Coursera, Reinforcement_Learning]\n",
    "- image: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Decision Making with Evaluative Feedback\n",
    "\n",
    "### Action-Values (or Action-Value function)\n",
    "\n",
    "The value is the expected reward\n",
    "\n",
    "$$ \\begin{aligned} q_{*}(a) &\\doteq \\mathbb{E}[R_t \\vert A_t = a] \\quad \\forall a \\in \\{1, \\dots, k\\} \\\\ &= \\sum_{r} p(r \\vert a ) r \\end{aligned}$$\n",
    "\n",
    "Here, $\\doteq$ means **\"is defined as\"**. \n",
    "\n",
    "The goal of Action-value is to maximize the expected reward. In formula, we can express it like this,\n",
    "\n",
    "$$ \\arg\\max_a q_{*}(a) $$\n",
    "\n",
    "Then, how can we calculate the $q_{*}(a)$?\n",
    "\n",
    "In K-armed bandit problem, we can explain it in medicine prescription case. Consider we have three types of medicines.\n",
    "\n",
    "![medicine](image/bandit_q_a.png)\n",
    "\n",
    "And each medicine has its own distribution.\n",
    "\n",
    "![medicine](image/bandit_distribution.png)\n",
    "\n",
    "Red one is Binomial, green one is normal, and blue one is uniform distribution. Based on this information, we can calculate each $q_{*}(a)$.\n",
    "\n",
    "$$ \\begin{aligned} \\text{Red :} q_{*}(a) &= 0.5 \\times -11 + 0.5 \\times 9 = -0.1 \\\\ \n",
    "   \\text{Green :} q_{*}(a) &= 1 \\\\\n",
    "   \\text{Blue :} q_{*}(a) &= 3 \\\\ \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn Action Values\n",
    "\n",
    "### Value of an Action\n",
    "\n",
    "- The value of an action is the **expected reward** when that action is taken\n",
    "\n",
    "$$ q_{*}(a) \\doteq \\mathbb{E}[R_t \\vert A_t = a] $$\n",
    "\n",
    "- Usually, $q_{*}(a)$ is not known for agent, so we need to estimate it. One way to estimate is the **Sample-average Method**\n",
    "\n",
    "### Sample Average Method\n",
    "\n",
    "$$ \\begin{aligned} Q_t(a) &\\doteq \\frac{\\text{sum of rewards when } a \\text{ taken prior to } t}{\\text{number of times } a \\text{ taken prior to } t} \\\\ &= \\frac{\\sum_{i=1}^{t-1}R_i}{t-1} \\end{aligned} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Action Values Incrementally\n",
    "\n",
    "### Incremental update rule\n",
    "\n",
    "$$ \\begin{aligned} Q_{n+1} &= \\frac{1}{n} \\sum_{i=1}^n R_i \\\\ \n",
    " &= \\frac{1}{n} (R_n + \\sum_{i=1}^{n-1}R_i) \\\\ \n",
    " &= \\frac{1}{n} \\big(R_n + (n-1) \\frac{1}{n-1} \\sum_{i=1}^{n-1}R_i \\big) \\end{aligned} $$\n",
    " \n",
    "Recall that,\n",
    "\n",
    "$$ Q_n = \\frac{1}{n-1} \\sum_{i=1}^{n-1}R_i $$\n",
    "\n",
    "So we can substitute it with $Q_n$.\n",
    "\n",
    "$$ \\begin{aligned} Q_{n+1} &= \\frac{1}{n} (R_n + (n-1)Q_n) \\\\\n",
    "  &= \\frac{1}{n}(R_n + nQ_n - Q_n) \\\\\n",
    "  &= Q_n + \\frac{1}{n}(R_n - Q_n) \\end{aligned} $$\n",
    "  \n",
    "In upper formula, $R_n - Q_n$ means the error between the target(new reward) and previous estimate. And we are using $\\frac{1}{n}$ as an step size, but we can choose another step size. If we assume the step size to $\\alpha_n$, we can express the formula with general form.\n",
    "\n",
    "$$ Q_{n+1} = Q_n + \\alpha_n ( R_n - Q_n) $$\n",
    "\n",
    "But when the $n$ is increased, that is, the older reward than just before will be decaying depending on step size $n$.\n",
    "\n",
    "### Decaying past rewards\n",
    "\n",
    "Using the formular that we derived from previous, we can explain the past reward decaying.\n",
    "\n",
    "$$ \\begin{aligned} Q_{n+1} &= Q_n + \\alpha_n (R_n - Q_n) \\\\\n",
    " &= \\alpha R_n + Q_n - \\alpha Q_n \\\\\n",
    " &= \\alpha R_n + (1 - \\alpha)Q_n \\\\\n",
    " &= \\alpha R_n + (1 - \\alpha) [\\alpha R_{n-1} + (1 - \\alpha) Q_{n-1}] \\\\\n",
    " &= \\alpha R_n + (1 - \\alpha) \\alpha R_{n-1} + (1 - \\alpha)^2 Q_{n-1} \\end{aligned} $$\n",
    " \n",
    "We can unroll this until the initial action value appears.\n",
    "\n",
    "$$ \\begin{aligned} Q_{n+1} &= Q_n + \\alpha_n (R_n - Q_n) \\\\\n",
    " &= \\alpha R_n + (1-\\alpha)\\alpha R_{n-1} + (1 - \\alpha)^2 \\alpha R_{n-2} + \\dots + (1-\\alpha)^{n-1} \\alpha R_1 + (1- \\alpha)^{n} Q_1 \\\\\n",
    " &= (1-\\alpha)^n Q_1 + \\sum_{i=1}^{n} \\alpha (1-\\alpha)^{n-i} R_i \\end{aligned} $$\n",
    " \n",
    "From the formula, we can see that the current estimated value($Q_{n+1}$) is related on the initial action value and all the observed reward. But when $n$ is increased, $Q_1$ term will be decreased exponentially. After that, the influence of initial action value goes to zero with more and more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the trade-off\n",
    "\n",
    "### Exploration and Exploitation\n",
    "\n",
    "- **Exploration**: improve knowledge for long-term benefit\n",
    "\n",
    "- **Exploitation**: exploit knowledge for short-term benefit\n",
    "\n",
    "- How do we choose when to explore and when to exploit? (Exploration-Exploitation dilemma)\n",
    "\n",
    "### Epsilon-Greedy Action Selection\n",
    "\n",
    "Here, Epsilon($\\epsilon$) stands for the probability of choosing to explore.\n",
    "\n",
    "$$ A_t \\leftarrow \\begin{cases} \\arg\\max_a Q_t(a) &\\mbox{with probability } 1 - \\epsilon \\\\\n",
    " a \\sim \\text{Uniform}(\\{a_1 \\dots a_k\\}) &\\mbox{with probability } \\epsilon \\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimistic Initial Values\n",
    "\n",
    "We can define the action value to be optimistic: that is, set the estimated value to higher than reward. \n",
    "\n",
    "### Limitation of optimistic initial values\n",
    "\n",
    "- Optimistic initial values only drive **early exploration**\n",
    "\n",
    " $\\rightarrow$ this means agents will not continue exploring after some time.\n",
    "- They are not well-suited for **non-stationary problems**.\n",
    " \n",
    " $\\rightarrow$ Sometimes action values may change over the time, but optimistic agent will not notice that a different action is better now.\n",
    "- We may not know what the optimistic initial value should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upper-Confidence Bound (UCB) Action Selection\n",
    "\n",
    "### Uncertainty in Estimates\n",
    "\n",
    "![ucb](image/ucb.png)\n",
    "\n",
    "As we already notice, we cannot get true estimated action value($q_*(a)$). If we get current estimated value (Q(a)), true estimated value may lower or higher than current value. The idea to handle this, is to set confidence interval about current estimated value. In that case, true estimated value may be in this boundary with uncertainty.\n",
    "\n",
    "For the optimism, we can set the initial value to the upper bound of confidence interval.\n",
    "\n",
    "![ucb2](image/ucb2.png)\n",
    "\n",
    "After time goes on, the confidence interval will be smaller than inital value, and close to the true estimated value. We can express this with formula\n",
    "\n",
    "$$ A_t \\doteq \\arg \\max \\big[ Q_t(a) + c \\sqrt{\\frac{\\ln{t}}{N_t(a)}} \\big] $$\n",
    "\n",
    "The first term($Q_t(a)$) is exploitation term, and the second term($c \\sqrt{\\frac{\\ln{t}}{N_t(a)}}$) is the upper confidence bound exploration term, which is exploration term."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
