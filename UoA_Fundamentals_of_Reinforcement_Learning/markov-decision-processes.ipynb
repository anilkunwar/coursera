{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes\n",
    "\n",
    "> In this post, we will learn the Markov decision processes (MDP), the mathematical framework for solving RL Problem. This is the summary of lecture \"Fundamentals of Reinforcement Learning\" from Coursera.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Chanseok Kang\n",
    "- categories: [Python, Coursera, Reinforcement_Learning]\n",
    "- image: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Processes\n",
    "\n",
    "![mdp](image/mdp.png)\n",
    "\n",
    "MDP provide a general framework for sequential decision-making.\n",
    "\n",
    "### Dynamics of an MDP\n",
    "\n",
    "<img src='image/mdp2.png' align='center' />\n",
    "\n",
    "$$ p(s', r \\vert s, a) $$\n",
    "\n",
    "the joint probability of next state s' and reward, given current state and action.\n",
    "\n",
    "- Markov property\n",
    "\n",
    "$$ p: \\mathcal{S} \\times \\mathcal{R} \\times \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1] \\\\\n",
    " \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\vert s, a) = 1, \\quad \\forall s \\in S, a \\in \\mathcal{A}(s)\n",
    "$$\n",
    "\n",
    "The present state contains all the information necessary to predict the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Goal of Reinforcement Learning\n",
    "\n",
    "### Formal definition\n",
    "\n",
    "$$ \\text{Return } G_t \\doteq R_{t+1} + R_{t+2} + R_{t+3} + \\dots $$\n",
    "\n",
    "**Return** is the random variable since the dynamics of the MDP can be stochastic. To maximize this value, we use expected value of return, the **expected return**.\n",
    "\n",
    "$$ \\mathbb{E}[{G_t}] = \\mathbb{E}[R_{t+1} + R_{t+2} + R_{t+3} + \\dots ] $$\n",
    "\n",
    "The sum of reward must be finite.\n",
    "\n",
    "### Episodic task\n",
    "\n",
    "![episodic](image/episodic_task.png)\n",
    "\n",
    "The interaction naturally breaks into chunks called episodes, and each episode begins independently of how the previous one ended. Each episode has terminal state, and at termination, the agent is reset to a start state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Hypothesis\n",
    "\n",
    "### Reinforcement-learning hypothesis\n",
    "\n",
    "*Intelligent behavior arises from the actions of an individual seeking to maximize its received reward signals in a complext and changing world.*\n",
    "\n",
    "- Research program\n",
    "    - indentify where reward signals come from,\n",
    "    - develop algorithms that search the space of behaviors to maximize reward signals.\n",
    "    \n",
    "### Goals as Rewards\n",
    "\n",
    "- 1 for goal, 0 otherwise\n",
    "    - goal-reward representation\n",
    "- -1 for not goal, 0 once goal reached\n",
    "    - action-penalty representation\n",
    "    \n",
    "### Reward Expression\n",
    "\n",
    "- Programming\n",
    "    - coding\n",
    "    - Human-in-the-loop\n",
    "- Examples\n",
    "    - Mimic reward\n",
    "    - Inverse reinforcement learning\n",
    "- Optimization\n",
    "    - Evolutionary optimization\n",
    "    - meta RL\n",
    "    \n",
    "### Challenges to the Hypothesis\n",
    "\n",
    "- Target is something other than expected cumulative reward:\n",
    "    - How represent risk-sensitive behavior?\n",
    "    - How capture diversity in behavior?\n",
    "- Good match for high-level human behavior?\n",
    "    - Blind reward pursuers aren't good people.\n",
    "    - We create our \"purpose\" over years, lifetimes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuing tasks\n",
    "\n",
    "### Difference between episodic task and continuing task\n",
    "\n",
    "- Episodic task\n",
    "    - Interaction breaks natually into **episodes**\n",
    "    - Each episode ends in a **terminal state**\n",
    "    - Episodes are **independent**\n",
    "    \n",
    "$$ G_t \\doteq R_{t+1} + R_{t+2} + R_{t+3} + \\dots + R_T $$\n",
    "\n",
    "- Continuing task\n",
    "    - Interaction goes on **continually**\n",
    "    - No terminal state\n",
    "    \n",
    "$$ G_t \\doteq R_{t+1} + R_{t+2} + R_{t+3} + \\dots = \\infty (\\text{?}) $$\n",
    "\n",
    "### Discounting\n",
    "\n",
    "So how can we make $G_t$ to finite? One solution is to **discount** reward in the future by $\\gamma$, where $0 \\le \\gamma < 1$.\n",
    "\n",
    "$$ \\begin{aligned} G_t &\\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots + \\gamma^{k-1} R_{t+k} + \\dots \\\\ &= \\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1} \\end{aligned} $$\n",
    "\n",
    "$G_t$ is finite as long as $0 \\le \\gamma < 1$.\n",
    "\n",
    "Assume $R_{max}$ is the maximum reward the agent can receive\n",
    "\n",
    "$$ \\begin{aligned} G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\le \\sum_{k=0}^{\\infty} \\gamma^k R_{max} &= R_{max} \\sum_{k=0}^{\\infty} \\gamma^k \\\\ &= R_{max} \\times \\frac{1}{1 - \\gamma} \\end{aligned} $$\n",
    "\n",
    "It will converge geometric series when $\\gamma \\lt 1$\n",
    "\n",
    "### Effect of discount factor on agent behavior\n",
    "\n",
    "$$ G_t \\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots + \\gamma^{k-1} R_{t+k} + \\dots $$\n",
    "\n",
    "If $\\gamma = 0$, \n",
    "\n",
    "$$ \\begin{aligned} G_t &\\doteq R_{t+1} + 0 R_{t+2} + 0^2 R_{t+3} + \\dots + 0^{k-1} R_{t+k} + \\dots \\\\\n",
    "&= R_{t+1} \\end{aligned}$$\n",
    "\n",
    "In this case, agent only cares about the immediate reward, that is **short-sighted agent!**\n",
    "\n",
    "If $\\gamma \\rightarrow 1$, Agent takes future rewards into account more strongly, that is **Far-sighted agent!**\n",
    "\n",
    "### Recursive nature of returns\n",
    "\n",
    "$$ \\begin{aligned} G_t &\\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + \\dots \\\\\n",
    "&= R_{t+1} + \\gamma ( R_{t+2} + \\gamma R_{t+3} + \\gamma^2 R_{t+4} + \\dots) \\\\\n",
    "&= R_{t+1} + \\gamma G_{t+1} \\end{aligned} $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
