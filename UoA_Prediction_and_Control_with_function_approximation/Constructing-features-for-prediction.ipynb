{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing features for Prediction\n",
    "\n",
    "> This is the summary of lecture \"Prediction and Control with Function Approximation\" from Coursera.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Chanseok Kang\n",
    "- categories: [Python, Coursera, Reinforcement_Learning]\n",
    "- image: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coarse Coding\n",
    "\n",
    "![coarse coding](image/coarse_coding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization Properties of Coarse Coding\n",
    "\n",
    "### Direction of Generalization\n",
    "\n",
    "![dg](image/direction_of_generalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tile Coding\n",
    "\n",
    "A specific type of coarse coding\n",
    "\n",
    "![tile](image/tile_coding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Neural Network?\n",
    "\n",
    "### Simple neural network\n",
    "\n",
    "![nn](image/neural_network.png)\n",
    "\n",
    "We call this a feed-forward neural network because the data always moves forward through the layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear Approximation with Neural Networks\n",
    "\n",
    "### Non-linear representation\n",
    "\n",
    "![nn2](image/neural_network2.png)\n",
    "\n",
    "When we first build the neural network, we need to specify the initial weights.\n",
    "\n",
    "$$ W_{\\text{init}} \\sim \\mathcal{N}(\\mu, \\sigma) $$\n",
    "\n",
    "If input is $x = [x_1, x_2]^T$,\n",
    "\n",
    "![1node](image/nn_1node.png)\n",
    "\n",
    "All inputs is mulitplied by each weights, and all nodes in layer do the same thing. This process is similar with tile coding. After that, we can construct a non-linear mapping of the inputs to produce the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Networks\n",
    "\n",
    "### Composition features\n",
    "\n",
    "![cf](image/composition_features.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Strategies for NNs\n",
    "\n",
    "### Weight initialization\n",
    "\n",
    "![wi](image/neural_network2.png)\n",
    "\n",
    "$$ W_{\\text{init}} \\sim \\mathcal{N}(0, 1) $$\n",
    "\n",
    "One downside to this strategy is that, as we add more inputs to a neuron, the variance of the output grows. We can get around this issue by scaling the variance of the weights.\n",
    "\n",
    "$$ W_{\\text{init}} \\sim \\frac{\\mathcal{N}(0, 1)}{\\sqrt{n_{inputs}}} $$\n",
    "\n",
    "### Update momentum\n",
    "\n",
    "$\\text{Original: } W_{t+1} \\leftarrow W_{t} - \\alpha \\nabla_{w} L(W_{t}) \\\\\n",
    "\\text{Adding Momentum: } W_{t+1} \\leftarrow W_{t} - \\alpha \\nabla_w L(W_{t}) + \\lambda M_t \\\\ M_{t+1} \\leftarrow \\lambda M_{t} - \\alpha \\nabla_w L $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
