{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Control with Approximation\n",
    "\n",
    "> This is the summary of lecture \"Prediction and Control with Function Approximation\" from Coursera.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Chanseok Kang\n",
    "- categories: [Python, Coursera, Reinforcement_Learning]\n",
    "- image: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episodic SARSA with Function Approximation\n",
    "\n",
    "### State-values to action-values\n",
    "\n",
    "$$ v_{\\pi}(s) \\approx \\hat{v}(s, w) \\doteq w^Tx(s) \\\\\n",
    "q_{\\pi}(s, a) \\approx \\hat{q}(s, a, w) \\doteq w^Tx(s, a) $$\n",
    "\n",
    "### Representing actions\n",
    "\n",
    "$x(s) = \\begin{bmatrix} x_0(s) \\\\ x_1(s) \\\\ x_2(s) \\\\ x_3(s) \\end{bmatrix} \\\\ \\mathcal{A}(s) = \\{a_0, a_1, a_2\\}$\n",
    "\n",
    "$x(s, a) = \\begin{bmatrix} x_0(s) \\\\ x_1(s) \\\\ x_2(s) \\\\ x_3(s) \\\\ x_0(s) \\\\ x_1(s) \\\\ x_2(s) \\\\ x_3(s) \\\\ x_0(s) \\\\ x_1(s) \\\\ x_2(s) \\\\ x_3(s) \\end{bmatrix}$\n",
    "\n",
    "This is called **stacked features**\n",
    "\n",
    "### Episodic Semi-gradient SARSA for Estimating $\\hat{q} \\approx q_{*}$\n",
    "$\\begin{aligned}\n",
    "&\\text{Input: a differentiable action-value function parameterization } \\hat{q}: \\mathcal{s} \\times \\mathcal{A} \\times \\mathbb{R}^d \\to \\mathbb{R} \\\\\n",
    "&\\text{Algorithm paramters: step size } \\alpha > 0, \\text{ small } \\epsilon > 0 \\\\\n",
    "&\\text{Initialize value-function weights } w \\in \\mathbb{R}^d \\text{ arbitrarily (e.g., } w = 0 \\text{)} \\\\\n",
    "\\newline\n",
    "&\\text{Loop for each episode:} \\\\\n",
    "&\\quad S, A \\leftarrow \\text{ initial state and action of episode (e.g., } \\epsilon \\text{-greedy)} \\\\\n",
    "&\\quad \\text{Loop for each step of episode:} \\\\\n",
    "&\\qquad \\text{Take action } A, \\text{ observe } R, S' \\\\\n",
    "&\\qquad \\text{If } S' \\text{ is terminal:} \\\\\n",
    "&\\qquad \\quad w \\leftarrow w + \\alpha[R - \\hat{q}(S, A, w)] \\nabla \\hat{q}(S, A, w) \\\\\n",
    "&\\qquad \\quad \\text{Go to next episode} \\\\\n",
    "&\\qquad \\text{Choose } A' \\text{ as a function of } \\hat{q}(S', \\cdot, w) \\text{ (e.g., } \\epsilon \\text{-greedy)} \\\\\n",
    "&\\qquad w \\leftarrow w + \\alpha[R + \\gamma \\hat{q}(S', A', w) - \\hat{q}(S, A, w)] \\nabla \\hat{q}(S, A, w) \\\\\n",
    "&\\qquad S \\leftarrow S' \\\\\n",
    "&\\qquad A \\leftarrow A' \\\\\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Sarsa with Function Approximation\n",
    "\n",
    "### From SARSA to Expected SARSA\n",
    "\n",
    "SARSA:\n",
    "\n",
    "$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha + \\big(R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)\\big) $\n",
    "\n",
    "Expected SARSA:\n",
    "\n",
    "$ Q(S_t, Q_t) \\leftarrow Q(S_t, A_t) + \\alpha + \\big(R_{t+1} + \\gamma \\sum\\limits_{a'} \\pi(a' \\vert S_{t+1} ) Q(S_{t+1}, a') - Q(S_t, A_t) \\big) $\n",
    "\n",
    "### Expected SARSA with Function Approximation\n",
    "\n",
    "SARSA:\n",
    "\n",
    "$ w \\leftarrow w + \\alpha \\big(R_{t+1} + \\gamma \\hat{q}(S_{t+1}, A_{t+1}, w) - \\hat{q}(S_t, A_t, w)\\big) \\nabla \\hat{q}(S_t, A_t, w) $\n",
    "\n",
    "Expected SARSA:\n",
    "\n",
    "$ w \\leftarrow w + \\alpha \\big(R_{t+1} + \\gamma \\sum\\limits_{a'} \\pi(a' \\vert S_{t+1}) \\hat{q}(S_{t+1}, a', w) - \\hat{q}(S_t, A_t, w) \\big) \\nabla \\hat{q}(S_t, A_t, w) $\n",
    "\n",
    "### Expected SARSA to Q-learning\n",
    "\n",
    "$ w \\leftarrow w + \\alpha \\big( R_{t+1} + \\gamma \\max\\limits_{a'} \\hat{q}(S_{t+1}, a', w) - \\hat{q}(S_t, A_t, w)\\big) \\nabla \\hat{q}(S_t, A_t, w) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration under Function Approximation\n",
    "\n",
    "### Epsilon-Greedy\n",
    "\n",
    "![eg](image/epsilon_greedy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Reward - A New Way of Formulating Control Problems\n",
    "\n",
    "### Simple example\n",
    "\n",
    "![](image/continuing_task.png)\n",
    "\n",
    "Two policies are existed: traversing the left ring and traversing the right ring. The reward is zero everywhere except for in one transition in each ring. In the left ring, the reward is +1 immediately after state $S$. In the right ring, the reward is +2 immediately before state $S$.\n",
    "\n",
    "If we calculate the value function in left policy with geometric series,\n",
    "\n",
    "$v_{L}(S) = \\frac{1}{1 - \\gamma^5}$\n",
    "\n",
    "In case of right policy,\n",
    "\n",
    "$v_{R}(S) = \\frac{2 \\gamma^4}{1 - \\gamma^5}$\n",
    "\n",
    "If we set inequality of above equations,\n",
    "\n",
    "$\\frac{1}{1 - \\gamma^5} = \\frac{2 \\gamma^4}{1 - \\gamma^5}$\n",
    "\n",
    "$v_{R}(S) > v_{L}(S) \\text{ when } \\gamma > 2^{\\frac{1}{4}} \\approx 0.841$\n",
    "\n",
    "In order to maintain the discount factor, we might need gamma to be quite large.\n",
    "\n",
    "### The Average Reward objective\n",
    "\n",
    "$$ r(\\pi) \\doteq \\lim\\limits_{h \\to \\infty} \\frac{1}{h} \\sum\\limits_{t=1}^h \\mathbb{E}[R_t \\vert S_0, A_{0:t-1} \\sim \\pi] $$\n",
    "\n",
    "Using state visitation $\\mu$, we can redefine this,\n",
    "\n",
    "$$ r(\\pi) = \\sum\\limits_s \\mu_{\\pi}(s) \\sum\\limits_{a} \\pi(a \\vert s) \\sum\\limits_{s', r} p(s', r \\vert s, a) r $$\n",
    "\n",
    "### Returns for Average Reward\n",
    "\n",
    "$$G_t = R_{t+1} - r(\\pi) + R_{t+2} - r(\\pi) + R_{t+3} - r(\\pi) + \\dots $$\n",
    "\n",
    "Each term ($R_{t+x} - r(\\pi)$) is called differential returns.\n",
    "\n",
    "### Value Functions for Average Reward\n",
    "\n",
    "$q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}[G_t \\vert S_t = s, A_t = a] $\n",
    "\n",
    "$q_{\\pi}(s, a) = \\sum\\limits_{s', r}p(s', r \\vert s, a) ( r - r(\\pi) + \\sum\\limits_{a'} \\pi(a' \\vert s') q_{\\pi}(s', a'))$\n",
    "\n",
    "### Differential semi-gradient SARSA for estimating $\\hat{q} \\approx q_{*}$\n",
    "\n",
    "$\\begin{aligned}\n",
    "&\\text{Input: a differentiable action-value function parameterization } \\hat{q} : \\mathcal{S} \\times \\mathcal{A} \\times \\mathbb{R}^d \\to \\mathbb{R} \\\\\n",
    "&\\text{Algorithm parameters: step sizes } \\alpha, \\beta > 0 \\\\\n",
    "&\\text{Initialize value-function weights } w \\in \\mathbb{R}^d \\text{ arbitrarily (e.g., } w=0 \\text{)} \\\\\n",
    "&\\text{Initialize average reward estimate } \\bar{R} \\in \\mathbb{R} \\text{ arbitrarily (e.g., } \\bar{R} = 0 \\text{)} \\\\\n",
    "\\newline\n",
    "&\\text{Initialize state } S, \\text{ and action } A \\\\\n",
    "&\\text{Loop for each step: } \\\\\n",
    "&\\quad \\text{Take action } A, \\text{ observe } R, S' \\\\\n",
    "&\\quad \\text{Choose } A', \\text{ as a function } \\hat{q}(S', \\cdot, w) \\text{ (e.g., } \\epsilon\\text{-greedy)} \\\\\n",
    "&\\quad \\delta \\leftarrow R - \\bar{R} + \\hat{q}(S', A', w) - \\hat{q}(S, A, w) \\\\\n",
    "&\\quad \\bar{R} \\leftarrow \\bar{R} + \\beta \\delta \\\\\n",
    "&\\quad w \\leftarrow w + \\alpha \\delta \\nabla \\hat{q}(S, A, w) \\\\\n",
    "&\\quad S \\leftarrow S' \\\\\n",
    "&\\quad A \\leftarrow A' \\\\\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intrinsic Rewards (Satinder Singh)\n",
    "\n",
    "### Preferences-Parameters Confound\n",
    "\n",
    "- (most often the)starting point is an **agent-designer** that has an objective reward fucntion that specifies preferences over agent behavior (it is often way too sparse and delayed)\n",
    "\n",
    "- What should the agent's reward function be?\n",
    "\n",
    "- A single reward function **confounds two** roles (from the designers point of view) simultaneously in RL agents\n",
    "\n",
    "    1. (Preference) it expresses the agent-designer's preferences over behaviorss\n",
    "    2. (Parameters) Through the reward hypothesis, it expresses the RL agent's goals/purposes and becomes parameters of actual agent behavior\n",
    "    \n",
    "These roles seem distinct; should they be confounded?\n",
    "\n",
    "### Revised Autonomous Agent\n",
    "\n",
    "![](image/autonomous_agent.png)\n",
    "\n",
    "Agent reward is internal to the agent.\n",
    "\n",
    "Parameters to be designed by agent-designer!\n",
    "\n",
    "### Approches to designing reward\n",
    "\n",
    "- Inverse Reinforcement Learning (Ng et al.)\n",
    "    \n",
    "    - Designer/operator demonstrates optimal behavior\n",
    "    - Clever algorithms for automatically determining **set of** reward function such that observed behavior is optimal (e.g., Bayesian IRL; Ramachandran & Amir)\n",
    "    - Ideal: Set agent reward = objective reward (i.e., preserve the preferences paraters confound)\n",
    "    \n",
    "- Reward shaping \n",
    "    - (Ng et al) agent reward = objective reward + potential-based reward (breaks PP confound)\n",
    "    - Objective: To achieve agent with objective reward's asymptotic behavior faster! [Also Bayesian Reward Shaping by Ramachandran et al.]\n",
    "    \n",
    "- Preference Elicitation (Ideal: preserves PP confound)\n",
    "- Mechanism Design (in Economics)\n",
    "- Other heuristic Approaches\n",
    "\n",
    "### Optimal Reward Problem\n",
    "\n",
    "- There are two reward functions\n",
    "\n",
    "    1. Agent-designer's: objective reward $R_o$ (given)\n",
    "    2. Agent's: internal reward $R_i$\n",
    "    \n",
    "$\\begin{aligned}\n",
    "&\\text{Agent } G(R_i; \\Theta) \\text{ in Environment(Env) produces (random)  interaction }\\\\ &h \\sim <Env, G(R_i; \\Theta)> \\\\\n",
    "&\\text{Utility of interaction } h \\text{ to agent is } U_i(h) = \\sum_t R_i(h_t) \\\\\n",
    "&\\text{Utility to agent designer is } U_o(h) = \\sum_t R_o(h_t) \\\\\n",
    "&\\text{Optimal Reward } R^{*}_i = \\arg\\max_{R_i \\in \\{R_i\\}} Exp_{Env}\\{Exp_{h \\sim <Env, G(R_i; \\Theta)>} \\{U_o(h)\\}\\} \n",
    "\\end{aligned}$\n",
    "\n",
    "This is called Nested optimization, Outer reward optimization, or Inner policy optimization\n",
    "\n",
    "### Policy Gradient for Reward Design (PGRD) (Sorg, Singh, Lewis; NIPS 2010)\n",
    "\n",
    "- Insight: In planning agents, the reward function parameterizes the agent's policy\n",
    "\n",
    "![](image/pgrd.png)\n",
    "\n",
    "PGRD optimizes the reward function via a standard policy gradient approach (OLPOMDP)\n",
    "\n",
    "- PGRD approximates gradient in 2 parts:\n",
    "\n",
    "$ \\nabla_{\\theta} \\mathbb{E}[R_o(\\tau) \\vert Agent(R(\\cdot; \\theta) =  $\n",
    "\n",
    "- $\\nabla_{\\mu} \\mathbb{E}[R_o (\\tau) \\vert \\mu]$\n",
    "    \n",
    "    - Gradient of performance w.r.t the policy\n",
    "    - Approximated by OLPOMDP\n",
    "        \n",
    "- $ \\nabla_{\\theta} \\mu(s, a; \\theta)$\n",
    "    \n",
    "    - Gradient of policy w.r.t reward parameters\n",
    "    - Accounts for the planning procedure\n",
    "    - The sub-gradient of $Q^D$ w.r.t:\n",
    "        \n",
    "$\\nabla_{\\theta} Q^D (s, a) = \\nabla_{\\theta} R(s, a; \\theta) + \\sum\\limits_{s', a'} T(s' \\vert s, a) \\pi(a \\vert s) \\nabla_{\\theta} Q^{D-1}(s', a')$\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
