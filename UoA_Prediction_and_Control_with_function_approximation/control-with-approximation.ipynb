{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Control with Approximation\n",
    "\n",
    "> This is the summary of lecture \"Prediction and Control with Function Approximation\" from Coursera.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Chanseok Kang\n",
    "- categories: [Python, Coursera, Reinforcement_Learning]\n",
    "- image: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episodic SARSA with Function Approximation\n",
    "\n",
    "### State-values to action-values\n",
    "\n",
    "$$ v_{\\pi}(s) \\approx \\hat{v}(s, w) \\doteq w^Tx(s) \\\\\n",
    "q_{\\pi}(s, a) \\approx \\hat{q}(s, a, w) \\doteq w^Tx(s, a) $$\n",
    "\n",
    "### Representing actions\n",
    "\n",
    "$x(s) = \\begin{bmatrix} x_0(s) \\\\ x_1(s) \\\\ x_2(s) \\\\ x_3(s) \\end{bmatrix} \\\\ \\mathcal{A}(s) = \\{a_0, a_1, a_2\\}$\n",
    "\n",
    "$x(s, a) = \\begin{bmatrix} x_0(s) \\\\ x_1(s) \\\\ x_2(s) \\\\ x_3(s) \\\\ x_0(s) \\\\ x_1(s) \\\\ x_2(s) \\\\ x_3(s) \\\\ x_0(s) \\\\ x_1(s) \\\\ x_2(s) \\\\ x_3(s) \\end{bmatrix}$\n",
    "\n",
    "This is called **stacked features**\n",
    "\n",
    "### Episodic Semi-gradient SARSA for Estimating $\\hat{q} \\approx q_{*}$\n",
    "$\\begin{aligned}\n",
    "&\\text{Input: a differentiable action-value function parameterization } \\hat{q}: \\mathcal{s} \\times \\mathcal{A} \\times \\mathbb{R}^d \\to \\mathbb{R} \\\\\n",
    "&\\text{Algorithm paramters: step size } \\alpha > 0, \\text{ small } \\epsilon > 0 \\\\\n",
    "&\\text{Initialize value-function weights } w \\in \\mathbb{R}^d \\text{ arbitrarily (e.g., } w = 0 \\text{)} \\\\\n",
    "\\newline\n",
    "&\\text{Loop for each episode:} \\\\\n",
    "&\\quad S, A \\leftarrow \\text{ initial state and action of episode (e.g., } \\epsilon \\text{-greedy)} \\\\\n",
    "&\\quad \\text{Loop for each step of episode:} \\\\\n",
    "&\\qquad \\text{Take action } A, \\text{ observe } R, S' \\\\\n",
    "&\\qquad \\text{If } S' \\text{ is terminal:} \\\\\n",
    "&\\qquad \\quad w \\leftarrow w + \\alpha[R - \\hat{q}(S, A, w)] \\nabla \\hat{q}(S, A, w) \\\\\n",
    "&\\qquad \\quad \\text{Go to next episode} \\\\\n",
    "&\\qquad \\text{Choose } A' \\text{ as a function of } \\hat{q}(S', \\cdot, w) \\text{ (e.g., } \\epsilon \\text{-greedy)} \\\\\n",
    "&\\qquad w \\leftarrow w + \\alpha[R + \\gamma \\hat{q}(S', A', w) - \\hat{q}(S, A, w)] \\nabla \\hat{q}(S, A, w) \\\\\n",
    "&\\qquad S \\leftarrow S' \\\\\n",
    "&\\qquad A \\leftarrow A' \\\\\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Sarsa with Function Approximation\n",
    "\n",
    "### From SARSA to Expected SARSA\n",
    "\n",
    "SARSA:\n",
    "\n",
    "$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha + \\big(R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)\\big) $\n",
    "\n",
    "Expected SARSA:\n",
    "\n",
    "$ Q(S_t, Q_t) \\leftarrow Q(S_t, A_t) + \\alpha + \\big(R_{t+1} + \\gamma \\sum\\limits_{a'} \\pi(a' \\vert S_{t+1} ) Q(S_{t+1}, a') - Q(S_t, A_t) \\big) $\n",
    "\n",
    "### Expected SARSA with Function Approximation\n",
    "\n",
    "SARSA:\n",
    "\n",
    "$ w \\leftarrow w + \\alpha \\big(R_{t+1} + \\gamma \\hat{q}(S_{t+1}, A_{t+1}, w) - \\hat{q}(S_t, A_t, w)\\big) \\nabla \\hat{q}(S_t, A_t, w) $\n",
    "\n",
    "Expected SARSA:\n",
    "\n",
    "$ w \\leftarrow w + \\alpha \\big(R_{t+1} + \\gamma \\sum\\limits_{a'} \\pi(a' \\vert S_{t+1}) \\hat{q}(S_{t+1}, a', w) - \\hat{q}(S_t, A_t, w) \\big) \\nabla \\hat{q}(S_t, A_t, w) $\n",
    "\n",
    "### Expected SARSA to Q-learning\n",
    "\n",
    "$ w \\leftarrow w + \\alpha \\big( R_{t+1} + \\gamma \\max\\limits_{a'} \\hat{q}(S_{t+1}, a', w) - \\hat{q}(S_t, A_t, w)\\big) \\nabla \\hat{q}(S_t, A_t, w) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration under Function Approximation\n",
    "\n",
    "### Epsilon-Greedy\n",
    "\n",
    "![eg](image/epsilon_greedy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Reward - A New Way of Formulating Control Problems\n",
    "\n",
    "### Simple example\n",
    "\n",
    "![](image/continuing_task.png)\n",
    "\n",
    "Two policies are existed: traversing the left ring and traversing the right ring. The reward is zero everywhere except for in one transition in each ring. In the left ring, the reward is +1 immediately after state $S$. In the right ring, the reward is +2 immediately before state $S$.\n",
    "\n",
    "If we calculate the value function in left policy with geometric series,\n",
    "\n",
    "$v_{L}(S) = \\frac{1}{1 - \\gamma^5}$\n",
    "\n",
    "In case of right policy,\n",
    "\n",
    "$v_{R}(S) = \\frac{2 \\gamma^4}{1 - \\gamma^5}$\n",
    "\n",
    "If we set inequality of above equations,\n",
    "\n",
    "$\\frac{1}{1 - \\gamma^5} = \\frac{2 \\gamma^4}{1 - \\gamma^5}$\n",
    "\n",
    "$v_{R}(S) > v_{L}(S) \\text{ when } \\gamma > 2^{\\frac{1}{4}} \\approx 0.841$\n",
    "\n",
    "In order to maintain the discount factor, we might need gamma to be quite large.\n",
    "\n",
    "### The Average Reward objective\n",
    "\n",
    "$$ r(\\pi) \\doteq \\lim\\limits_{h \\to \\infty} \\frac{1}{h} \\sum\\limits_{t=1}^h \\mathbb{E}[R_t \\vert S_0, A_{0:t-1} \\sim \\pi] $$\n",
    "\n",
    "Using state visitation $\\mu$, we can redefine this,\n",
    "\n",
    "$$ r(\\pi) = \\sum\\limits_s \\mu_{\\pi}(s) \\sum\\limits_{a} \\pi(a \\vert s) \\sum\\limits_{s', r} p(s', r \\vert s, a) r $$\n",
    "\n",
    "### Returns for Average Reward\n",
    "\n",
    "$$G_t = R_{t+1} - r(\\pi) + R_{t+2} - r(\\pi) + R_{t+3} - r(\\pi) + \\dots $$\n",
    "\n",
    "Each term ($R_{t+x} - r(\\pi)$) is called differential returns.\n",
    "\n",
    "### Value Functions for Average Reward\n",
    "\n",
    "$q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}[G_t \\vert S_t = s, A_t = a] $\n",
    "\n",
    "$q_{\\pi}(s, a) = \\sum\\limits_{s', r}p(s', r \\vert s, a) ( r - r(\\pi) + \\sum\\limits_{a'} \\pi(a' \\vert s') q_{\\pi}(s', a'))$\n",
    "\n",
    "### Differential semi-gradient SARSA for estimating $\\hat{q} \\approx q_{*}$\n",
    "\n",
    "$\\begin{aligned}\n",
    "&\\text{Input: a differentiable action-value function parameterization } \\hat{q} : \\mathcal{S} \\times \\mathcal{A} \\times \\mathbb{R}^d \\to \\mathbb{R} \\\\\n",
    "&\\text{Algorithm parameters: step sizes } \\alpha, \\beta > 0 \\\\\n",
    "&\\text{Initialize value-function weights } w \\in \\mathbb{R}^d \\text{ arbitrarily (e.g., } w=0 \\text{)} \\\\\n",
    "&\\text{Initialize average reward estimate } \\bar{R} \\in \\mathbb{R} \\text{ arbitrarily (e.g., } \\bar{R} = 0 \\text{)} \\\\\n",
    "\\newline\n",
    "&\\text{Initialize state } S, \\text{ and action } A \\\\\n",
    "&\\text{Loop for each step: } \\\\\n",
    "&\\quad \\text{Take action } A, \\text{ observe } R, S' \\\\\n",
    "&\\quad \\text{Choose } A', \\text{ as a function } \\hat{q}(S', \\cdot, w) \\text{ (e.g., } \\epsilon\\text{-greedy)} \\\\\n",
    "&\\quad \\delta \\leftarrow R - \\bar{R} + \\hat{q}(S', A', w) - \\hat{q}(S, A, w) \\\\\n",
    "&\\quad \\bar{R} \\leftarrow \\bar{R} + \\beta \\delta \\\\\n",
    "&\\quad w \\leftarrow w + \\alpha \\delta \\nabla \\hat{q}(S, A, w) \\\\\n",
    "&\\quad S \\leftarrow S' \\\\\n",
    "&\\quad A \\leftarrow A' \\\\\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
