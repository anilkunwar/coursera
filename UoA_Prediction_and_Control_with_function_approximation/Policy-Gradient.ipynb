{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient\n",
    "\n",
    "> This is the summary of lecture \"Prediction and Control with Function Approximation\" from Coursera.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Chanseok Kang\n",
    "- categories: [Python, Coursera, Reinforcement_Learning]\n",
    "- image: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Policies Directly\n",
    "\n",
    "### Parameterizing Policy Directly\n",
    "\n",
    "![](image/parameter_policy.png)\n",
    "\n",
    "### Constraints on the Policy Parameterization\n",
    "\n",
    "$\\pi(a \\vert s, \\theta) \\ge 0 \\quad \\text{for all } a \\in \\mathcal{A} \\text{ and } s \\in \\mathcal{S} $\n",
    "\n",
    "$ \\sum\\limits_{a \\in \\mathcal{A}} \\pi( a \\vert s, \\theta) = 1) \\quad \\text{ for all } s \\in \\mathcal{S} $\n",
    "\n",
    "### The softmax Policy Parameterization\n",
    "\n",
    "$ \\pi(a \\vert s, \\theta) \\doteq \\frac{e^{h(s, a, \\theta)}}{ \\sum_{b \\in \\mathcal{A}}e^{h(s, b, \\theta)}} $\n",
    "\n",
    "$ e^{h(s, a, \\theta)}$ is called Action Preference\n",
    "\n",
    "### Action preferences are not action values\n",
    "\n",
    "![](image/action_preference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Objective for Learning Policies\n",
    "\n",
    "### Formalizing the Goal as an objective\n",
    "\n",
    "- Episodic\n",
    "\n",
    "$ G_t = \\sum\\limits_{t=0}^{T} R_t $\n",
    "\n",
    "- Continuing\n",
    "\n",
    "$G_t = \\sum\\limits_{t=0}^{\\infty} \\gamma^t R_t \\text{ (original) } \\\\\n",
    " G_t = \\sum\\limits_{t=0}^{\\infty}  R_t - r(\\pi) \\text{ (applying Average reward) } $\n",
    " \n",
    "### Optimizing the Average Reward Objective\n",
    "\n",
    "$\\nabla r(\\pi) = \\nabla \\sum_s \\mu(s) \\sum_a \\pi(a \\vert s, \\theta) \\sum_{s', r} p(s', r \\vert s, a) r $\n",
    "\n",
    "This is called **policy-gradient** method.\n",
    "\n",
    "### The Challenge of Policy Gradient Method\n",
    "\n",
    "$\\mu(s)$ depends on $\\theta$\n",
    "\n",
    "$\\begin{aligned} \\nabla_w \\overline{VE} &= \\nabla_w \\sum_s \\mu(s)[v_{\\pi}(s) - \\hat{v}(s, w)]^2 \\\\ &= \\sum_s \\mu(s) \\nabla_w [v_{\\pi}(s) - \\hat{v}(s, w)]^2 \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Policy Gradient Theorem\n",
    "\n",
    "### The Gradient of the Objective\n",
    "\n",
    "$\\text{Product Rule: } \\nabla(f(x) g(x)) = \\nabla f(x)g(x) + f(x) \\nabla g(x) $\n",
    "\n",
    "$\\begin{aligned} \\nabla r(\\pi) &= \\nabla \\sum_{s} \\mu(s) \\sum_a \\pi(a \\vert s, \\theta) \\sum_{s', r}p(s', r \\vert s, a) r \\\\ \n",
    " &= \\sum_s \\nabla \\mu(s) \\sum_a \\pi(a \\vert s, \\theta) \\sum_{s', r}p(s', r \\vert s, a) r \\\\ &+ \\sum_s \\mu(s) \\nabla \\sum_a \\pi(a \\vert s, \\theta) \\sum_{s', r} p(s', r \\vert s, a) r \\end{aligned}$\n",
    " \n",
    "### Policy Gradient Theorem\n",
    "\n",
    "$\\nabla r(\\pi) = \\sum_s \\mu(s) \\sum_a \\nabla \\pi(a \\vert s, \\theta) q_{\\pi}(s, a) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
