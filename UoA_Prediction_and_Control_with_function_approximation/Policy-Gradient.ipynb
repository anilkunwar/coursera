{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient\n",
    "\n",
    "> This is the summary of lecture \"Prediction and Control with Function Approximation\" from Coursera.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Chanseok Kang\n",
    "- categories: [Python, Coursera, Reinforcement_Learning]\n",
    "- image: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Policies Directly\n",
    "\n",
    "### Parameterizing Policy Directly\n",
    "\n",
    "![](image/parameter_policy.png)\n",
    "\n",
    "### Constraints on the Policy Parameterization\n",
    "\n",
    "$\\pi(a \\vert s, \\theta) \\ge 0 \\quad \\text{for all } a \\in \\mathcal{A} \\text{ and } s \\in \\mathcal{S} $\n",
    "\n",
    "$ \\sum\\limits_{a \\in \\mathcal{A}} \\pi( a \\vert s, \\theta) = 1) \\quad \\text{ for all } s \\in \\mathcal{S} $\n",
    "\n",
    "### The softmax Policy Parameterization\n",
    "\n",
    "$ \\pi(a \\vert s, \\theta) \\doteq \\frac{e^{h(s, a, \\theta)}}{ \\sum_{b \\in \\mathcal{A}}e^{h(s, b, \\theta)}} $\n",
    "\n",
    "$ e^{h(s, a, \\theta)}$ is called Action Preference\n",
    "\n",
    "### Action preferences are not action values\n",
    "\n",
    "![](image/action_preference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Objective for Learning Policies\n",
    "\n",
    "### Formalizing the Goal as an objective\n",
    "\n",
    "- Episodic\n",
    "\n",
    "$ G_t = \\sum\\limits_{t=0}^{T} R_t $\n",
    "\n",
    "- Continuing\n",
    "\n",
    "$G_t = \\sum\\limits_{t=0}^{\\infty} \\gamma^t R_t \\text{ (original) } \\\\\n",
    " G_t = \\sum\\limits_{t=0}^{\\infty}  R_t - r(\\pi) \\text{ (applying Average reward) } $\n",
    " \n",
    "### Optimizing the Average Reward Objective\n",
    "\n",
    "$\\nabla r(\\pi) = \\nabla \\sum_s \\mu(s) \\sum_a \\pi(a \\vert s, \\theta) \\sum_{s', r} p(s', r \\vert s, a) r $\n",
    "\n",
    "This is called **policy-gradient** method.\n",
    "\n",
    "### The Challenge of Policy Gradient Method\n",
    "\n",
    "$\\mu(s)$ depends on $\\theta$\n",
    "\n",
    "$\\begin{aligned} \\nabla_w \\overline{VE} &= \\nabla_w \\sum_s \\mu(s)[v_{\\pi}(s) - \\hat{v}(s, w)]^2 \\\\ &= \\sum_s \\mu(s) \\nabla_w [v_{\\pi}(s) - \\hat{v}(s, w)]^2 \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Policy Gradient Theorem\n",
    "\n",
    "### The Gradient of the Objective\n",
    "\n",
    "$\\text{Product Rule: } \\nabla(f(x) g(x)) = \\nabla f(x)g(x) + f(x) \\nabla g(x) $\n",
    "\n",
    "$\\begin{aligned} \\nabla r(\\pi) &= \\nabla \\sum_{s} \\mu(s) \\sum_a \\pi(a \\vert s, \\theta) \\sum_{s', r}p(s', r \\vert s, a) r \\\\ \n",
    " &= \\sum_s \\nabla \\mu(s) \\sum_a \\pi(a \\vert s, \\theta) \\sum_{s', r}p(s', r \\vert s, a) r \\\\ &+ \\sum_s \\mu(s) \\nabla \\sum_a \\pi(a \\vert s, \\theta) \\sum_{s', r} p(s', r \\vert s, a) r \\end{aligned}$\n",
    " \n",
    "### Policy Gradient Theorem\n",
    "\n",
    "$\\nabla r(\\pi) = \\sum_s \\mu(s) \\sum_a \\nabla \\pi(a \\vert s, \\theta) q_{\\pi}(s, a) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the policy gradient\n",
    "\n",
    "### Getting Stochastic samples of the gradient\n",
    "\n",
    "$\\nabla r(\\pi) = \\sum\\limits_s \\mu(s) \\sum\\limits_a \\nabla \\pi (s \\vert s, \\theta) q_{\\pi}(s, a) $\n",
    "\n",
    "$ \\theta_{t+1} \\doteq \\theta_t \\alpha \\sum\\limits_a \\nabla \\pi(a \\vert S_t, \\theta_t) q_{\\pi}(S_t, a) $\n",
    "\n",
    "### Unbiasedness of the stochastic samples\n",
    "\n",
    "$ \\begin{aligned} \\nabla r(\\pi) &= \\sum\\limits_s \\mu(s) \\sum\\limits_a \\nabla \\pi(a \\vert s, \\theta) q_{\\pi}(s, a) \\\\\n",
    " &= \\mathbb{E}_{\\mu} [ \\sum\\limits_a \\nabla \\pi(a \\vert S, \\theta) q_{\\pi}(S, a) ] \\end{aligned} $\n",
    " \n",
    "$\\mu$ is the stationary distribution for $\\pi$ which reflects state visitation under $\\pi$. By computing the gradient from a state $S_t$ which is populated from $mu$, we get an unbiased estimate of this expectation.\n",
    "\n",
    "### Getting Stochastic Samples with one action\n",
    "\n",
    "$\\begin{aligned} \\sum\\limits_a \\nabla \\pi (a \\vert S, \\theta) q_{\\pi}(S, a) &= \n",
    " \\sum\\limits_a \\pi(s \\vert S, \\theta) \\frac{1}{\\pi(a \\vert S, \\theta)} \\nabla \\pi(a \\vert S, \\theta) q_{\\pi}(S, a) \\\\\n",
    " &= \\mathbb{E}_{\\pi}[\\frac{\\nabla \\pi(A \\vert S, \\theta)}{\\pi(A \\vert S, \\theta)} q_{\\pi}(S, A) ] \\end{aligned} $\n",
    " \n",
    "### Stochastic Gradient Ascent for policy parameters\n",
    "\n",
    "$ \\begin{aligned} \\theta_{t+1} &\\doteq \\theta_t + \\alpha \\frac{\\nabla \\pi (A_t \\vert S_t, \\theta_t)}{\\pi(A_t \\vert S_t, \\theta_t)} q_{\\pi}(S_t, A_t) \\\\ \n",
    " &= \\theta_t + \\alpha \\nabla \\ln \\pi(A_t \\vert S_t, \\theta_t) q_{\\pi}(S_t \\vert A_t) \\end{aligned} $\n",
    " \n",
    "Note that,\n",
    "\n",
    "$\\nabla \\ln \\big(f(x)\\big) = \\frac{\\nabla f(x)}{f(x)} $\n",
    "\n",
    "### Computing the update\n",
    "\n",
    "$ \\theta_{t+1} \\doteq \\theta_t + \\alpha \\nabla \\ln \\pi(A_t \\vert S_t, \\theta_t) q_{\\pi}(S_t, A_t) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic Algorithm\n",
    "\n",
    "### Approximating the Action Value in the policy update\n",
    "\n",
    "Using one step boot-strapping,\n",
    "\n",
    "$ \\begin{aligned} \\theta_{t+1} &\\doteq + \\alpha \\nabla \\ln \\pi(A_t \\vert S_t, \\theta_t) q_{\\pi}(S_t \\vert A_t) \\\\\n",
    " &= \\theta_t + \\alpha \\nabla \\ln \\pi(A_t \\vert S_t, \\theta_t)[ R_{t+1} - \\bar{R} + \\hat{v}(S_{t+1}, w)] \\end{aligned} $\n",
    " \n",
    "![ac](image/actor_critic.png)\n",
    "\n",
    "### Subtracting the current state's value estimate\n",
    "\n",
    "$ \\theta_{t+1} \\doteq \\theta_t + \\alpha \\nabla \\ln \\pi(A_t \\vert S_t, \\theta_t) [ R_{t+1} - \\bar{R} + \\hat{v}(S_{t+1}, w) - \\hat{v}(S_t, w)] $\n",
    "\n",
    "$R_{t+1} - \\bar{R} + \\hat{v}(S_{t+1}, w) - \\hat{v}(S_t, w)$ is TD error $\\delta$, and it does not affect the expected update.\n",
    "\n",
    "### Adding a baseline\n",
    "\n",
    "$\\mathbb{E}_{\\pi}[ \\nabla \\ln \\pi(A_t \\vert S_t, \\theta_t) [R_{t+1} - \\bar{R} + \\hat{v}(S_{t+1}, w) - \\hat{v}(S_t, w)] \\vert S_t = s ] \\\\\n",
    "= \\mathbb{E}_{\\pi}[ \\nabla \\ln \\pi(A_t \\vert S_t, \\theta_t) [ R_{t+1} - \\bar{R} + \\hat{v}(S_{t+1}, w)] \\vert S_t = s] - \\mathbb{E}_{\\pi} [ \\nabla \\ln \\pi (A_t \\vert S_t, \\theta_t) \\hat{v}(S_t, w) \\vert S_t = s] $\n",
    "\n",
    "Baseline term ($\\mathbb{E}_{\\pi}[ \\nabla \\ln \\pi(A_t \\vert S_t, \\theta_t) \\hat{v}(S_t, w) \\vert S_t = s$) is 0. But we can reduce the update variance through it, which results in faster learning.\n",
    "\n",
    "### How the actor and the critic interact\n",
    "\n",
    "![](image/actor_critic2.png)\n",
    "\n",
    "$ \\theta_{t+1} \\doteq \\theta_t + \\alpha \\nabla \\ln \\pi (A_t \\vert S_t, \\theta_t) \\delta_t $\n",
    "\n",
    "### Actor-Critic (continuing), for estimating $\\pi_{\\theta} \\approx \\pi_{*}$\n",
    "\n",
    "$\\begin{aligned}\n",
    "&\\text{Input: a differentiable policy parameterization } \\pi(a \\vert s, \\theta) \\\\\n",
    "&\\text{Input: a differentiable state-value function parameterization } \\hat{v}(S, w) \\\\\n",
    "&\\text{Initialize } \\bar{R} \\in \\mathbb{R} \\text{ to } 0 \\\\\n",
    "&\\text{Initialize state-value weights } w \\in \\mathbb{R}^d \\text{ and policy parameter } \\theta \\in \\mathbb{R}^d \\text{ (e.g., to } 0 ) \\\\\n",
    "&\\text{Algorithm parameters: } \\alpha^w > 0, \\alpha^{\\theta} > 0, \\alpha^{\\bar{R}} > 0 \\\\\n",
    "&\\text{Initialize } S \\in \\mathcal{S} \\\\\n",
    "&\\text{Loop forever (for each time step):} \\\\\n",
    "&\\quad A \\sim \\pi(\\cdot \\vert S, \\theta) \\\\\n",
    "&\\quad \\text{Take action } A, \\text{ observe } S', R \\\\\n",
    "&\\quad \\delta \\leftarrow R - \\bar{R} + \\hat{v}(s', w) - \\hat{v}(S, w) \\\\ \n",
    "&\\quad \\bar{R} \\leftarrow \\bar{R} + \\alpha^{\\bar{R}} \\delta \\\\\n",
    "&\\quad w \\leftarrow w + \\alpha^w \\delta \\nabla \\hat{v}(S, w) \\\\\n",
    "&\\quad \\theta \\leftarrow \\theta + \\alpha^{\\theta} \\delta \\nabla \\ln \\pi(A \\vert S, \\theta) \\\\\n",
    "&\\quad S \\leftarrow S'\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic with Softmax Policies\n",
    "\n",
    "### Recap - Actor-Critic\n",
    "\n",
    "$ w \\leftarrow w + \\alpha^w \\delta \\nabla \\hat{v}(S, w) $\n",
    "\n",
    "$ \\theta \\leftarrow \\theta + \\alpha^{\\theta} \\delta \\nabla \\ln \\pi(A \\vert S, \\theta) $\n",
    "\n",
    "### Policy update with a softmax policy\n",
    "\n",
    "$ \\theta \\leftarrow \\theta + \\alpha^{\\theta} \\delta \\nabla \\ln \\pi(A \\vert S, \\theta) $\n",
    "\n",
    "$ \\pi(a \\vert s, \\theta) \\doteq \\frac{e^{h(s, a, \\theta}}{\\sum_{b \\in \\mathcal{A}} e^{h(s, b, \\theta)}} $\n",
    "\n",
    "### Features of the Action preference function\n",
    "\n",
    "$ \\hat{v}(s, w) \\doteq w^T x(s) $\n",
    "\n",
    "$ h(s, a, \\theta) \\doteq \\theta^T x_h(s, a) $\n",
    "\n",
    "### Actor-Critic with a softmax policy\n",
    "\n",
    "Critic's update:\n",
    "\n",
    "$ w \\leftarrow w + \\alpha^w \\delta \\nabla \\hat{v}(S, w) $\n",
    "\n",
    "The gradient of linear value function is just the feature vector\n",
    "\n",
    "$ \\nabla \\hat{v}(s, w) = x(s) $\n",
    "\n",
    "So the critic's weight update is like this,\n",
    "\n",
    "$ w \\leftarrow w + \\alpha^w \\delta x(S) $\n",
    "\n",
    "Actor's update:\n",
    "\n",
    "$ \\theta \\leftarrow \\theta + \\alpha^{\\theta} \\delta \\nabla \\ln \\pi(A \\vert S, \\theta) $\n",
    "\n",
    "$ \\nabla \\ln \\pi(a \\vert s, \\theta) = x_h(s, a) - \\sum\\limits_b \\pi(b \\vert s, \\theta) x_h(s, b) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Policies for Continuous Actions\n",
    "\n",
    "### Gaussian distribution\n",
    "\n",
    "$ p(x) \\doteq \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp(- \\frac{(x-\\mu)^2}{2 \\sigma^2}) $\n",
    "\n",
    "### Gaussian policy\n",
    "\n",
    "$\\pi(a \\vert s, \\theta) \\doteq \\frac{1}{\\sigma(s, \\theta) \\sqrt{2\\pi}} \\exp (- \\frac{(a - \\mu(s, \\theta))^2}{2 \\sigma(s, \\theta)^2}) $\n",
    "\n",
    "$ \\mu(s, \\theta) \\doteq \\theta_{\\mu}^T x(s) $\n",
    "\n",
    "$ \\sigma(s, \\theta) \\doteq \\exp \\big(\\theta_{\\sigma}^T x(s)\\big)$\n",
    "\n",
    "$ \\theta \\doteq \\begin{bmatrix} \\theta_{\\mu} \\\\ \\theta_{\\sigma} \\end{bmatrix} $\n",
    "\n",
    "### gaussian policies in action\n",
    "\n",
    "![](image/gaussian_policy.png)\n",
    "\n",
    "![](image/gaussian_policy2.png)\n",
    "\n",
    "### Gradient of the Log of the Gaussian policy\n",
    "\n",
    "$\\nabla \\ln \\pi(a \\vert s, \\theta_{\\mu}) = \\frac{1}{\\sigma(s, \\theta)^2} (a - \\mu(s, \\theta)) x(s) $\n",
    "\n",
    "$ \\nabla \\ln \\pi (a \\vert s, \\theta_{\\sigma}) = \\Big( \\frac{(a - \\mu(s, \\theta))^2}{\\sigma(s, \\theta)^2} - 1 \\Big) x(s) $\n",
    "\n",
    "### Advantages of continuous actions\n",
    "\n",
    "- It might not be straightforward to choose a proper discrete set of actions\n",
    "\n",
    "- Continuous actions allow us to generalize over actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
